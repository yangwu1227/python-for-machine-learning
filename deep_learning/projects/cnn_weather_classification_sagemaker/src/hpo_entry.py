import os
from typing import Any, MutableMapping, Tuple, Union

import numpy as np

os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"  # Nopep8
import optuna
import tensorflow as tf
from model_utils import baseline_cnn, get_secret, load_data, parser, setup_logger
from optuna.trial import TrialState
from tensorflow.errors import InvalidArgumentError

logger = setup_logger(name=__name__)

# -------------------------- Optimization objective -------------------------- #


def objective(
    trial: optuna.Trial,
    train_data: Tuple[np.ndarray, np.ndarray],
    val_data: Tuple[np.ndarray, np.ndarray],
    job_name: str,
) -> float:
    """
    Surrogate function for Optuna to optimize.

    Parameters
    ----------
    trial : optuna.Trial
        Optuna trial object.
    train_data : Tuple[np.ndarray, np.ndarray]
        Training data.
    val_data : Tuple[np.ndarray, np.ndarray]
        Validation data.
    job_name : str
        Name of the aws training job.

    Returns
    -------
    float
        Accuracy metric on validation data.
    """
    # Reset all state generated by Keras for each trial
    tf.keras.backend.clear_session()

    # Data augmentation hyperparameters
    aug_params: MutableMapping[str, Union[float, str]] = {}
    # aug_params['random_brightness_factor'] = trial.suggest_float('random_brightness_factor', 0.1, 1.0)
    aug_params["random_contrast_factor"] = trial.suggest_float(
        "random_contrast_factor", 0.1, 1.0
    )
    aug_params["random_flip_mode"] = trial.suggest_categorical(
        "random_flip_mode", ["horizontal", "vertical", "horizontal_and_vertical"]
    )
    aug_params["random_rotation_factor"] = trial.suggest_float(
        "random_rotation_factor", 0.1, 0.5
    )
    aug_params["random_zoom_factor"] = trial.suggest_float(
        "random_zoom_factor", 0.1, 0.5
    )

    # Convolutional layers hyperparameters
    conv_params: MutableMapping[str, Any] = {}
    conv_params["n_conv_layers"] = trial.suggest_int("n_conv_layers", 1, 3)
    conv_params["filters_list"] = [
        trial.suggest_categorical(f"filters_{i}", [32, 64, 128, 256])
        for i in range(conv_params["n_conv_layers"])
    ]
    conv_params["pool_size"] = trial.suggest_categorical("pool_size", [2, 3])
    conv_params["conv_batch_norm_momentum"] = trial.suggest_float(
        "conv_batch_norm_momentum", 0.7, 0.99
    )
    conv_params["conv2d_weight_decay"] = trial.suggest_float(
        "conv2d_regularizer_decay", 1e-8, 1e-3, log=True
    )

    # Dense layers hyperparameters
    dense_params: MutableMapping[str, Any] = {}
    dense_params["n_dense_layers"] = trial.suggest_int("n_dense_layers", 1, 2)
    dense_params["units_list"] = [
        trial.suggest_categorical(f"units_{i}", [32, 64, 128, 256])
        for i in range(dense_params["n_dense_layers"])
    ]
    dense_params["dense_batch_norm_momentum"] = trial.suggest_float(
        "dense_batch_norm_momentum", 0.7, 0.99
    )
    dense_params["dense_weight_decay"] = trial.suggest_float(
        "dense_regularizer_decay", 1e-8, 1e-3, log=True
    )
    dense_params["dropout_rate"] = trial.suggest_float("dropout_rate", 0.1, 0.5)

    # Optimizer hyperparameters
    opt_params: MutableMapping[str, Union[int, float]] = {}
    opt_params["learning_rate"] = trial.suggest_float(
        "learning_rate", 1e-5, 1e-1, log=True
    )
    opt_params["clipnorm"] = trial.suggest_float("clipnorm", 0.1, 1.0)

    # Fit hyperparameters
    fit_params: MutableMapping[str, int] = {}
    fit_params["epochs"] = trial.suggest_int("epochs", 10, 30)

    # Create and compile model
    compiled_cnn_model = baseline_cnn(conv_params, dense_params, aug_params, opt_params)

    # Callbacks
    early_stopper = tf.keras.callbacks.EarlyStopping(
        monitor="val_accuracy", patience=5, restore_best_weights=True
    )
    keras_pruner = optuna.integration.TFKerasPruningCallback(
        trial, monitor="val_accuracy"
    )

    compiled_cnn_model.fit(
        x=train_data[0],
        y=train_data[1],
        batch_size=32,
        epochs=fit_params["epochs"],
        validation_data=(val_data[0], val_data[1]),
        callbacks=[early_stopper, keras_pruner],
        verbose=2,
    )

    # Save model for the given trial to temp directory in the container
    compiled_cnn_model.save(os.path.join("/tmp", f"model_trial_{trial.number}"))

    # Set user attribute to the trial
    trial.set_user_attr("job_name", job_name)

    return early_stopper.best


# ------------------------ Function for creating study ----------------------- #


def create_study(
    study_name: str, storage: str, direction: str = "maximize"
) -> optuna.study.Study:
    """
    Create Optuna study instance.

    Parameters
    ----------
    study_name : str
        Name of the study.
    storage : str
        Database url.
    direction: str
        Direction of the metric--- maximize or minimize.

    Returns
    -------
    optuna.study.Study
        Optuna study instance.
    """
    study = optuna.create_study(
        storage=storage,
        sampler=optuna.samplers.TPESampler(),
        study_name=study_name,
        direction=direction,
        load_if_exists=True,
    )

    return study


# ------------------------------ Load best model ----------------------------- #


def model_fn(model_dir: str) -> tf.keras.models.Sequential:
    """
    Function that loads the model from 'model_dir'.

    Parameters
    ----------
    model_dir : str
        The path to the directory containing the model.

    Returns
    -------
    tf.keras.models.Sequential
        The loaded model.
    """
    cnn_model = tf.keras.models.load_model(os.path.join(model_dir, "00000000"))

    return cnn_model


def main() -> int:
    args = parser()

    X_train, y_train, X_val, y_val = load_data(
        paths={"train": args.train, "val": args.val}, test_mode=False
    )

    # Wrapper that allows us to pass additional arguments to the surrogate objective function
    def objective_wrapper(trial):
        return objective(
            trial=trial,
            train_data=(X_train, y_train),
            val_data=(X_val, y_val),
            job_name=args.training_env["job_name"],
        )

    # ------------------------------ Set up database ----------------------------- #

    secret = get_secret(args.db_secret, args.region_name)
    connector = "pymysql"
    user_name = secret["username"]
    password = secret["password"]
    db = f"mysql+{connector}://{user_name}:{password}@{args.host}/{args.db_name}"

    # --------------------------------- Optimize --------------------------------- #

    study = create_study(args.study_name, db)
    study.optimize(
        objective_wrapper,
        n_trials=args.n_trials,
        n_jobs=-1,
        catch=(InvalidArgumentError,),
    )

    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])
    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])

    best_trial = study.best_trial

    logger.info(f"Number of pruned trials: {len(pruned_trials)}")
    logger.info(f"Number of complete trials: {len(complete_trials)}")
    logger.info(f"Best trial Accuracy: {best_trial.value}")
    logger.info(f"Best trial Params: {best_trial.params}")

    # ----------------------- Retrieve and save best model ----------------------- #

    best_model = tf.keras.models.load_model(
        os.path.join("/tmp", f"model_trial_{best_trial.number}")
    )

    # Save to model_dir for persistent storage
    best_model.save(os.path.join(args.model_dir, "00000000"))

    return 0


if __name__ == "__main__":
    main()
