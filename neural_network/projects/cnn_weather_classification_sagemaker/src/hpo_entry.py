import os
import argparse
from typing import Tuple, Union, List, Dict
import pickle
import boto3

import numpy as np
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Nopep8
import tensorflow as tf
from tensorflow.errors import InvalidArgumentError

import optuna
from optuna.trial import TrialState

from custom_utils import get_logger, parser, get_secret, load_data, baseline_cnn

# -------------------------- Optimization objective -------------------------- #

def objective(trial: optuna.Trial,
              train_data: Tuple[np.ndarray],
              val_data: Tuple[np.ndarray],
              job_name: str) -> float:
    """
    Surrogate function for Optuna to optimize.

    Parameters
    ----------
    trial : optuna.Trial
        Optuna trial object.
    train_data : Tuple[np.ndarray]
        Training data.
    val_data : Tuple[np.ndarray]
        Validation data.
    job_name : str
        Name of the aws training job.

    Returns
    -------
    float
        Accuracy metric on validation data.
    """
    # Reset all state generated by Keras for each trial
    tf.keras.backend.clear_session()

    # Data augmentation hyperparameters
    aug_params = {}
    # aug_params['random_brightness_factor'] = trial.suggest_float('random_brightness_factor', 0.1, 1.0)
    aug_params['random_contrast_factor'] = trial.suggest_float('random_contrast_factor', 0.1, 1.0)
    aug_params['random_flip_mode'] = trial.suggest_categorical('random_flip_mode', ['horizontal', 'vertical', 'horizontal_and_vertical'])
    aug_params['random_rotation_factor'] = trial.suggest_float('random_rotation_factor', 0.1, 0.5)
    aug_params['random_zoom_factor'] = trial.suggest_float('random_zoom_factor', 0.1, 0.5)
    
    # Convolutional layers hyperparameters
    conv_params = {}
    conv_params['n_conv_layers'] = trial.suggest_int('n_conv_layers', 1, 3)
    conv_params['filters_list'] = [trial.suggest_categorical(f'filters_{i}', [32, 64, 128, 256]) for i in range(conv_params['n_conv_layers'])]
    conv_params['pool_size'] = trial.suggest_categorical('pool_size', [2, 3])
    conv_params['conv_batch_norm_momentum'] = trial.suggest_float('conv_batch_norm_momentum', 0.7, 0.99)
    conv_params['conv2d_weight_decay'] = trial.suggest_float('conv2d_regularizer_decay', 1e-8, 1e-3, log=True)

    # Dense layers hyperparameters
    dense_params = {}
    dense_params['n_dense_layers'] = trial.suggest_int('n_dense_layers', 1, 2)
    dense_params['units_list'] = [trial.suggest_categorical(f'units_{i}', [32, 64, 128, 256]) for i in range(dense_params['n_dense_layers'])]
    dense_params['dense_batch_norm_momentum'] = trial.suggest_float('dense_batch_norm_momentum', 0.7, 0.99)
    dense_params['dense_weight_decay'] = trial.suggest_float('dense_regularizer_decay', 1e-8, 1e-3, log=True)
    dense_params['dropout_rate'] = trial.suggest_float('dropout_rate', 0.1, 0.5)

    # Optimizer hyperparameters
    opt_params = {}
    opt_params['learning_rate'] = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)
    opt_params['clipnorm'] = trial.suggest_float('clipnorm', 0.1, 1.0)
    
    # Fit hyperparameters
    fit_params = {}
    fit_params['epochs'] = trial.suggest_int('epochs', 10, 30)

    # Create and compile model
    compiled_cnn_model = baseline_cnn(conv_params, dense_params, aug_params, opt_params)

    # Callbacks
    early_stopper = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)
    keras_pruner = optuna.integration.TFKerasPruningCallback(trial, monitor='val_accuracy')

    compiled_cnn_model.fit(
        x=train_data[0],
        y=train_data[1],
        batch_size=32,
        epochs=fit_params['epochs'],
        validation_data=(val_data[0], val_data[1]),
        callbacks=[early_stopper, keras_pruner],
        verbose=2
    )
    
    # Save model for the given trial to temp directory in the container
    compiled_cnn_model.save(os.path.join('/tmp', f'model_trial_{trial.number}'))
    
    # Set user attribute to the trial
    trial.set_user_attr('job_name', job_name)

    return early_stopper.best

# ------------------------ Function for creating study ----------------------- #


def create_study(study_name: str, storage: str, direction: str = 'maximize') -> optuna.study.Study:
    """
    Create Optuna study instance.
    
    Parameters
    ----------
    study_name : str
        Name of the study.
    storage : str
        Database url.
    direction: str
        Direction of the metric--- maximize or minimize.
        
    Returns
    -------
    optuna.study.Study
        Optuna study instance.
    """
    study = optuna.create_study(
        storage=storage,
        sampler=optuna.samplers.TPESampler(),
        study_name=study_name,
        direction=direction,
        load_if_exists=True
    )

    return study

# ------------------------------ Load best model ----------------------------- #

def model_fn(model_dir: str) -> tf.keras.models.Sequential:
    """
    Function that loads the model from 'model_dir'.
    
    Parameters
    ----------
    model_dir : str
        The path to the directory containing the model.
    
    Returns
    -------
    tf.keras.models.Sequential
        The loaded model.
    """
    cnn_model = tf.keras.models.load_model(os.path.join(args.model_dir, '00000000'))
    
    return cnn_model

if __name__ == '__main__':
    
    args = parser()

    logger = get_logger(name=__name__)

    X_train, y_train, X_val, y_val = load_data(paths={'train': args.train, 'val': args.val}, test_mode=False)

    # Wrapper that allows us to pass additional arguments to the surrogate objective function
    def objective_wrapper(trial): return objective(
        trial=trial,
        train_data=(X_train, y_train),
        val_data=(X_val, y_val),
        job_name=args.training_env['job_name']
    )

    # ------------------------------ Set up database ----------------------------- #
    
    secret = get_secret(args.db_secret, args.region_name)
    connector = 'pymysql'
    user_name = secret['username']
    password = secret['password']
    db = f'mysql+{connector}://{user_name}:{password}@{args.host}/{args.db_name}'
    
    # --------------------------------- Optimize --------------------------------- #
    
    study = create_study(args.study_name, db)
    study.optimize(objective_wrapper, n_trials=args.n_trials, n_jobs=-1, catch=(InvalidArgumentError,))
    
    pruned_trials = study.get_trials(
        deepcopy=False,
        states=[TrialState.PRUNED]
    )
    complete_trials = study.get_trials(
        deepcopy=False,
        states=[TrialState.COMPLETE]
    )

    best_trial = study.best_trial
    
    logger.info(f'Number of pruned trials: {len(pruned_trials)}')
    logger.info(f'Number of complete trials: {len(complete_trials)}')
    logger.info(f'Best trial Accuracy: {best_trial.value}')
    logger.info(f'Best trial Params: {best_trial.params}')

    # ----------------------- Retrieve and save best model ----------------------- #
    
    best_model = tf.keras.models.load_model(os.path.join('/tmp', f'model_trial_{best_trial.number}'))
    
    # Save to model_dir for persistent storage
    best_model.save(os.path.join(args.model_dir, '00000000'))