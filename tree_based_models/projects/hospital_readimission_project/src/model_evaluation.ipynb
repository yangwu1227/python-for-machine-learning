{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Version 1.7.2\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "# Standard library\n",
    "from pprint import pprint\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "from functools import reduce\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.feature_selection import SelectKBest, RFE, mutual_info_classif\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import balanced_accuracy_score, log_loss, f1_score, make_scorer, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "# Hyperparameter tuning\n",
    "from hyperopt import hp, tpe, fmin, STATUS_OK, Trials\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Custom modules and other\n",
    "import joblib\n",
    "\n",
    "print('XGB Version',xgb.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 17\n",
    "rs = np.random.RandomState(seed)\n",
    "\n",
    "# Folds\n",
    "folds = 5\n",
    "\n",
    "# Paths\n",
    "data_path = '../data/train_test/'\n",
    "model_path = '../outputs/models/'\n",
    "pipe_path = '../outputs/pipeline/'\n",
    "plot_path = '../outputs/plots/'\n",
    "\n",
    "top_num_features = 25"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = pd.read_parquet(data_path + 'train_X.parquet'), pd.read_parquet(data_path + 'train_y.parquet').to_numpy().reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((81412, 49), (81412,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, train_y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Search Using Bayesian Optimization\n",
    "\n",
    "The models with default parameters suffer from severe overfitting. We need to optimize the hyperparameter to reduce the variance. We will use Bayesian optimization to search through the hyperparameter search space. The `hyperopt` package's implementation of the Tree-structured Parzen Estimator algorithm is what we will use.\n",
    "\n",
    "* First, define the search space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search space\n",
    "search_space = {\n",
    "        # booster parameters\n",
    "        'booster_params': {\n",
    "                'booster': 'gbtree',\n",
    "                'objective': 'multi:softprob',\n",
    "                'num_class': 3,\n",
    "                'learning_rate': hp.uniform('learning_rate', 0.001, 0.3), # Range: [0, 1], larger eta shrinks the feature weights more to make the boosting process more conservative, i.e., fewer trees (regularizer)\n",
    "                'gamma': hp.randint('gamma', 0, 9), # Range: [0, inf], the larger the more conservative the algorithm (regularizer)\n",
    "                'max_delta_step': hp.randint('max_delta_step', 1, 10), # Range: [0, inf], values from 1-10 might help control the update for imbalanced data (regularizer)\n",
    "                'lambda': hp.choice('lambda', [1, 10, 100]), # Range: [0, inf], L2 regularization term on weights, the larger the more conservative the algorithm (regularizer)\n",
    "                'alpha': hp.choice('alpha', [1, 10, 100]), # Range: [0, inf], L1 regularization term on weights, the larger the more conservative the algorithm (regularizer)\n",
    "                'colsample_bylevel': hp.choice('colsample_bylevel', np.linspace(0.5, 1, 6).tolist()),\n",
    "                'colsample_bynode': hp.choice('colsample_bynode', np.linspace(0.5, 1, 6).tolist()),\n",
    "                'colsample_bytree': hp.choice('colsample_bytree', np.linspace(0.5, 1, 6).tolist()), # Range: (0, 1], subsample ratio of columns when constructing each tree, the smaller the more conservative the algorithm (regularizer)\n",
    "                'subsample': hp.choice('subsample', np.linspace(0.5, 1, 6).tolist()), # Range: (0, 1], subsample ratio of the training instances every boosting iteration, the smaller the more conservative the algorithm (regularizer)\n",
    "                'max_depth': hp.choice('max_depth', np.arange(3, 12, dtype=np.int16).tolist()), # Range: [0, inf], deep trees boost predictive power but are more likely to overfit (bias reducer)\n",
    "                'tree_method': 'hist',\n",
    "                'predictor': 'cpu_predictor',\n",
    "                'eval_metric': 'mlogloss'\n",
    "        },\n",
    "        # Non-booster parameters\n",
    "        'num_feat': hp.choice('num_feat', [50, 60, 70, 80]),\n",
    "        'step': hp.choice('step', [0.1, 0.2, 0.3]),\n",
    "        'k_neighbors': hp.choice('k_neighbors', [5, 10, 15, 20, 25, 30]),\n",
    "        'num_boost_round': hp.randint('num_boost_round', 500, 2000), # Range: [0, inf], number of boosting iterations, the larger the more likely to overfit (bias reducer)\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, we define the objective function, which contains our cross-validation logic and returns the loss value we wish to minimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params: Dict):\n",
    "    \n",
    "    # Create copies since we passed mutable objects\n",
    "    train_X, train_y = pd.read_parquet('../data/train_test/train_X.parquet'), pd.read_parquet('../data/train_test/train_y.parquet').to_numpy().reshape(-1,)\n",
    "    folds = 2\n",
    "    seed = 1227\n",
    "    \n",
    "    print(params)\n",
    "    \n",
    "    # Stratified K-fold cross-validation\n",
    "    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "    \n",
    "    losses = np.empty(folds)\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_X, train_y)):\n",
    "        \n",
    "        # Train and validation sets\n",
    "        fold_train_X, fold_train_y = train_X.iloc[train_idx], train_y[train_idx]\n",
    "        fold_val_X, fold_val_y = train_X.iloc[val_idx], train_y[val_idx]\n",
    "        \n",
    "        # Processing using fresh copies for every fold\n",
    "        print(f'Start processing fold {fold + 1}...')\n",
    "        preprocessor = joblib.load(pipe_path + 'preprocessor.joblib')\n",
    "        label_encoder = joblib.load(pipe_path + 'label_encoder.joblib')\n",
    "        # Fit and transform on training data\n",
    "        fold_train_X = preprocessor.fit_transform(fold_train_X)\n",
    "        fold_train_y = label_encoder.fit_transform(fold_train_y)\n",
    "        # Transform validation data\n",
    "        fold_val_X = preprocessor.transform(fold_val_X)\n",
    "        fold_val_y = label_encoder.transform(fold_val_y)\n",
    "        \n",
    "        # Feature selection (tuning number of features and step size)\n",
    "        selector = RFE(estimator=DecisionTreeClassifier(random_state=seed), n_features_to_select=int(params['num_feat']), step=float(params['step']))\n",
    "        fold_train_X = selector.fit_transform(fold_train_X, fold_train_y)\n",
    "        fold_val_X = selector.transform(fold_val_X)\n",
    "        \n",
    "        # Oversampling training data (tuning number of neighbors for SMOTE)\n",
    "        print('Class distribution before oversampling:', np.unique(fold_train_y, return_counts=True)[1])\n",
    "        smote = SMOTE(sampling_strategy='not majority', k_neighbors=int(params['k_neighbors']), random_state=seed)\n",
    "        fold_train_X, fold_train_y = smote.fit_resample(fold_train_X, fold_train_y)\n",
    "        print('Class distribution after oversampling:', np.unique(fold_train_y, return_counts=True)[1])\n",
    "        \n",
    "        # Model (tuning hyperparameters)\n",
    "        print(f'Start training model for fold {fold + 1}...')\n",
    "        feature_names = selector.get_feature_names_out().tolist()\n",
    "        dtrain = xgb.DMatrix(data=fold_train_X, label=fold_train_y, feature_names=feature_names)\n",
    "        dvalid = xgb.DMatrix(data=fold_val_X, label=fold_val_y, feature_names=feature_names)\n",
    "        model = xgb.train(\n",
    "            params=params['booster_params'],\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=int(params['num_boost_round']),\n",
    "            early_stopping_rounds=400,\n",
    "            evals=[(dtrain, 'train'), (dvalid, 'validate')], \n",
    "            verbose_eval=200\n",
    "        )\n",
    "        \n",
    "        # Out-of-fold prediction for the current fold\n",
    "        print(f'Predicting for fold {fold + 1}...')\n",
    "        oof_pred = model.predict(data=dvalid)\n",
    "        losses[fold] = log_loss(y_true=fold_val_y, y_pred=oof_pred)\n",
    "        \n",
    "    mean_log_loss = np.mean(losses)\n",
    "        \n",
    "    print(f'Average log loss: {mean_log_loss}')\n",
    "    \n",
    "    return {'loss': mean_log_loss, 'status': STATUS_OK}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Minimize the objective function with the optimal value (multi-class log-loss) with respect to the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'booster_params': {'alpha': 100, 'booster': 'gbtree', 'colsample_bylevel': 0.8, 'colsample_bynode': 0.7, 'colsample_bytree': 0.7, 'eval_metric': 'mlogloss', 'gamma': 8, 'lambda': 100, 'learning_rate': 0.006922847769586983, 'max_delta_step': 6, 'max_depth': 3, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 25, 'num_boost_round': 1002, 'num_feat': 60, 'step': 0.1}\n",
      "Start processing fold 1...                             \n",
      "Class distribution before oversampling:                \n",
      "[ 4543 14218 21945]                                    \n",
      "Class distribution after oversampling:                 \n",
      "[21945 21945 21945]                                    \n",
      "Start training model for fold 1...                     \n",
      "[0]\ttrain-mlogloss:1.09693\tvalidate-mlogloss:1.09695   \n",
      "[200]\ttrain-mlogloss:0.93790\tvalidate-mlogloss:0.94848 \n",
      "[400]\ttrain-mlogloss:0.88034\tvalidate-mlogloss:0.90746 \n",
      "[600]\ttrain-mlogloss:0.84730\tvalidate-mlogloss:0.89907 \n",
      "[800]\ttrain-mlogloss:0.82561\tvalidate-mlogloss:0.89731 \n",
      "[1000]\ttrain-mlogloss:0.80953\tvalidate-mlogloss:0.90272\n",
      "[1001]\ttrain-mlogloss:0.80943\tvalidate-mlogloss:0.90264\n",
      "Predicting for fold 1...                               \n",
      "Start processing fold 2...                             \n",
      "Class distribution before oversampling:                \n",
      "[ 4542 14218 21946]                                    \n",
      "Class distribution after oversampling:                 \n",
      "[21946 21946 21946]                                    \n",
      "Start training model for fold 2...                     \n",
      "[0]\ttrain-mlogloss:1.09697\tvalidate-mlogloss:1.09705   \n",
      "[200]\ttrain-mlogloss:0.93998\tvalidate-mlogloss:0.94949 \n",
      "[400]\ttrain-mlogloss:0.88099\tvalidate-mlogloss:0.90548 \n",
      "[600]\ttrain-mlogloss:0.84721\tvalidate-mlogloss:0.88741 \n",
      "[800]\ttrain-mlogloss:0.82404\tvalidate-mlogloss:0.88390 \n",
      "[1000]\ttrain-mlogloss:0.80717\tvalidate-mlogloss:0.88179\n",
      "[1001]\ttrain-mlogloss:0.80707\tvalidate-mlogloss:0.88185\n",
      "Predicting for fold 2...                               \n",
      "Average log loss: 0.8922483830039394                   \n",
      "{'booster_params': {'alpha': 1, 'booster': 'gbtree', 'colsample_bylevel': 0.9, 'colsample_bynode': 0.8, 'colsample_bytree': 0.7, 'eval_metric': 'mlogloss', 'gamma': 5, 'lambda': 100, 'learning_rate': 0.09682070267054985, 'max_delta_step': 6, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.8, 'tree_method': 'hist'}, 'k_neighbors': 15, 'num_boost_round': 1261, 'num_feat': 50, 'step': 0.1}\n",
      "Start processing fold 1...                                                         \n",
      "Class distribution before oversampling:                                            \n",
      "[ 4543 14218 21945]                                                                \n",
      "Class distribution after oversampling:                                             \n",
      "[21945 21945 21945]                                                                \n",
      "Start training model for fold 1...                                                 \n",
      "[0]\ttrain-mlogloss:1.06716\tvalidate-mlogloss:1.08375                               \n",
      "[200]\ttrain-mlogloss:0.69231\tvalidate-mlogloss:0.93394                             \n",
      "[400]\ttrain-mlogloss:0.68988\tvalidate-mlogloss:0.94100                             \n",
      "[435]\ttrain-mlogloss:0.68948\tvalidate-mlogloss:0.94081                             \n",
      "Predicting for fold 1...                                                           \n",
      "Start processing fold 2...                                                         \n",
      "Class distribution before oversampling:                                            \n",
      "[ 4542 14218 21946]                                                                \n",
      "Class distribution after oversampling:                                             \n",
      "[21946 21946 21946]                                                                \n",
      "Start training model for fold 2...                                                 \n",
      "[0]\ttrain-mlogloss:1.06821\tvalidate-mlogloss:1.07001                               \n",
      "[200]\ttrain-mlogloss:0.69351\tvalidate-mlogloss:0.83868                             \n",
      "[400]\ttrain-mlogloss:0.69206\tvalidate-mlogloss:0.84091                             \n",
      "[447]\ttrain-mlogloss:0.69187\tvalidate-mlogloss:0.84052                             \n",
      "Predicting for fold 2...                                                           \n",
      "Average log loss: 0.8906796769074827                                               \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.5, 'colsample_bynode': 0.8, 'colsample_bytree': 0.5, 'eval_metric': 'mlogloss', 'gamma': 4, 'lambda': 1, 'learning_rate': 0.18884021713108634, 'max_delta_step': 2, 'max_depth': 8, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.7, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1639, 'num_feat': 60, 'step': 0.2}\n",
      "Start processing fold 1...                                                         \n",
      "Class distribution before oversampling:                                            \n",
      "[ 4543 14218 21945]                                                                \n",
      "Class distribution after oversampling:                                             \n",
      "[21945 21945 21945]                                                                \n",
      "Start training model for fold 1...                                                 \n",
      "[0]\ttrain-mlogloss:1.03782\tvalidate-mlogloss:1.04548                               \n",
      "[200]\ttrain-mlogloss:0.67221\tvalidate-mlogloss:0.83930                             \n",
      "[400]\ttrain-mlogloss:0.66812\tvalidate-mlogloss:0.84123                             \n",
      "[465]\ttrain-mlogloss:0.66679\tvalidate-mlogloss:0.84395                             \n",
      "Predicting for fold 1...                                                           \n",
      "Start processing fold 2...                                                         \n",
      "Class distribution before oversampling:                                            \n",
      "[ 4542 14218 21946]                                                                \n",
      "Class distribution after oversampling:                                             \n",
      "[21946 21946 21946]                                                                \n",
      "Start training model for fold 2...                                                 \n",
      "[0]\ttrain-mlogloss:1.03598\tvalidate-mlogloss:1.04535                               \n",
      "[200]\ttrain-mlogloss:0.67045\tvalidate-mlogloss:0.81175                             \n",
      "[400]\ttrain-mlogloss:0.66734\tvalidate-mlogloss:0.81079                             \n",
      "[600]\ttrain-mlogloss:0.66512\tvalidate-mlogloss:0.80951                             \n",
      "[800]\ttrain-mlogloss:0.66441\tvalidate-mlogloss:0.80943                             \n",
      "[1000]\ttrain-mlogloss:0.66312\tvalidate-mlogloss:0.80853                            \n",
      "[1200]\ttrain-mlogloss:0.66248\tvalidate-mlogloss:0.80830                            \n",
      "[1400]\ttrain-mlogloss:0.66192\tvalidate-mlogloss:0.80847                            \n",
      "[1600]\ttrain-mlogloss:0.66154\tvalidate-mlogloss:0.80853                            \n",
      "[1638]\ttrain-mlogloss:0.66152\tvalidate-mlogloss:0.80842                            \n",
      "Predicting for fold 2...                                                           \n",
      "Average log loss: 0.8261874976962715                                               \n",
      "{'booster_params': {'alpha': 100, 'booster': 'gbtree', 'colsample_bylevel': 0.8, 'colsample_bynode': 0.8, 'colsample_bytree': 0.5, 'eval_metric': 'mlogloss', 'gamma': 1, 'lambda': 100, 'learning_rate': 0.27957767572945796, 'max_delta_step': 4, 'max_depth': 6, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.7, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1733, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                         \n",
      "Class distribution before oversampling:                                            \n",
      "[ 4543 14218 21945]                                                                \n",
      "Class distribution after oversampling:                                             \n",
      "[21945 21945 21945]                                                                \n",
      "Start training model for fold 1...                                                 \n",
      "[0]\ttrain-mlogloss:1.02897\tvalidate-mlogloss:1.03043                               \n",
      "[200]\ttrain-mlogloss:0.69914\tvalidate-mlogloss:0.81746                             \n",
      "[400]\ttrain-mlogloss:0.69312\tvalidate-mlogloss:0.81884                             \n",
      "[528]\ttrain-mlogloss:0.69088\tvalidate-mlogloss:0.82017                             \n",
      "Predicting for fold 1...                                                           \n",
      "Start processing fold 2...                                                         \n",
      "Class distribution before oversampling:                                            \n",
      "[ 4542 14218 21946]                                                                \n",
      "Class distribution after oversampling:                                             \n",
      "[21946 21946 21946]                                                                \n",
      "Start training model for fold 2...                                                 \n",
      "[0]\ttrain-mlogloss:1.03774\tvalidate-mlogloss:1.03993                               \n",
      "[200]\ttrain-mlogloss:0.70093\tvalidate-mlogloss:0.79320                             \n",
      "[400]\ttrain-mlogloss:0.69572\tvalidate-mlogloss:0.79138                             \n",
      "[600]\ttrain-mlogloss:0.69272\tvalidate-mlogloss:0.79113                             \n",
      "[800]\ttrain-mlogloss:0.69060\tvalidate-mlogloss:0.79074                             \n",
      "[1000]\ttrain-mlogloss:0.68872\tvalidate-mlogloss:0.79042                            \n",
      "[1200]\ttrain-mlogloss:0.68728\tvalidate-mlogloss:0.79032                            \n",
      "[1400]\ttrain-mlogloss:0.68593\tvalidate-mlogloss:0.78976                            \n",
      "[1600]\ttrain-mlogloss:0.68483\tvalidate-mlogloss:0.79017                            \n",
      "[1732]\ttrain-mlogloss:0.68424\tvalidate-mlogloss:0.78992                            \n",
      "Predicting for fold 2...                                                           \n",
      "Average log loss: 0.8050432588815796                                               \n",
      "{'booster_params': {'alpha': 100, 'booster': 'gbtree', 'colsample_bylevel': 0.7, 'colsample_bynode': 0.5, 'colsample_bytree': 1.0, 'eval_metric': 'mlogloss', 'gamma': 2, 'lambda': 1, 'learning_rate': 0.03427938388720722, 'max_delta_step': 7, 'max_depth': 10, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 30, 'num_boost_round': 1541, 'num_feat': 60, 'step': 0.1}\n",
      "Start processing fold 1...                                                         \n",
      "Class distribution before oversampling:                                            \n",
      "[ 4543 14218 21945]                                                                \n",
      "Class distribution after oversampling:                                             \n",
      "[21945 21945 21945]                                                                \n",
      "Start training model for fold 1...                                                 \n",
      "[0]\ttrain-mlogloss:1.08892\tvalidate-mlogloss:1.09401                               \n",
      "[200]\ttrain-mlogloss:0.76017\tvalidate-mlogloss:0.92480                             \n",
      "[400]\ttrain-mlogloss:0.73145\tvalidate-mlogloss:0.96827                             \n",
      "[501]\ttrain-mlogloss:0.72543\tvalidate-mlogloss:0.98076                             \n",
      "Predicting for fold 1...                                                           \n",
      "Start processing fold 2...                                                         \n",
      "Class distribution before oversampling:                                            \n",
      "[ 4542 14218 21946]                                                                \n",
      "Class distribution after oversampling:                                             \n",
      "[21946 21946 21946]                                                                \n",
      "Start training model for fold 2...                                                 \n",
      "[0]\ttrain-mlogloss:1.08872\tvalidate-mlogloss:1.08940                               \n",
      "[200]\ttrain-mlogloss:0.75853\tvalidate-mlogloss:0.87682                             \n",
      "[400]\ttrain-mlogloss:0.73113\tvalidate-mlogloss:0.89892                             \n",
      "[565]\ttrain-mlogloss:0.72253\tvalidate-mlogloss:0.91773                             \n",
      "Predicting for fold 2...                                                           \n",
      "Average log loss: 0.9493098469494619                                               \n",
      "{'booster_params': {'alpha': 100, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 0.6, 'colsample_bytree': 0.7, 'eval_metric': 'mlogloss', 'gamma': 2, 'lambda': 1, 'learning_rate': 0.0014219280970443086, 'max_delta_step': 4, 'max_depth': 4, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.8, 'tree_method': 'hist'}, 'k_neighbors': 5, 'num_boost_round': 505, 'num_feat': 70, 'step': 0.3}\n",
      "Start processing fold 1...                                                         \n",
      "Class distribution before oversampling:                                            \n",
      "[ 4543 14218 21945]                                                                \n",
      "Class distribution after oversampling:                                             \n",
      "[21945 21945 21945]                                                                \n",
      "Start training model for fold 1...                                                 \n",
      "[0]\ttrain-mlogloss:1.09825\tvalidate-mlogloss:1.09825                               \n",
      "[200]\ttrain-mlogloss:1.03570\tvalidate-mlogloss:1.03798                             \n",
      "[400]\ttrain-mlogloss:0.99070\tvalidate-mlogloss:0.99512                             \n",
      "[504]\ttrain-mlogloss:0.97170\tvalidate-mlogloss:0.97731                             \n",
      "Predicting for fold 1...                                                           \n",
      "Start processing fold 2...                                                         \n",
      "Class distribution before oversampling:                                            \n",
      "[ 4542 14218 21946]                                                                \n",
      "Class distribution after oversampling:                                             \n",
      "[21946 21946 21946]                                                                \n",
      "Start training model for fold 2...                                                 \n",
      "[0]\ttrain-mlogloss:1.09828\tvalidate-mlogloss:1.09829                               \n",
      "[200]\ttrain-mlogloss:1.03999\tvalidate-mlogloss:1.04195                             \n",
      "[400]\ttrain-mlogloss:0.99556\tvalidate-mlogloss:0.99995                             \n",
      "[504]\ttrain-mlogloss:0.97694\tvalidate-mlogloss:0.98259                             \n",
      "Predicting for fold 2...                                                           \n",
      "Average log loss: 0.979952661830259                                                \n",
      "{'booster_params': {'alpha': 100, 'booster': 'gbtree', 'colsample_bylevel': 0.6, 'colsample_bynode': 0.7, 'colsample_bytree': 0.9, 'eval_metric': 'mlogloss', 'gamma': 1, 'lambda': 10, 'learning_rate': 0.26380744221768065, 'max_delta_step': 6, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 1.0, 'tree_method': 'hist'}, 'k_neighbors': 25, 'num_boost_round': 935, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                         \n",
      "Class distribution before oversampling:                                            \n",
      "[ 4543 14218 21945]                                                                \n",
      "Class distribution after oversampling:                                             \n",
      "[21945 21945 21945]                                                                \n",
      "Start training model for fold 1...                                                 \n",
      "[0]\ttrain-mlogloss:1.02408\tvalidate-mlogloss:1.03193                               \n",
      "[200]\ttrain-mlogloss:0.69098\tvalidate-mlogloss:0.81408                             \n",
      "[400]\ttrain-mlogloss:0.69098\tvalidate-mlogloss:0.81408                             \n",
      "[446]\ttrain-mlogloss:0.69098\tvalidate-mlogloss:0.81408                             \n",
      "Predicting for fold 1...                                                           \n",
      "Start processing fold 2...                                                         \n",
      "Class distribution before oversampling:                                            \n",
      "[ 4542 14218 21946]                                                                \n",
      "Class distribution after oversampling:                                             \n",
      "[21946 21946 21946]                                                                \n",
      "Start training model for fold 2...                                                 \n",
      "[0]\ttrain-mlogloss:1.01939\tvalidate-mlogloss:1.02630                               \n",
      "[200]\ttrain-mlogloss:0.68922\tvalidate-mlogloss:0.79414                             \n",
      "[400]\ttrain-mlogloss:0.68922\tvalidate-mlogloss:0.79414                             \n",
      "[483]\ttrain-mlogloss:0.68922\tvalidate-mlogloss:0.79414                             \n",
      "Predicting for fold 2...                                                           \n",
      "Average log loss: 0.8041059694045017                                               \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.9, 'colsample_bynode': 0.9, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 7, 'lambda': 10, 'learning_rate': 0.03721164594737995, 'max_delta_step': 2, 'max_depth': 9, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.6, 'tree_method': 'hist'}, 'k_neighbors': 5, 'num_boost_round': 541, 'num_feat': 70, 'step': 0.3}\n",
      "Start processing fold 1...                                                       \n",
      "Class distribution before oversampling:                                          \n",
      "[ 4543 14218 21945]                                                              \n",
      "Class distribution after oversampling:                                           \n",
      "[21945 21945 21945]                                                              \n",
      "Start training model for fold 1...                                               \n",
      "[0]\ttrain-mlogloss:1.08530\tvalidate-mlogloss:1.08813                             \n",
      "[200]\ttrain-mlogloss:0.70825\tvalidate-mlogloss:0.80656                           \n",
      "[400]\ttrain-mlogloss:0.69922\tvalidate-mlogloss:0.80358                           \n",
      "[540]\ttrain-mlogloss:0.69755\tvalidate-mlogloss:0.80377                           \n",
      "Predicting for fold 1...                                                         \n",
      "Start processing fold 2...                                                       \n",
      "Class distribution before oversampling:                                          \n",
      "[ 4542 14218 21946]                                                              \n",
      "Class distribution after oversampling:                                           \n",
      "[21946 21946 21946]                                                              \n",
      "Start training model for fold 2...                                               \n",
      "[0]\ttrain-mlogloss:1.08549\tvalidate-mlogloss:1.08679                             \n",
      "[200]\ttrain-mlogloss:0.70710\tvalidate-mlogloss:0.80698                           \n",
      "[400]\ttrain-mlogloss:0.69760\tvalidate-mlogloss:0.80435                           \n",
      "[540]\ttrain-mlogloss:0.69592\tvalidate-mlogloss:0.80379                           \n",
      "Predicting for fold 2...                                                         \n",
      "Average log loss: 0.8037783107188421                                             \n",
      "{'booster_params': {'alpha': 100, 'booster': 'gbtree', 'colsample_bylevel': 0.8, 'colsample_bynode': 0.7, 'colsample_bytree': 0.5, 'eval_metric': 'mlogloss', 'gamma': 1, 'lambda': 1, 'learning_rate': 0.179509170878629, 'max_delta_step': 4, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.6, 'tree_method': 'hist'}, 'k_neighbors': 20, 'num_boost_round': 820, 'num_feat': 60, 'step': 0.1}\n",
      "Start processing fold 1...                                                       \n",
      "Class distribution before oversampling:                                          \n",
      "[ 4543 14218 21945]                                                              \n",
      "Class distribution after oversampling:                                           \n",
      "[21945 21945 21945]                                                              \n",
      "Start training model for fold 1...                                               \n",
      "[0]\ttrain-mlogloss:1.05482\tvalidate-mlogloss:1.06053                             \n",
      "[200]\ttrain-mlogloss:0.70554\tvalidate-mlogloss:1.02802                           \n",
      "[400]\ttrain-mlogloss:0.69639\tvalidate-mlogloss:1.05087                           \n",
      "[426]\ttrain-mlogloss:0.69576\tvalidate-mlogloss:1.05188                           \n",
      "Predicting for fold 1...                                                         \n",
      "Start processing fold 2...                                                       \n",
      "Class distribution before oversampling:                                          \n",
      "[ 4542 14218 21946]                                                              \n",
      "Class distribution after oversampling:                                           \n",
      "[21946 21946 21946]                                                              \n",
      "Start training model for fold 2...                                               \n",
      "[0]\ttrain-mlogloss:1.05607\tvalidate-mlogloss:1.06083                             \n",
      "[200]\ttrain-mlogloss:0.70556\tvalidate-mlogloss:0.92976                           \n",
      "[400]\ttrain-mlogloss:0.69895\tvalidate-mlogloss:0.94420                           \n",
      "[414]\ttrain-mlogloss:0.69861\tvalidate-mlogloss:0.94374                           \n",
      "Predicting for fold 2...                                                         \n",
      "Average log loss: 0.9977880498462921                                             \n",
      "{'booster_params': {'alpha': 1, 'booster': 'gbtree', 'colsample_bylevel': 0.5, 'colsample_bynode': 0.8, 'colsample_bytree': 0.6, 'eval_metric': 'mlogloss', 'gamma': 8, 'lambda': 1, 'learning_rate': 0.013365637931851193, 'max_delta_step': 3, 'max_depth': 3, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 1.0, 'tree_method': 'hist'}, 'k_neighbors': 30, 'num_boost_round': 703, 'num_feat': 60, 'step': 0.1}\n",
      "Start processing fold 1...                                                       \n",
      "Class distribution before oversampling:                                          \n",
      "[ 4543 14218 21945]                                                              \n",
      "Class distribution after oversampling:                                           \n",
      "[21945 21945 21945]                                                              \n",
      "Start training model for fold 1...                                               \n",
      "[0]\ttrain-mlogloss:1.09563\tvalidate-mlogloss:1.09569                             \n",
      "[200]\ttrain-mlogloss:0.87139\tvalidate-mlogloss:0.90751                           \n",
      "[400]\ttrain-mlogloss:0.80576\tvalidate-mlogloss:0.89552                           \n",
      "[600]\ttrain-mlogloss:0.77042\tvalidate-mlogloss:0.91732                           \n",
      "[702]\ttrain-mlogloss:0.75844\tvalidate-mlogloss:0.92202                           \n",
      "Predicting for fold 1...                                                         \n",
      "Start processing fold 2...                                                       \n",
      "Class distribution before oversampling:                                          \n",
      "[ 4542 14218 21946]                                                              \n",
      "Class distribution after oversampling:                                           \n",
      "[21946 21946 21946]                                                              \n",
      "Start training model for fold 2...                                               \n",
      "[0]\ttrain-mlogloss:1.09541\tvalidate-mlogloss:1.09549                             \n",
      "[200]\ttrain-mlogloss:0.86761\tvalidate-mlogloss:0.89881                           \n",
      "[400]\ttrain-mlogloss:0.80392\tvalidate-mlogloss:0.87203                           \n",
      "[600]\ttrain-mlogloss:0.76944\tvalidate-mlogloss:0.86573                           \n",
      "[702]\ttrain-mlogloss:0.75850\tvalidate-mlogloss:0.86259                           \n",
      "Predicting for fold 2...                                                         \n",
      "Average log loss: 0.8923061955495657                                             \n",
      "{'booster_params': {'alpha': 100, 'booster': 'gbtree', 'colsample_bylevel': 0.8, 'colsample_bynode': 1.0, 'colsample_bytree': 0.6, 'eval_metric': 'mlogloss', 'gamma': 3, 'lambda': 10, 'learning_rate': 0.21797517044614675, 'max_delta_step': 6, 'max_depth': 4, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.9, 'tree_method': 'hist'}, 'k_neighbors': 20, 'num_boost_round': 647, 'num_feat': 60, 'step': 0.1}\n",
      "Start processing fold 1...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4543 14218 21945]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21945 21945 21945]                                                                 \n",
      "Start training model for fold 1...                                                  \n",
      "[0]\ttrain-mlogloss:1.04222\tvalidate-mlogloss:1.05734                                \n",
      "[200]\ttrain-mlogloss:0.71329\tvalidate-mlogloss:0.94160                              \n",
      "[400]\ttrain-mlogloss:0.71199\tvalidate-mlogloss:0.94885                              \n",
      "[426]\ttrain-mlogloss:0.71180\tvalidate-mlogloss:0.94882                              \n",
      "Predicting for fold 1...                                                            \n",
      "Start processing fold 2...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4542 14218 21946]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21946 21946 21946]                                                                 \n",
      "Start training model for fold 2...                                                  \n",
      "[0]\ttrain-mlogloss:1.04341\tvalidate-mlogloss:1.04772                                \n",
      "[200]\ttrain-mlogloss:0.71265\tvalidate-mlogloss:0.91178                              \n",
      "[400]\ttrain-mlogloss:0.71117\tvalidate-mlogloss:0.91612                              \n",
      "[422]\ttrain-mlogloss:0.71117\tvalidate-mlogloss:0.91612                              \n",
      "Predicting for fold 2...                                                            \n",
      "Average log loss: 0.9324721974286787                                                \n",
      "{'booster_params': {'alpha': 100, 'booster': 'gbtree', 'colsample_bylevel': 0.9, 'colsample_bynode': 1.0, 'colsample_bytree': 1.0, 'eval_metric': 'mlogloss', 'gamma': 8, 'lambda': 10, 'learning_rate': 0.11490330090624763, 'max_delta_step': 1, 'max_depth': 8, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.6, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1361, 'num_feat': 70, 'step': 0.3}\n",
      "Start processing fold 1...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4543 14218 21945]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21945 21945 21945]                                                                 \n",
      "Start training model for fold 1...                                                  \n",
      "[0]\ttrain-mlogloss:1.06405\tvalidate-mlogloss:1.06613                                \n",
      "[200]\ttrain-mlogloss:0.73615\tvalidate-mlogloss:0.82346                              \n",
      "[400]\ttrain-mlogloss:0.73397\tvalidate-mlogloss:0.82396                              \n",
      "[598]\ttrain-mlogloss:0.73333\tvalidate-mlogloss:0.82528                              \n",
      "Predicting for fold 1...                                                            \n",
      "Start processing fold 2...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4542 14218 21946]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21946 21946 21946]                                                                 \n",
      "Start training model for fold 2...                                                  \n",
      "[0]\ttrain-mlogloss:1.06314\tvalidate-mlogloss:1.06514                                \n",
      "[200]\ttrain-mlogloss:0.73378\tvalidate-mlogloss:0.82125                              \n",
      "[400]\ttrain-mlogloss:0.73165\tvalidate-mlogloss:0.82052                              \n",
      "[600]\ttrain-mlogloss:0.73109\tvalidate-mlogloss:0.82003                              \n",
      "[800]\ttrain-mlogloss:0.73083\tvalidate-mlogloss:0.81968                              \n",
      "[1000]\ttrain-mlogloss:0.73051\tvalidate-mlogloss:0.81950                             \n",
      "[1200]\ttrain-mlogloss:0.73009\tvalidate-mlogloss:0.81993                             \n",
      "[1360]\ttrain-mlogloss:0.72990\tvalidate-mlogloss:0.81974                             \n",
      "Predicting for fold 2...                                                            \n",
      "Average log loss: 0.8225059957724536                                                \n",
      "{'booster_params': {'alpha': 1, 'booster': 'gbtree', 'colsample_bylevel': 0.6, 'colsample_bynode': 0.8, 'colsample_bytree': 0.9, 'eval_metric': 'mlogloss', 'gamma': 6, 'lambda': 1, 'learning_rate': 0.17489784172457687, 'max_delta_step': 2, 'max_depth': 6, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.9, 'tree_method': 'hist'}, 'k_neighbors': 25, 'num_boost_round': 931, 'num_feat': 70, 'step': 0.1}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.04098\tvalidate-mlogloss:1.04567                              \n",
      "[200]\ttrain-mlogloss:0.67792\tvalidate-mlogloss:0.94670                            \n",
      "[400]\ttrain-mlogloss:0.67544\tvalidate-mlogloss:0.94417                            \n",
      "[417]\ttrain-mlogloss:0.67529\tvalidate-mlogloss:0.94416                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.04127\tvalidate-mlogloss:1.04688                              \n",
      "[200]\ttrain-mlogloss:0.67497\tvalidate-mlogloss:0.86475                            \n",
      "[400]\ttrain-mlogloss:0.67373\tvalidate-mlogloss:0.86878                            \n",
      "[421]\ttrain-mlogloss:0.67364\tvalidate-mlogloss:0.86870                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.9064551068609219                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.8, 'colsample_bynode': 0.7, 'colsample_bytree': 1.0, 'eval_metric': 'mlogloss', 'gamma': 7, 'lambda': 100, 'learning_rate': 0.14463723503724327, 'max_delta_step': 5, 'max_depth': 6, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.6, 'tree_method': 'hist'}, 'k_neighbors': 5, 'num_boost_round': 903, 'num_feat': 60, 'step': 0.2}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.05530\tvalidate-mlogloss:1.07137                              \n",
      "[200]\ttrain-mlogloss:0.71200\tvalidate-mlogloss:0.83135                            \n",
      "[400]\ttrain-mlogloss:0.70899\tvalidate-mlogloss:0.83143                            \n",
      "[600]\ttrain-mlogloss:0.70783\tvalidate-mlogloss:0.83100                            \n",
      "[800]\ttrain-mlogloss:0.70712\tvalidate-mlogloss:0.83006                            \n",
      "[902]\ttrain-mlogloss:0.70667\tvalidate-mlogloss:0.82972                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.05432\tvalidate-mlogloss:1.05690                              \n",
      "[200]\ttrain-mlogloss:0.70965\tvalidate-mlogloss:0.83074                            \n",
      "[400]\ttrain-mlogloss:0.70728\tvalidate-mlogloss:0.82940                            \n",
      "[475]\ttrain-mlogloss:0.70649\tvalidate-mlogloss:0.82889                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.829301344348405                                               \n",
      "{'booster_params': {'alpha': 1, 'booster': 'gbtree', 'colsample_bylevel': 0.6, 'colsample_bynode': 0.8, 'colsample_bytree': 0.9, 'eval_metric': 'mlogloss', 'gamma': 8, 'lambda': 10, 'learning_rate': 0.2851824478422553, 'max_delta_step': 5, 'max_depth': 4, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 25, 'num_boost_round': 1202, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.01820\tvalidate-mlogloss:1.02091                              \n",
      "[200]\ttrain-mlogloss:0.71486\tvalidate-mlogloss:0.82109                            \n",
      "[400]\ttrain-mlogloss:0.71237\tvalidate-mlogloss:0.81967                            \n",
      "[519]\ttrain-mlogloss:0.71158\tvalidate-mlogloss:0.81839                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.02408\tvalidate-mlogloss:1.02800                              \n",
      "[200]\ttrain-mlogloss:0.71127\tvalidate-mlogloss:0.79713                            \n",
      "[400]\ttrain-mlogloss:0.70834\tvalidate-mlogloss:0.79652                            \n",
      "[600]\ttrain-mlogloss:0.70647\tvalidate-mlogloss:0.79634                            \n",
      "[800]\ttrain-mlogloss:0.70563\tvalidate-mlogloss:0.79532                            \n",
      "[1000]\ttrain-mlogloss:0.70525\tvalidate-mlogloss:0.79528                           \n",
      "[1200]\ttrain-mlogloss:0.70469\tvalidate-mlogloss:0.79827                           \n",
      "[1201]\ttrain-mlogloss:0.70469\tvalidate-mlogloss:0.79809                           \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8082408095570885                                              \n",
      "{'booster_params': {'alpha': 1, 'booster': 'gbtree', 'colsample_bylevel': 0.5, 'colsample_bynode': 0.7, 'colsample_bytree': 0.9, 'eval_metric': 'mlogloss', 'gamma': 8, 'lambda': 100, 'learning_rate': 0.07793045466884788, 'max_delta_step': 3, 'max_depth': 3, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 15, 'num_boost_round': 799, 'num_feat': 60, 'step': 0.2}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.08200\tvalidate-mlogloss:1.08213                              \n",
      "[200]\ttrain-mlogloss:0.74547\tvalidate-mlogloss:0.84297                            \n",
      "[400]\ttrain-mlogloss:0.73455\tvalidate-mlogloss:0.84690                            \n",
      "[600]\ttrain-mlogloss:0.73272\tvalidate-mlogloss:0.84896                            \n",
      "[615]\ttrain-mlogloss:0.73270\tvalidate-mlogloss:0.84885                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.08127\tvalidate-mlogloss:1.08116                              \n",
      "[200]\ttrain-mlogloss:0.74340\tvalidate-mlogloss:0.82235                            \n",
      "[400]\ttrain-mlogloss:0.73295\tvalidate-mlogloss:0.82249                            \n",
      "[599]\ttrain-mlogloss:0.73045\tvalidate-mlogloss:0.82245                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8356087229682335                                              \n",
      "{'booster_params': {'alpha': 100, 'booster': 'gbtree', 'colsample_bylevel': 0.5, 'colsample_bynode': 0.7, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 3, 'lambda': 1, 'learning_rate': 0.19247491634163397, 'max_delta_step': 9, 'max_depth': 8, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.8, 'tree_method': 'hist'}, 'k_neighbors': 15, 'num_boost_round': 839, 'num_feat': 60, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.04831\tvalidate-mlogloss:1.05511                              \n",
      "[200]\ttrain-mlogloss:0.71382\tvalidate-mlogloss:0.86935                            \n",
      "[400]\ttrain-mlogloss:0.71157\tvalidate-mlogloss:0.86916                            \n",
      "[434]\ttrain-mlogloss:0.71120\tvalidate-mlogloss:0.86851                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.05268\tvalidate-mlogloss:1.05639                              \n",
      "[200]\ttrain-mlogloss:0.71658\tvalidate-mlogloss:0.83720                            \n",
      "[400]\ttrain-mlogloss:0.71521\tvalidate-mlogloss:0.83683                            \n",
      "[471]\ttrain-mlogloss:0.71466\tvalidate-mlogloss:0.83657                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8525418194133467                                              \n",
      "{'booster_params': {'alpha': 1, 'booster': 'gbtree', 'colsample_bylevel': 0.8, 'colsample_bynode': 0.9, 'colsample_bytree': 0.7, 'eval_metric': 'mlogloss', 'gamma': 2, 'lambda': 1, 'learning_rate': 0.2855838325010951, 'max_delta_step': 1, 'max_depth': 4, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.8, 'tree_method': 'hist'}, 'k_neighbors': 30, 'num_boost_round': 1268, 'num_feat': 80, 'step': 0.2}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.01507\tvalidate-mlogloss:1.01978                              \n",
      "[200]\ttrain-mlogloss:0.62458\tvalidate-mlogloss:0.82289                            \n",
      "[400]\ttrain-mlogloss:0.61002\tvalidate-mlogloss:0.83523                            \n",
      "[487]\ttrain-mlogloss:0.60430\tvalidate-mlogloss:0.83557                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.01793\tvalidate-mlogloss:1.02199                              \n",
      "[200]\ttrain-mlogloss:0.62283\tvalidate-mlogloss:0.89082                            \n",
      "[400]\ttrain-mlogloss:0.60756\tvalidate-mlogloss:0.91437                            \n",
      "[442]\ttrain-mlogloss:0.60643\tvalidate-mlogloss:0.91365                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8746540208013153                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 0.9, 'colsample_bytree': 0.9, 'eval_metric': 'mlogloss', 'gamma': 8, 'lambda': 1, 'learning_rate': 0.2522631594888545, 'max_delta_step': 4, 'max_depth': 6, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.9, 'tree_method': 'hist'}, 'k_neighbors': 30, 'num_boost_round': 1032, 'num_feat': 80, 'step': 0.1}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.01149\tvalidate-mlogloss:1.01894                              \n",
      "[200]\ttrain-mlogloss:0.69358\tvalidate-mlogloss:0.87593                            \n",
      "[400]\ttrain-mlogloss:0.69323\tvalidate-mlogloss:0.87592                            \n",
      "[411]\ttrain-mlogloss:0.69323\tvalidate-mlogloss:0.87615                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.01149\tvalidate-mlogloss:1.01804                              \n",
      "[200]\ttrain-mlogloss:0.69431\tvalidate-mlogloss:0.83602                            \n",
      "[400]\ttrain-mlogloss:0.69420\tvalidate-mlogloss:0.83589                            \n",
      "[417]\ttrain-mlogloss:0.69420\tvalidate-mlogloss:0.83582                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8559641508935162                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 0.7, 'colsample_bytree': 1.0, 'eval_metric': 'mlogloss', 'gamma': 1, 'lambda': 100, 'learning_rate': 0.20523705202643763, 'max_delta_step': 8, 'max_depth': 3, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.9, 'tree_method': 'hist'}, 'k_neighbors': 5, 'num_boost_round': 1593, 'num_feat': 70, 'step': 0.1}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.05226\tvalidate-mlogloss:1.05187                              \n",
      "[200]\ttrain-mlogloss:0.68080\tvalidate-mlogloss:0.91907                            \n",
      "[400]\ttrain-mlogloss:0.64959\tvalidate-mlogloss:0.96366                            \n",
      "[475]\ttrain-mlogloss:0.64321\tvalidate-mlogloss:0.96870                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.05263\tvalidate-mlogloss:1.05301                              \n",
      "[200]\ttrain-mlogloss:0.67674\tvalidate-mlogloss:0.87119                            \n",
      "[400]\ttrain-mlogloss:0.64658\tvalidate-mlogloss:0.91228                            \n",
      "[439]\ttrain-mlogloss:0.64287\tvalidate-mlogloss:0.91972                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.9442293498424847                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.6, 'colsample_bynode': 0.9, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 7, 'lambda': 10, 'learning_rate': 0.05017502278903675, 'max_delta_step': 2, 'max_depth': 9, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 1.0, 'tree_method': 'hist'}, 'k_neighbors': 25, 'num_boost_round': 1580, 'num_feat': 50, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.08018\tvalidate-mlogloss:1.08323                              \n",
      "[200]\ttrain-mlogloss:0.70255\tvalidate-mlogloss:0.82319                            \n",
      "[400]\ttrain-mlogloss:0.70255\tvalidate-mlogloss:0.82326                            \n",
      "[525]\ttrain-mlogloss:0.70255\tvalidate-mlogloss:0.82326                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.07979\tvalidate-mlogloss:1.08264                              \n",
      "[200]\ttrain-mlogloss:0.69884\tvalidate-mlogloss:0.80027                            \n",
      "[400]\ttrain-mlogloss:0.69884\tvalidate-mlogloss:0.80021                            \n",
      "[574]\ttrain-mlogloss:0.69884\tvalidate-mlogloss:0.80021                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8117365786095563                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.9, 'colsample_bynode': 0.6, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 0, 'lambda': 10, 'learning_rate': 0.24126075731837235, 'max_delta_step': 6, 'max_depth': 9, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 1.0, 'tree_method': 'hist'}, 'k_neighbors': 5, 'num_boost_round': 1172, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.00627\tvalidate-mlogloss:1.01911                              \n",
      "[200]\ttrain-mlogloss:0.32809\tvalidate-mlogloss:0.80660                            \n",
      "[400]\ttrain-mlogloss:0.21670\tvalidate-mlogloss:0.82949                            \n",
      "[459]\ttrain-mlogloss:0.20020\tvalidate-mlogloss:0.83406                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.00875\tvalidate-mlogloss:1.02212                              \n",
      "[200]\ttrain-mlogloss:0.33420\tvalidate-mlogloss:0.79363                            \n",
      "[400]\ttrain-mlogloss:0.21972\tvalidate-mlogloss:0.81567                            \n",
      "[452]\ttrain-mlogloss:0.20591\tvalidate-mlogloss:0.81979                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.827009104686875                                               \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.9, 'colsample_bynode': 0.5, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 7, 'lambda': 10, 'learning_rate': 0.14022158985221944, 'max_delta_step': 2, 'max_depth': 11, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 1.0, 'tree_method': 'hist'}, 'k_neighbors': 25, 'num_boost_round': 1575, 'num_feat': 70, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.04680\tvalidate-mlogloss:1.05237                              \n",
      "[200]\ttrain-mlogloss:0.69220\tvalidate-mlogloss:0.80963                            \n",
      "[400]\ttrain-mlogloss:0.69220\tvalidate-mlogloss:0.80963                            \n",
      "[432]\ttrain-mlogloss:0.69220\tvalidate-mlogloss:0.80963                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.04680\tvalidate-mlogloss:1.05264                              \n",
      "[200]\ttrain-mlogloss:0.69343\tvalidate-mlogloss:0.81969                            \n",
      "[400]\ttrain-mlogloss:0.69343\tvalidate-mlogloss:0.81969                            \n",
      "[443]\ttrain-mlogloss:0.69343\tvalidate-mlogloss:0.81969                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8146610808056978                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.6, 'colsample_bynode': 0.9, 'colsample_bytree': 0.9, 'eval_metric': 'mlogloss', 'gamma': 0, 'lambda': 10, 'learning_rate': 0.06706456864399245, 'max_delta_step': 8, 'max_depth': 5, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.6, 'tree_method': 'hist'}, 'k_neighbors': 5, 'num_boost_round': 1314, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.07817\tvalidate-mlogloss:1.07940                              \n",
      "[200]\ttrain-mlogloss:0.67849\tvalidate-mlogloss:0.79767                            \n",
      "[400]\ttrain-mlogloss:0.63007\tvalidate-mlogloss:0.79668                            \n",
      "[584]\ttrain-mlogloss:0.59854\tvalidate-mlogloss:0.80122                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.07744\tvalidate-mlogloss:1.07894                              \n",
      "[200]\ttrain-mlogloss:0.67707\tvalidate-mlogloss:0.78630                            \n",
      "[400]\ttrain-mlogloss:0.62961\tvalidate-mlogloss:0.78165                            \n",
      "[600]\ttrain-mlogloss:0.59641\tvalidate-mlogloss:0.77989                            \n",
      "[800]\ttrain-mlogloss:0.56972\tvalidate-mlogloss:0.78023                            \n",
      "[941]\ttrain-mlogloss:0.55323\tvalidate-mlogloss:0.78123                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.7912254302383879                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.7, 'colsample_bynode': 0.9, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 0, 'lambda': 10, 'learning_rate': 0.065721439243724, 'max_delta_step': 8, 'max_depth': 5, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.6, 'tree_method': 'hist'}, 'k_neighbors': 5, 'num_boost_round': 908, 'num_feat': 70, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.07766\tvalidate-mlogloss:1.07883                              \n",
      "[200]\ttrain-mlogloss:0.67841\tvalidate-mlogloss:0.81080                            \n",
      "[400]\ttrain-mlogloss:0.63114\tvalidate-mlogloss:0.81448                            \n",
      "[518]\ttrain-mlogloss:0.60971\tvalidate-mlogloss:0.81851                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.08019\tvalidate-mlogloss:1.08110                              \n",
      "[200]\ttrain-mlogloss:0.67823\tvalidate-mlogloss:0.80823                            \n",
      "[400]\ttrain-mlogloss:0.63087\tvalidate-mlogloss:0.80751                            \n",
      "[600]\ttrain-mlogloss:0.59751\tvalidate-mlogloss:0.80899                            \n",
      "[661]\ttrain-mlogloss:0.58889\tvalidate-mlogloss:0.81085                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8146944830147074                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.6, 'colsample_bynode': 0.9, 'colsample_bytree': 0.6, 'eval_metric': 'mlogloss', 'gamma': 0, 'lambda': 10, 'learning_rate': 0.027879121814829932, 'max_delta_step': 8, 'max_depth': 5, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.6, 'tree_method': 'hist'}, 'k_neighbors': 5, 'num_boost_round': 1382, 'num_feat': 50, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.09099\tvalidate-mlogloss:1.09205                              \n",
      "[200]\ttrain-mlogloss:0.74680\tvalidate-mlogloss:0.83952                            \n",
      "[400]\ttrain-mlogloss:0.70015\tvalidate-mlogloss:0.82924                            \n",
      "[600]\ttrain-mlogloss:0.67439\tvalidate-mlogloss:0.82979                            \n",
      "[800]\ttrain-mlogloss:0.65553\tvalidate-mlogloss:0.83791                            \n",
      "[856]\ttrain-mlogloss:0.65096\tvalidate-mlogloss:0.83881                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.09183\tvalidate-mlogloss:1.09203                              \n",
      "[200]\ttrain-mlogloss:0.74620\tvalidate-mlogloss:0.82097                            \n",
      "[400]\ttrain-mlogloss:0.69828\tvalidate-mlogloss:0.80433                            \n",
      "[600]\ttrain-mlogloss:0.67356\tvalidate-mlogloss:0.80044                            \n",
      "[800]\ttrain-mlogloss:0.65507\tvalidate-mlogloss:0.80222                            \n",
      "[1000]\ttrain-mlogloss:0.63973\tvalidate-mlogloss:0.80423                           \n",
      "[1016]\ttrain-mlogloss:0.63864\tvalidate-mlogloss:0.80420                           \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8215018508523                                                 \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.9, 'colsample_bynode': 0.9, 'colsample_bytree': 0.9, 'eval_metric': 'mlogloss', 'gamma': 5, 'lambda': 10, 'learning_rate': 0.11437788235685291, 'max_delta_step': 8, 'max_depth': 5, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.6, 'tree_method': 'hist'}, 'k_neighbors': 5, 'num_boost_round': 504, 'num_feat': 70, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.06131\tvalidate-mlogloss:1.06399                              \n",
      "[200]\ttrain-mlogloss:0.68821\tvalidate-mlogloss:0.80603                            \n",
      "[400]\ttrain-mlogloss:0.68334\tvalidate-mlogloss:0.80484                            \n",
      "[465]\ttrain-mlogloss:0.68246\tvalidate-mlogloss:0.80419                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.06210\tvalidate-mlogloss:1.06480                              \n",
      "[200]\ttrain-mlogloss:0.68805\tvalidate-mlogloss:0.81690                            \n",
      "[400]\ttrain-mlogloss:0.68277\tvalidate-mlogloss:0.81430                            \n",
      "[503]\ttrain-mlogloss:0.68166\tvalidate-mlogloss:0.81486                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8095339444525378                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.7, 'colsample_bynode': 0.9, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 6, 'lambda': 10, 'learning_rate': 0.0760286525898411, 'max_delta_step': 9, 'max_depth': 9, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.6, 'tree_method': 'hist'}, 'k_neighbors': 5, 'num_boost_round': 754, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.07035\tvalidate-mlogloss:1.07340                              \n",
      "[200]\ttrain-mlogloss:0.69196\tvalidate-mlogloss:0.79151                            \n",
      "[400]\ttrain-mlogloss:0.68926\tvalidate-mlogloss:0.79059                            \n",
      "[600]\ttrain-mlogloss:0.68741\tvalidate-mlogloss:0.79008                            \n",
      "[753]\ttrain-mlogloss:0.68635\tvalidate-mlogloss:0.78944                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.07260\tvalidate-mlogloss:1.07550                              \n",
      "[200]\ttrain-mlogloss:0.69175\tvalidate-mlogloss:0.78753                            \n",
      "[400]\ttrain-mlogloss:0.68828\tvalidate-mlogloss:0.78599                            \n",
      "[600]\ttrain-mlogloss:0.68670\tvalidate-mlogloss:0.78551                            \n",
      "[753]\ttrain-mlogloss:0.68639\tvalidate-mlogloss:0.78535                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.7873913799930944                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.7, 'colsample_bynode': 0.9, 'colsample_bytree': 0.9, 'eval_metric': 'mlogloss', 'gamma': 6, 'lambda': 10, 'learning_rate': 0.08352618709439551, 'max_delta_step': 9, 'max_depth': 9, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.6, 'tree_method': 'hist'}, 'k_neighbors': 20, 'num_boost_round': 1426, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.06751\tvalidate-mlogloss:1.07018                              \n",
      "[200]\ttrain-mlogloss:0.69082\tvalidate-mlogloss:0.79939                            \n",
      "[400]\ttrain-mlogloss:0.68759\tvalidate-mlogloss:0.79759                            \n",
      "[600]\ttrain-mlogloss:0.68635\tvalidate-mlogloss:0.79665                            \n",
      "[800]\ttrain-mlogloss:0.68541\tvalidate-mlogloss:0.79609                            \n",
      "[1000]\ttrain-mlogloss:0.68467\tvalidate-mlogloss:0.79591                           \n",
      "[1200]\ttrain-mlogloss:0.68406\tvalidate-mlogloss:0.79534                           \n",
      "[1400]\ttrain-mlogloss:0.68363\tvalidate-mlogloss:0.79529                           \n",
      "[1425]\ttrain-mlogloss:0.68360\tvalidate-mlogloss:0.79532                           \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.06627\tvalidate-mlogloss:1.06960                              \n",
      "[200]\ttrain-mlogloss:0.69184\tvalidate-mlogloss:0.78443                            \n",
      "[400]\ttrain-mlogloss:0.68901\tvalidate-mlogloss:0.78469                            \n",
      "[600]\ttrain-mlogloss:0.68765\tvalidate-mlogloss:0.78411                            \n",
      "[800]\ttrain-mlogloss:0.68676\tvalidate-mlogloss:0.78445                            \n",
      "[972]\ttrain-mlogloss:0.68622\tvalidate-mlogloss:0.78421                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.7897661117360548                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.7, 'colsample_bynode': 1.0, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 6, 'lambda': 10, 'learning_rate': 0.10951877682343943, 'max_delta_step': 9, 'max_depth': 9, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.7, 'tree_method': 'hist'}, 'k_neighbors': 20, 'num_boost_round': 1998, 'num_feat': 80, 'step': 0.2}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.05653\tvalidate-mlogloss:1.06265                              \n",
      "[200]\ttrain-mlogloss:0.68343\tvalidate-mlogloss:0.79907                            \n",
      "[400]\ttrain-mlogloss:0.68065\tvalidate-mlogloss:0.79781                            \n",
      "[600]\ttrain-mlogloss:0.67901\tvalidate-mlogloss:0.79665                            \n",
      "[800]\ttrain-mlogloss:0.67842\tvalidate-mlogloss:0.79629                            \n",
      "[1000]\ttrain-mlogloss:0.67787\tvalidate-mlogloss:0.79615                           \n",
      "[1200]\ttrain-mlogloss:0.67736\tvalidate-mlogloss:0.79659                           \n",
      "[1375]\ttrain-mlogloss:0.67701\tvalidate-mlogloss:0.79665                           \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.05764\tvalidate-mlogloss:1.06202                              \n",
      "[200]\ttrain-mlogloss:0.68161\tvalidate-mlogloss:0.81311                            \n",
      "[400]\ttrain-mlogloss:0.67940\tvalidate-mlogloss:0.81179                            \n",
      "[491]\ttrain-mlogloss:0.67914\tvalidate-mlogloss:0.81157                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8040937424686501                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.7, 'colsample_bynode': 0.9, 'colsample_bytree': 0.7, 'eval_metric': 'mlogloss', 'gamma': 6, 'lambda': 10, 'learning_rate': 0.08878258407815126, 'max_delta_step': 9, 'max_depth': 9, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.6, 'tree_method': 'hist'}, 'k_neighbors': 20, 'num_boost_round': 1812, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.06579\tvalidate-mlogloss:1.06934                              \n",
      "[200]\ttrain-mlogloss:0.69457\tvalidate-mlogloss:0.79808                            \n",
      "[400]\ttrain-mlogloss:0.69113\tvalidate-mlogloss:0.79859                            \n",
      "[600]\ttrain-mlogloss:0.68964\tvalidate-mlogloss:0.79955                            \n",
      "[676]\ttrain-mlogloss:0.68949\tvalidate-mlogloss:0.79935                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.06645\tvalidate-mlogloss:1.07066                              \n",
      "[200]\ttrain-mlogloss:0.69507\tvalidate-mlogloss:0.78788                            \n",
      "[400]\ttrain-mlogloss:0.69178\tvalidate-mlogloss:0.78629                            \n",
      "[600]\ttrain-mlogloss:0.69041\tvalidate-mlogloss:0.78670                            \n",
      "[800]\ttrain-mlogloss:0.68948\tvalidate-mlogloss:0.78611                            \n",
      "[1000]\ttrain-mlogloss:0.68877\tvalidate-mlogloss:0.78598                           \n",
      "[1200]\ttrain-mlogloss:0.68850\tvalidate-mlogloss:0.78606                           \n",
      "[1400]\ttrain-mlogloss:0.68787\tvalidate-mlogloss:0.78561                           \n",
      "[1600]\ttrain-mlogloss:0.68763\tvalidate-mlogloss:0.78569                           \n",
      "[1800]\ttrain-mlogloss:0.68725\tvalidate-mlogloss:0.78547                           \n",
      "[1811]\ttrain-mlogloss:0.68725\tvalidate-mlogloss:0.78545                           \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.7924015390053121                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.7, 'colsample_bynode': 0.6, 'colsample_bytree': 0.5, 'eval_metric': 'mlogloss', 'gamma': 6, 'lambda': 10, 'learning_rate': 0.1316629217721963, 'max_delta_step': 9, 'max_depth': 10, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.6, 'tree_method': 'hist'}, 'k_neighbors': 20, 'num_boost_round': 1592, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.05656\tvalidate-mlogloss:1.06069                              \n",
      "[200]\ttrain-mlogloss:0.70283\tvalidate-mlogloss:0.80754                            \n",
      "[400]\ttrain-mlogloss:0.69967\tvalidate-mlogloss:0.81008                            \n",
      "[600]\ttrain-mlogloss:0.69824\tvalidate-mlogloss:0.81216                            \n",
      "[609]\ttrain-mlogloss:0.69814\tvalidate-mlogloss:0.81193                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.05448\tvalidate-mlogloss:1.05931                              \n",
      "[200]\ttrain-mlogloss:0.70280\tvalidate-mlogloss:0.79111                            \n",
      "[400]\ttrain-mlogloss:0.70002\tvalidate-mlogloss:0.78959                            \n",
      "[600]\ttrain-mlogloss:0.69863\tvalidate-mlogloss:0.78951                            \n",
      "[800]\ttrain-mlogloss:0.69764\tvalidate-mlogloss:0.78887                            \n",
      "[934]\ttrain-mlogloss:0.69701\tvalidate-mlogloss:0.78885                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8003693027156197                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.7, 'colsample_bynode': 0.5, 'colsample_bytree': 0.6, 'eval_metric': 'mlogloss', 'gamma': 4, 'lambda': 100, 'learning_rate': 0.16659447968435553, 'max_delta_step': 9, 'max_depth': 11, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.6, 'tree_method': 'hist'}, 'k_neighbors': 20, 'num_boost_round': 1958, 'num_feat': 50, 'step': 0.2}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.05534\tvalidate-mlogloss:1.05897                              \n",
      "[200]\ttrain-mlogloss:0.70378\tvalidate-mlogloss:0.87865                            \n",
      "[400]\ttrain-mlogloss:0.69921\tvalidate-mlogloss:0.88661                            \n",
      "[454]\ttrain-mlogloss:0.69852\tvalidate-mlogloss:0.88649                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.05435\tvalidate-mlogloss:1.05690                              \n",
      "[200]\ttrain-mlogloss:0.69893\tvalidate-mlogloss:0.82715                            \n",
      "[400]\ttrain-mlogloss:0.69540\tvalidate-mlogloss:0.82572                            \n",
      "[492]\ttrain-mlogloss:0.69416\tvalidate-mlogloss:0.82476                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8555681198485825                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.7, 'colsample_bynode': 0.9, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 6, 'lambda': 10, 'learning_rate': 0.05652475555510092, 'max_delta_step': 9, 'max_depth': 9, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.7, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 884, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.07746\tvalidate-mlogloss:1.07965                              \n",
      "[200]\ttrain-mlogloss:0.68987\tvalidate-mlogloss:0.79264                            \n",
      "[400]\ttrain-mlogloss:0.68675\tvalidate-mlogloss:0.79235                            \n",
      "[600]\ttrain-mlogloss:0.68461\tvalidate-mlogloss:0.79129                            \n",
      "[800]\ttrain-mlogloss:0.68337\tvalidate-mlogloss:0.79066                            \n",
      "[883]\ttrain-mlogloss:0.68313\tvalidate-mlogloss:0.79060                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.07708\tvalidate-mlogloss:1.07962                              \n",
      "[200]\ttrain-mlogloss:0.68961\tvalidate-mlogloss:0.78417                            \n",
      "[400]\ttrain-mlogloss:0.68602\tvalidate-mlogloss:0.78306                            \n",
      "[600]\ttrain-mlogloss:0.68485\tvalidate-mlogloss:0.78260                            \n",
      "[800]\ttrain-mlogloss:0.68398\tvalidate-mlogloss:0.78227                            \n",
      "[883]\ttrain-mlogloss:0.68383\tvalidate-mlogloss:0.78246                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.7865325616522583                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.7, 'colsample_bynode': 0.9, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 6, 'lambda': 10, 'learning_rate': 0.05052342611430378, 'max_delta_step': 7, 'max_depth': 9, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.7, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 508, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.07966\tvalidate-mlogloss:1.08163                              \n",
      "[200]\ttrain-mlogloss:0.69094\tvalidate-mlogloss:0.79326                            \n",
      "[400]\ttrain-mlogloss:0.68664\tvalidate-mlogloss:0.79246                            \n",
      "[507]\ttrain-mlogloss:0.68524\tvalidate-mlogloss:0.79159                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.07933\tvalidate-mlogloss:1.08160                              \n",
      "[200]\ttrain-mlogloss:0.69132\tvalidate-mlogloss:0.78505                            \n",
      "[400]\ttrain-mlogloss:0.68663\tvalidate-mlogloss:0.78328                            \n",
      "[507]\ttrain-mlogloss:0.68554\tvalidate-mlogloss:0.78283                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.7872098604620856                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.7, 'colsample_bynode': 0.5, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 5, 'lambda': 100, 'learning_rate': 0.013568709537177696, 'max_delta_step': 7, 'max_depth': 10, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.7, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1520, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.09410\tvalidate-mlogloss:1.09445                              \n",
      "[200]\ttrain-mlogloss:0.79546\tvalidate-mlogloss:0.84279                            \n",
      "[400]\ttrain-mlogloss:0.73910\tvalidate-mlogloss:0.81355                            \n",
      "[600]\ttrain-mlogloss:0.71775\tvalidate-mlogloss:0.80544                            \n",
      "[800]\ttrain-mlogloss:0.70731\tvalidate-mlogloss:0.80290                            \n",
      "[1000]\ttrain-mlogloss:0.70208\tvalidate-mlogloss:0.80062                           \n",
      "[1200]\ttrain-mlogloss:0.69980\tvalidate-mlogloss:0.79994                           \n",
      "[1400]\ttrain-mlogloss:0.69834\tvalidate-mlogloss:0.79997                           \n",
      "[1519]\ttrain-mlogloss:0.69768\tvalidate-mlogloss:0.79957                           \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.09445\tvalidate-mlogloss:1.09479                              \n",
      "[200]\ttrain-mlogloss:0.79127\tvalidate-mlogloss:0.83759                            \n",
      "[400]\ttrain-mlogloss:0.73714\tvalidate-mlogloss:0.80536                            \n",
      "[600]\ttrain-mlogloss:0.71709\tvalidate-mlogloss:0.79638                            \n",
      "[800]\ttrain-mlogloss:0.70737\tvalidate-mlogloss:0.79238                            \n",
      "[1000]\ttrain-mlogloss:0.70291\tvalidate-mlogloss:0.79076                           \n",
      "[1200]\ttrain-mlogloss:0.70082\tvalidate-mlogloss:0.79000                           \n",
      "[1400]\ttrain-mlogloss:0.69970\tvalidate-mlogloss:0.78946                           \n",
      "[1519]\ttrain-mlogloss:0.69914\tvalidate-mlogloss:0.78918                           \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.794376771816705                                               \n",
      "{'booster_params': {'alpha': 1, 'booster': 'gbtree', 'colsample_bylevel': 0.7, 'colsample_bynode': 0.6, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 4, 'lambda': 10, 'learning_rate': 0.05125157992220619, 'max_delta_step': 7, 'max_depth': 9, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.7, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1008, 'num_feat': 50, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.08043\tvalidate-mlogloss:1.08347                              \n",
      "[200]\ttrain-mlogloss:0.66743\tvalidate-mlogloss:0.83987                            \n",
      "[400]\ttrain-mlogloss:0.65938\tvalidate-mlogloss:0.84785                            \n",
      "[491]\ttrain-mlogloss:0.65753\tvalidate-mlogloss:0.84686                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.08069\tvalidate-mlogloss:1.08328                              \n",
      "[200]\ttrain-mlogloss:0.67064\tvalidate-mlogloss:0.80979                            \n",
      "[400]\ttrain-mlogloss:0.66306\tvalidate-mlogloss:0.80955                            \n",
      "[550]\ttrain-mlogloss:0.66067\tvalidate-mlogloss:0.80843                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8276439476748418                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.7, 'colsample_bynode': 0.9, 'colsample_bytree': 0.5, 'eval_metric': 'mlogloss', 'gamma': 6, 'lambda': 10, 'learning_rate': 0.051054984278929065, 'max_delta_step': 7, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.7, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1305, 'num_feat': 80, 'step': 0.2}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.08144\tvalidate-mlogloss:1.08319                              \n",
      "[200]\ttrain-mlogloss:0.69961\tvalidate-mlogloss:0.79827                            \n",
      "[400]\ttrain-mlogloss:0.69320\tvalidate-mlogloss:0.79516                            \n",
      "[600]\ttrain-mlogloss:0.69154\tvalidate-mlogloss:0.79469                            \n",
      "[800]\ttrain-mlogloss:0.69031\tvalidate-mlogloss:0.79468                            \n",
      "[1000]\ttrain-mlogloss:0.68982\tvalidate-mlogloss:0.79455                           \n",
      "[1103]\ttrain-mlogloss:0.68962\tvalidate-mlogloss:0.79456                           \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.08176\tvalidate-mlogloss:1.08362                              \n",
      "[200]\ttrain-mlogloss:0.69774\tvalidate-mlogloss:0.80956                            \n",
      "[400]\ttrain-mlogloss:0.69091\tvalidate-mlogloss:0.81143                            \n",
      "[599]\ttrain-mlogloss:0.68899\tvalidate-mlogloss:0.81342                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8039908788915955                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 1.0, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 6, 'lambda': 10, 'learning_rate': 0.0035351590213044196, 'max_delta_step': 7, 'max_depth': 9, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.7, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 806, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.09720\tvalidate-mlogloss:1.09733                              \n",
      "[200]\ttrain-mlogloss:0.91428\tvalidate-mlogloss:0.93758                            \n",
      "[400]\ttrain-mlogloss:0.82753\tvalidate-mlogloss:0.86746                            \n",
      "[600]\ttrain-mlogloss:0.78077\tvalidate-mlogloss:0.83403                            \n",
      "[800]\ttrain-mlogloss:0.75244\tvalidate-mlogloss:0.81681                            \n",
      "[805]\ttrain-mlogloss:0.75191\tvalidate-mlogloss:0.81651                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.09721\tvalidate-mlogloss:1.09739                              \n",
      "[200]\ttrain-mlogloss:0.91089\tvalidate-mlogloss:0.93508                            \n",
      "[400]\ttrain-mlogloss:0.82484\tvalidate-mlogloss:0.86499                            \n",
      "[600]\ttrain-mlogloss:0.77830\tvalidate-mlogloss:0.83082                            \n",
      "[800]\ttrain-mlogloss:0.75019\tvalidate-mlogloss:0.81247                            \n",
      "[805]\ttrain-mlogloss:0.74964\tvalidate-mlogloss:0.81211                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8143103709039206                                              \n",
      "{'booster_params': {'alpha': 100, 'booster': 'gbtree', 'colsample_bylevel': 0.7, 'colsample_bynode': 0.9, 'colsample_bytree': 0.7, 'eval_metric': 'mlogloss', 'gamma': 2, 'lambda': 100, 'learning_rate': 0.03162107436003915, 'max_delta_step': 7, 'max_depth': 11, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.7, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1720, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.08918\tvalidate-mlogloss:1.08978                              \n",
      "[200]\ttrain-mlogloss:0.75385\tvalidate-mlogloss:0.81907                            \n",
      "[400]\ttrain-mlogloss:0.72173\tvalidate-mlogloss:0.81235                            \n",
      "[600]\ttrain-mlogloss:0.71007\tvalidate-mlogloss:0.81119                            \n",
      "[800]\ttrain-mlogloss:0.70563\tvalidate-mlogloss:0.81017                            \n",
      "[1000]\ttrain-mlogloss:0.70356\tvalidate-mlogloss:0.80943                           \n",
      "[1200]\ttrain-mlogloss:0.70227\tvalidate-mlogloss:0.80920                           \n",
      "[1400]\ttrain-mlogloss:0.70125\tvalidate-mlogloss:0.80926                           \n",
      "[1600]\ttrain-mlogloss:0.70043\tvalidate-mlogloss:0.80888                           \n",
      "[1719]\ttrain-mlogloss:0.69998\tvalidate-mlogloss:0.80878                           \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.08928\tvalidate-mlogloss:1.08992                              \n",
      "[200]\ttrain-mlogloss:0.75236\tvalidate-mlogloss:0.81783                            \n",
      "[400]\ttrain-mlogloss:0.72137\tvalidate-mlogloss:0.80341                            \n",
      "[600]\ttrain-mlogloss:0.71071\tvalidate-mlogloss:0.79848                            \n",
      "[800]\ttrain-mlogloss:0.70686\tvalidate-mlogloss:0.79661                            \n",
      "[1000]\ttrain-mlogloss:0.70503\tvalidate-mlogloss:0.79578                           \n",
      "[1200]\ttrain-mlogloss:0.70377\tvalidate-mlogloss:0.79526                           \n",
      "[1400]\ttrain-mlogloss:0.70288\tvalidate-mlogloss:0.79494                           \n",
      "[1600]\ttrain-mlogloss:0.70218\tvalidate-mlogloss:0.79456                           \n",
      "[1719]\ttrain-mlogloss:0.70178\tvalidate-mlogloss:0.79432                           \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.801550123486864                                               \n",
      "{'booster_params': {'alpha': 1, 'booster': 'gbtree', 'colsample_bylevel': 0.5, 'colsample_bynode': 0.5, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 3, 'lambda': 10, 'learning_rate': 0.09563913672112528, 'max_delta_step': 3, 'max_depth': 9, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.7, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 580, 'num_feat': 50, 'step': 0.1}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.06435\tvalidate-mlogloss:1.08416                              \n",
      "[200]\ttrain-mlogloss:0.63394\tvalidate-mlogloss:0.96385                            \n",
      "[400]\ttrain-mlogloss:0.62820\tvalidate-mlogloss:0.96988                            \n",
      "[445]\ttrain-mlogloss:0.62663\tvalidate-mlogloss:0.96942                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.06496\tvalidate-mlogloss:1.07121                              \n",
      "[200]\ttrain-mlogloss:0.63764\tvalidate-mlogloss:0.88956                            \n",
      "[400]\ttrain-mlogloss:0.63096\tvalidate-mlogloss:0.91256                            \n",
      "[441]\ttrain-mlogloss:0.63024\tvalidate-mlogloss:0.91191                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.9406499343303092                                              \n",
      "{'booster_params': {'alpha': 100, 'booster': 'gbtree', 'colsample_bylevel': 0.7, 'colsample_bynode': 0.8, 'colsample_bytree': 0.5, 'eval_metric': 'mlogloss', 'gamma': 6, 'lambda': 10, 'learning_rate': 0.13058460998781923, 'max_delta_step': 1, 'max_depth': 8, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.7, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 999, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.06517\tvalidate-mlogloss:1.06710                              \n",
      "[200]\ttrain-mlogloss:0.73284\tvalidate-mlogloss:0.81661                            \n",
      "[400]\ttrain-mlogloss:0.73044\tvalidate-mlogloss:0.81681                            \n",
      "[585]\ttrain-mlogloss:0.72966\tvalidate-mlogloss:0.81618                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.06424\tvalidate-mlogloss:1.06625                              \n",
      "[200]\ttrain-mlogloss:0.73070\tvalidate-mlogloss:0.80286                            \n",
      "[400]\ttrain-mlogloss:0.72851\tvalidate-mlogloss:0.80151                            \n",
      "[600]\ttrain-mlogloss:0.72771\tvalidate-mlogloss:0.80093                            \n",
      "[800]\ttrain-mlogloss:0.72714\tvalidate-mlogloss:0.80065                            \n",
      "[998]\ttrain-mlogloss:0.72642\tvalidate-mlogloss:0.80022                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8081985233684077                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 0.6, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 5, 'lambda': 1, 'learning_rate': 0.1566125353837196, 'max_delta_step': 5, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.7, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1197, 'num_feat': 80, 'step': 0.2}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.04133\tvalidate-mlogloss:1.04638                              \n",
      "[200]\ttrain-mlogloss:0.67092\tvalidate-mlogloss:0.79194                            \n",
      "[400]\ttrain-mlogloss:0.66761\tvalidate-mlogloss:0.79604                            \n",
      "[600]\ttrain-mlogloss:0.66592\tvalidate-mlogloss:0.79525                            \n",
      "[698]\ttrain-mlogloss:0.66544\tvalidate-mlogloss:0.79495                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.04224\tvalidate-mlogloss:1.04747                              \n",
      "[200]\ttrain-mlogloss:0.67045\tvalidate-mlogloss:0.79756                            \n",
      "[400]\ttrain-mlogloss:0.66715\tvalidate-mlogloss:0.79637                            \n",
      "[600]\ttrain-mlogloss:0.66519\tvalidate-mlogloss:0.79585                            \n",
      "[800]\ttrain-mlogloss:0.66414\tvalidate-mlogloss:0.79529                            \n",
      "[1000]\ttrain-mlogloss:0.66330\tvalidate-mlogloss:0.79520                           \n",
      "[1196]\ttrain-mlogloss:0.66278\tvalidate-mlogloss:0.79509                           \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.7950143123657205                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.8, 'colsample_bynode': 1.0, 'colsample_bytree': 1.0, 'eval_metric': 'mlogloss', 'gamma': 4, 'lambda': 10, 'learning_rate': 0.018117334898620475, 'max_delta_step': 4, 'max_depth': 10, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 15, 'num_boost_round': 1633, 'num_feat': 80, 'step': 0.1}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.09159\tvalidate-mlogloss:1.09361                              \n",
      "[200]\ttrain-mlogloss:0.72029\tvalidate-mlogloss:0.85718                            \n",
      "[400]\ttrain-mlogloss:0.67593\tvalidate-mlogloss:0.87092                            \n",
      "[600]\ttrain-mlogloss:0.66030\tvalidate-mlogloss:0.88168                            \n",
      "[675]\ttrain-mlogloss:0.65751\tvalidate-mlogloss:0.88552                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.09145\tvalidate-mlogloss:1.09221                              \n",
      "[200]\ttrain-mlogloss:0.72302\tvalidate-mlogloss:0.82593                            \n",
      "[400]\ttrain-mlogloss:0.68120\tvalidate-mlogloss:0.81984                            \n",
      "[600]\ttrain-mlogloss:0.66818\tvalidate-mlogloss:0.82955                            \n",
      "[666]\ttrain-mlogloss:0.66624\tvalidate-mlogloss:0.83047                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8579950501344922                                              \n",
      "{'booster_params': {'alpha': 100, 'booster': 'gbtree', 'colsample_bylevel': 0.7, 'colsample_bynode': 0.9, 'colsample_bytree': 0.6, 'eval_metric': 'mlogloss', 'gamma': 1, 'lambda': 100, 'learning_rate': 0.10281749841263216, 'max_delta_step': 6, 'max_depth': 9, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.8, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 884, 'num_feat': 60, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.07121\tvalidate-mlogloss:1.07200                              \n",
      "[200]\ttrain-mlogloss:0.70085\tvalidate-mlogloss:0.85090                            \n",
      "[400]\ttrain-mlogloss:0.69018\tvalidate-mlogloss:0.85625                            \n",
      "[513]\ttrain-mlogloss:0.68820\tvalidate-mlogloss:0.85608                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.06972\tvalidate-mlogloss:1.07135                              \n",
      "[200]\ttrain-mlogloss:0.70058\tvalidate-mlogloss:0.82489                            \n",
      "[400]\ttrain-mlogloss:0.68989\tvalidate-mlogloss:0.82757                            \n",
      "[600]\ttrain-mlogloss:0.68662\tvalidate-mlogloss:0.82782                            \n",
      "[610]\ttrain-mlogloss:0.68656\tvalidate-mlogloss:0.82775                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8418582629115675                                              \n",
      "{'booster_params': {'alpha': 1, 'booster': 'gbtree', 'colsample_bylevel': 0.5, 'colsample_bynode': 0.8, 'colsample_bytree': 0.7, 'eval_metric': 'mlogloss', 'gamma': 6, 'lambda': 10, 'learning_rate': 0.043051445176650396, 'max_delta_step': 7, 'max_depth': 3, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.7, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1465, 'num_feat': 50, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.08950\tvalidate-mlogloss:1.08925                              \n",
      "[200]\ttrain-mlogloss:0.77626\tvalidate-mlogloss:0.84844                            \n",
      "[400]\ttrain-mlogloss:0.73719\tvalidate-mlogloss:0.83609                            \n",
      "[600]\ttrain-mlogloss:0.72616\tvalidate-mlogloss:0.83927                            \n",
      "[753]\ttrain-mlogloss:0.72346\tvalidate-mlogloss:0.84321                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.08973\tvalidate-mlogloss:1.08959                              \n",
      "[200]\ttrain-mlogloss:0.77438\tvalidate-mlogloss:0.84641                            \n",
      "[400]\ttrain-mlogloss:0.73521\tvalidate-mlogloss:0.82516                            \n",
      "[600]\ttrain-mlogloss:0.72367\tvalidate-mlogloss:0.82270                            \n",
      "[800]\ttrain-mlogloss:0.71977\tvalidate-mlogloss:0.82200                            \n",
      "[1000]\ttrain-mlogloss:0.71838\tvalidate-mlogloss:0.82260                           \n",
      "[1072]\ttrain-mlogloss:0.71780\tvalidate-mlogloss:0.82228                           \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8327666417134099                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.7, 'colsample_bynode': 0.9, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 3, 'lambda': 1, 'learning_rate': 0.0681491447033812, 'max_delta_step': 3, 'max_depth': 4, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.7, 'tree_method': 'hist'}, 'k_neighbors': 30, 'num_boost_round': 1164, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.07974\tvalidate-mlogloss:1.08043                              \n",
      "[200]\ttrain-mlogloss:0.70281\tvalidate-mlogloss:0.81017                            \n",
      "[400]\ttrain-mlogloss:0.67702\tvalidate-mlogloss:0.81536                            \n",
      "[588]\ttrain-mlogloss:0.66895\tvalidate-mlogloss:0.81660                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.07855\tvalidate-mlogloss:1.08011                              \n",
      "[200]\ttrain-mlogloss:0.70056\tvalidate-mlogloss:0.79355                            \n",
      "[400]\ttrain-mlogloss:0.67398\tvalidate-mlogloss:0.78940                            \n",
      "[600]\ttrain-mlogloss:0.66684\tvalidate-mlogloss:0.79076                            \n",
      "[727]\ttrain-mlogloss:0.66417\tvalidate-mlogloss:0.79042                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.803516266965538                                               \n",
      "{'booster_params': {'alpha': 100, 'booster': 'gbtree', 'colsample_bylevel': 0.9, 'colsample_bynode': 0.9, 'colsample_bytree': 1.0, 'eval_metric': 'mlogloss', 'gamma': 2, 'lambda': 10, 'learning_rate': 0.0023565740782183486, 'max_delta_step': 5, 'max_depth': 6, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 15, 'num_boost_round': 1466, 'num_feat': 60, 'step': 0.1}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.09794\tvalidate-mlogloss:1.09803                              \n",
      "[200]\ttrain-mlogloss:0.99224\tvalidate-mlogloss:1.00165                            \n",
      "[400]\ttrain-mlogloss:0.92875\tvalidate-mlogloss:0.95197                            \n",
      "[600]\ttrain-mlogloss:0.88711\tvalidate-mlogloss:0.92402                            \n",
      "[800]\ttrain-mlogloss:0.85848\tvalidate-mlogloss:0.90773                            \n",
      "[1000]\ttrain-mlogloss:0.83725\tvalidate-mlogloss:0.89747                           \n",
      "[1200]\ttrain-mlogloss:0.82091\tvalidate-mlogloss:0.89152                           \n",
      "[1400]\ttrain-mlogloss:0.80762\tvalidate-mlogloss:0.88982                           \n",
      "[1465]\ttrain-mlogloss:0.80379\tvalidate-mlogloss:0.88938                           \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.09790\tvalidate-mlogloss:1.09797                              \n",
      "[200]\ttrain-mlogloss:0.99170\tvalidate-mlogloss:1.00200                            \n",
      "[400]\ttrain-mlogloss:0.92850\tvalidate-mlogloss:0.94815                            \n",
      "[600]\ttrain-mlogloss:0.88699\tvalidate-mlogloss:0.91654                            \n",
      "[800]\ttrain-mlogloss:0.85824\tvalidate-mlogloss:0.89678                            \n",
      "[1000]\ttrain-mlogloss:0.83682\tvalidate-mlogloss:0.88642                           \n",
      "[1200]\ttrain-mlogloss:0.81982\tvalidate-mlogloss:0.87842                           \n",
      "[1400]\ttrain-mlogloss:0.80613\tvalidate-mlogloss:0.87405                           \n",
      "[1465]\ttrain-mlogloss:0.80237\tvalidate-mlogloss:0.87297                           \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8811739823455194                                              \n",
      "{'booster_params': {'alpha': 1, 'booster': 'gbtree', 'colsample_bylevel': 0.7, 'colsample_bynode': 0.7, 'colsample_bytree': 0.5, 'eval_metric': 'mlogloss', 'gamma': 6, 'lambda': 100, 'learning_rate': 0.12224306025299697, 'max_delta_step': 1, 'max_depth': 8, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.9, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1345, 'num_feat': 80, 'step': 0.2}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.06375\tvalidate-mlogloss:1.06746                              \n",
      "[200]\ttrain-mlogloss:0.69684\tvalidate-mlogloss:0.79760                            \n",
      "[400]\ttrain-mlogloss:0.69534\tvalidate-mlogloss:0.79706                            \n",
      "[600]\ttrain-mlogloss:0.69487\tvalidate-mlogloss:0.79675                            \n",
      "[800]\ttrain-mlogloss:0.69454\tvalidate-mlogloss:0.79657                            \n",
      "[1000]\ttrain-mlogloss:0.69434\tvalidate-mlogloss:0.79623                           \n",
      "[1200]\ttrain-mlogloss:0.69416\tvalidate-mlogloss:0.79614                           \n",
      "[1344]\ttrain-mlogloss:0.69414\tvalidate-mlogloss:0.79620                           \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.06086\tvalidate-mlogloss:1.06383                              \n",
      "[200]\ttrain-mlogloss:0.69552\tvalidate-mlogloss:0.80511                            \n",
      "[400]\ttrain-mlogloss:0.69387\tvalidate-mlogloss:0.80595                            \n",
      "[500]\ttrain-mlogloss:0.69361\tvalidate-mlogloss:0.80576                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8009743593317467                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.8, 'colsample_bynode': 1.0, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 1, 'lambda': 1, 'learning_rate': 0.021239520998594802, 'max_delta_step': 7, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.8, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1014, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.09022\tvalidate-mlogloss:1.09108                              \n",
      "[200]\ttrain-mlogloss:0.70255\tvalidate-mlogloss:0.79977                            \n",
      "[400]\ttrain-mlogloss:0.64388\tvalidate-mlogloss:0.78618                            \n",
      "[600]\ttrain-mlogloss:0.60797\tvalidate-mlogloss:0.78371                            \n",
      "[800]\ttrain-mlogloss:0.57919\tvalidate-mlogloss:0.78391                            \n",
      "[1000]\ttrain-mlogloss:0.55567\tvalidate-mlogloss:0.78494                           \n",
      "[1013]\ttrain-mlogloss:0.55440\tvalidate-mlogloss:0.78494                           \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.09039\tvalidate-mlogloss:1.09127                              \n",
      "[200]\ttrain-mlogloss:0.69989\tvalidate-mlogloss:0.79418                            \n",
      "[400]\ttrain-mlogloss:0.64436\tvalidate-mlogloss:0.77959                            \n",
      "[600]\ttrain-mlogloss:0.61060\tvalidate-mlogloss:0.77496                            \n",
      "[800]\ttrain-mlogloss:0.58345\tvalidate-mlogloss:0.77368                            \n",
      "[1000]\ttrain-mlogloss:0.56049\tvalidate-mlogloss:0.77310                           \n",
      "[1013]\ttrain-mlogloss:0.55916\tvalidate-mlogloss:0.77307                           \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.7790077492800599                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.8, 'colsample_bynode': 1.0, 'colsample_bytree': 0.6, 'eval_metric': 'mlogloss', 'gamma': 1, 'lambda': 1, 'learning_rate': 0.2284681516619606, 'max_delta_step': 4, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.8, 'tree_method': 'hist'}, 'k_neighbors': 30, 'num_boost_round': 1785, 'num_feat': 70, 'step': 0.1}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.01776\tvalidate-mlogloss:1.03515                              \n",
      "[200]\ttrain-mlogloss:0.49706\tvalidate-mlogloss:0.96507                            \n",
      "[400]\ttrain-mlogloss:0.46364\tvalidate-mlogloss:0.98709                            \n",
      "[422]\ttrain-mlogloss:0.46090\tvalidate-mlogloss:0.98798                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.02078\tvalidate-mlogloss:1.03116                              \n",
      "[200]\ttrain-mlogloss:0.50353\tvalidate-mlogloss:0.94125                            \n",
      "[400]\ttrain-mlogloss:0.47117\tvalidate-mlogloss:0.95280                            \n",
      "[416]\ttrain-mlogloss:0.46982\tvalidate-mlogloss:0.95232                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.9701495455817912                                              \n",
      "{'booster_params': {'alpha': 100, 'booster': 'gbtree', 'colsample_bylevel': 0.8, 'colsample_bynode': 1.0, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 1, 'lambda': 1, 'learning_rate': 0.025404144980170754, 'max_delta_step': 6, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.8, 'tree_method': 'hist'}, 'k_neighbors': 25, 'num_boost_round': 1512, 'num_feat': 60, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.09071\tvalidate-mlogloss:1.09172                              \n",
      "[200]\ttrain-mlogloss:0.75462\tvalidate-mlogloss:0.84096                            \n",
      "[400]\ttrain-mlogloss:0.71752\tvalidate-mlogloss:0.84252                            \n",
      "[600]\ttrain-mlogloss:0.70250\tvalidate-mlogloss:0.85127                            \n",
      "[624]\ttrain-mlogloss:0.70129\tvalidate-mlogloss:0.85239                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.09033\tvalidate-mlogloss:1.09108                              \n",
      "[200]\ttrain-mlogloss:0.75580\tvalidate-mlogloss:0.83205                            \n",
      "[400]\ttrain-mlogloss:0.71755\tvalidate-mlogloss:0.82024                            \n",
      "[600]\ttrain-mlogloss:0.70282\tvalidate-mlogloss:0.81778                            \n",
      "[800]\ttrain-mlogloss:0.69559\tvalidate-mlogloss:0.81778                            \n",
      "[1000]\ttrain-mlogloss:0.69195\tvalidate-mlogloss:0.81850                           \n",
      "[1160]\ttrain-mlogloss:0.69042\tvalidate-mlogloss:0.81898                           \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8356747334230727                                              \n",
      "{'booster_params': {'alpha': 1, 'booster': 'gbtree', 'colsample_bylevel': 0.8, 'colsample_bynode': 1.0, 'colsample_bytree': 0.7, 'eval_metric': 'mlogloss', 'gamma': 1, 'lambda': 1, 'learning_rate': 0.0017844031720439732, 'max_delta_step': 2, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.8, 'tree_method': 'hist'}, 'k_neighbors': 15, 'num_boost_round': 1824, 'num_feat': 80, 'step': 0.2}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.09786\tvalidate-mlogloss:1.09795                              \n",
      "[200]\ttrain-mlogloss:0.98382\tvalidate-mlogloss:1.00128                            \n",
      "[400]\ttrain-mlogloss:0.90727\tvalidate-mlogloss:0.93908                            \n",
      "[600]\ttrain-mlogloss:0.85297\tvalidate-mlogloss:0.89708                            \n",
      "[800]\ttrain-mlogloss:0.81288\tvalidate-mlogloss:0.86788                            \n",
      "[1000]\ttrain-mlogloss:0.78224\tvalidate-mlogloss:0.84756                           \n",
      "[1200]\ttrain-mlogloss:0.75819\tvalidate-mlogloss:0.83327                           \n",
      "[1400]\ttrain-mlogloss:0.73879\tvalidate-mlogloss:0.82304                           \n",
      "[1600]\ttrain-mlogloss:0.72228\tvalidate-mlogloss:0.81502                           \n",
      "[1800]\ttrain-mlogloss:0.70811\tvalidate-mlogloss:0.80883                           \n",
      "[1823]\ttrain-mlogloss:0.70663\tvalidate-mlogloss:0.80823                           \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.09790\tvalidate-mlogloss:1.09800                              \n",
      "[200]\ttrain-mlogloss:0.98286\tvalidate-mlogloss:1.00247                            \n",
      "[400]\ttrain-mlogloss:0.90618\tvalidate-mlogloss:0.94070                            \n",
      "[600]\ttrain-mlogloss:0.85231\tvalidate-mlogloss:0.90035                            \n",
      "[800]\ttrain-mlogloss:0.81234\tvalidate-mlogloss:0.87189                            \n",
      "[1000]\ttrain-mlogloss:0.78183\tvalidate-mlogloss:0.85249                           \n",
      "[1200]\ttrain-mlogloss:0.75781\tvalidate-mlogloss:0.83858                           \n",
      "[1400]\ttrain-mlogloss:0.73809\tvalidate-mlogloss:0.82826                           \n",
      "[1600]\ttrain-mlogloss:0.72150\tvalidate-mlogloss:0.82006                           \n",
      "[1800]\ttrain-mlogloss:0.70719\tvalidate-mlogloss:0.81377                           \n",
      "[1823]\ttrain-mlogloss:0.70569\tvalidate-mlogloss:0.81309                           \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8106602160165288                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.8, 'colsample_bynode': 1.0, 'colsample_bytree': 1.0, 'eval_metric': 'mlogloss', 'gamma': 1, 'lambda': 1, 'learning_rate': 0.06105689738802879, 'max_delta_step': 9, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.8, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1572, 'num_feat': 50, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.07584\tvalidate-mlogloss:1.07805                              \n",
      "[200]\ttrain-mlogloss:0.61708\tvalidate-mlogloss:0.81949                            \n",
      "[400]\ttrain-mlogloss:0.54243\tvalidate-mlogloss:0.83647                            \n",
      "[514]\ttrain-mlogloss:0.51960\tvalidate-mlogloss:0.84184                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.07545\tvalidate-mlogloss:1.07811                              \n",
      "[200]\ttrain-mlogloss:0.61612\tvalidate-mlogloss:0.79799                            \n",
      "[400]\ttrain-mlogloss:0.54767\tvalidate-mlogloss:0.80897                            \n",
      "[590]\ttrain-mlogloss:0.51583\tvalidate-mlogloss:0.81146                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8266504187427732                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.8, 'colsample_bynode': 1.0, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 7, 'lambda': 1, 'learning_rate': 0.18961273915678656, 'max_delta_step': 3, 'max_depth': 3, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.8, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 644, 'num_feat': 70, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.05071\tvalidate-mlogloss:1.05165                              \n",
      "[200]\ttrain-mlogloss:0.71409\tvalidate-mlogloss:0.83055                            \n",
      "[400]\ttrain-mlogloss:0.71141\tvalidate-mlogloss:0.82777                            \n",
      "[600]\ttrain-mlogloss:0.71037\tvalidate-mlogloss:0.83424                            \n",
      "[643]\ttrain-mlogloss:0.70999\tvalidate-mlogloss:0.83375                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.05403\tvalidate-mlogloss:1.05384                              \n",
      "[200]\ttrain-mlogloss:0.71240\tvalidate-mlogloss:0.81810                            \n",
      "[400]\ttrain-mlogloss:0.70946\tvalidate-mlogloss:0.81878                            \n",
      "[600]\ttrain-mlogloss:0.70824\tvalidate-mlogloss:0.81820                            \n",
      "[643]\ttrain-mlogloss:0.70819\tvalidate-mlogloss:0.81803                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8258870065348509                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.8, 'colsample_bynode': 0.8, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 8, 'lambda': 1, 'learning_rate': 0.15311949373053108, 'max_delta_step': 5, 'max_depth': 4, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 1.0, 'tree_method': 'hist'}, 'k_neighbors': 30, 'num_boost_round': 1074, 'num_feat': 80, 'step': 0.1}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.05637\tvalidate-mlogloss:1.05780                              \n",
      "[200]\ttrain-mlogloss:0.71344\tvalidate-mlogloss:0.85994                            \n",
      "[400]\ttrain-mlogloss:0.71344\tvalidate-mlogloss:0.85994                            \n",
      "[458]\ttrain-mlogloss:0.71344\tvalidate-mlogloss:0.85994                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.05274\tvalidate-mlogloss:1.05470                              \n",
      "[200]\ttrain-mlogloss:0.71408\tvalidate-mlogloss:0.81813                            \n",
      "[400]\ttrain-mlogloss:0.71408\tvalidate-mlogloss:0.81813                            \n",
      "[465]\ttrain-mlogloss:0.71408\tvalidate-mlogloss:0.81813                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8390371879812384                                              \n",
      "{'booster_params': {'alpha': 100, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 0.7, 'colsample_bytree': 0.5, 'eval_metric': 'mlogloss', 'gamma': 1, 'lambda': 1, 'learning_rate': 0.021794664432070746, 'max_delta_step': 1, 'max_depth': 6, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.9, 'tree_method': 'hist'}, 'k_neighbors': 25, 'num_boost_round': 518, 'num_feat': 60, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.09241\tvalidate-mlogloss:1.09292                              \n",
      "[200]\ttrain-mlogloss:0.77643\tvalidate-mlogloss:0.85267                            \n",
      "[400]\ttrain-mlogloss:0.73358\tvalidate-mlogloss:0.85041                            \n",
      "[517]\ttrain-mlogloss:0.72191\tvalidate-mlogloss:0.84781                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.09246\tvalidate-mlogloss:1.09296                              \n",
      "[200]\ttrain-mlogloss:0.77959\tvalidate-mlogloss:0.84458                            \n",
      "[400]\ttrain-mlogloss:0.73496\tvalidate-mlogloss:0.82657                            \n",
      "[517]\ttrain-mlogloss:0.72288\tvalidate-mlogloss:0.82282                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.835316247958477                                               \n",
      "{'booster_params': {'alpha': 1, 'booster': 'gbtree', 'colsample_bylevel': 0.6, 'colsample_bynode': 1.0, 'colsample_bytree': 0.6, 'eval_metric': 'mlogloss', 'gamma': 3, 'lambda': 1, 'learning_rate': 0.2729475638676547, 'max_delta_step': 7, 'max_depth': 11, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 15, 'num_boost_round': 1014, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:0.98743\tvalidate-mlogloss:1.01681                              \n",
      "[200]\ttrain-mlogloss:0.56159\tvalidate-mlogloss:0.80873                            \n",
      "[400]\ttrain-mlogloss:0.55230\tvalidate-mlogloss:0.81522                            \n",
      "[527]\ttrain-mlogloss:0.54742\tvalidate-mlogloss:0.81756                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:0.99130\tvalidate-mlogloss:1.01505                              \n",
      "[200]\ttrain-mlogloss:0.56453\tvalidate-mlogloss:0.78502                            \n",
      "[400]\ttrain-mlogloss:0.55704\tvalidate-mlogloss:0.78576                            \n",
      "[600]\ttrain-mlogloss:0.55252\tvalidate-mlogloss:0.78603                            \n",
      "[607]\ttrain-mlogloss:0.55217\tvalidate-mlogloss:0.78602                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.801768251564014                                               \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.5, 'colsample_bynode': 0.5, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 2, 'lambda': 1, 'learning_rate': 0.037505454903810964, 'max_delta_step': 2, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.8, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1427, 'num_feat': 70, 'step': 0.2}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.08674\tvalidate-mlogloss:1.08914                              \n",
      "[200]\ttrain-mlogloss:0.67997\tvalidate-mlogloss:0.83322                            \n",
      "[400]\ttrain-mlogloss:0.63402\tvalidate-mlogloss:0.82824                            \n",
      "[600]\ttrain-mlogloss:0.61850\tvalidate-mlogloss:0.83019                            \n",
      "[711]\ttrain-mlogloss:0.61418\tvalidate-mlogloss:0.83046                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.08618\tvalidate-mlogloss:1.08768                              \n",
      "[200]\ttrain-mlogloss:0.68234\tvalidate-mlogloss:0.82106                            \n",
      "[400]\ttrain-mlogloss:0.63956\tvalidate-mlogloss:0.83866                            \n",
      "[600]\ttrain-mlogloss:0.62557\tvalidate-mlogloss:0.84381                            \n",
      "[612]\ttrain-mlogloss:0.62511\tvalidate-mlogloss:0.84423                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8373424852075517                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.8, 'colsample_bynode': 0.6, 'colsample_bytree': 1.0, 'eval_metric': 'mlogloss', 'gamma': 5, 'lambda': 1, 'learning_rate': 0.09689423566383105, 'max_delta_step': 9, 'max_depth': 8, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 1.0, 'tree_method': 'hist'}, 'k_neighbors': 25, 'num_boost_round': 1391, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.06101\tvalidate-mlogloss:1.06646                              \n",
      "[200]\ttrain-mlogloss:0.66925\tvalidate-mlogloss:0.80283                            \n",
      "[400]\ttrain-mlogloss:0.66925\tvalidate-mlogloss:0.80283                            \n",
      "[600]\ttrain-mlogloss:0.66925\tvalidate-mlogloss:0.80283                            \n",
      "[667]\ttrain-mlogloss:0.66925\tvalidate-mlogloss:0.80283                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.06036\tvalidate-mlogloss:1.06470                              \n",
      "[200]\ttrain-mlogloss:0.67000\tvalidate-mlogloss:0.78474                            \n",
      "[400]\ttrain-mlogloss:0.67000\tvalidate-mlogloss:0.78474                            \n",
      "[491]\ttrain-mlogloss:0.67000\tvalidate-mlogloss:0.78474                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.7937808649655831                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.9, 'colsample_bynode': 1.0, 'colsample_bytree': 0.9, 'eval_metric': 'mlogloss', 'gamma': 0, 'lambda': 100, 'learning_rate': 0.012862783162271359, 'max_delta_step': 4, 'max_depth': 10, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.8, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1424, 'num_feat': 50, 'step': 0.1}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.09395\tvalidate-mlogloss:1.09442                              \n",
      "[200]\ttrain-mlogloss:0.75070\tvalidate-mlogloss:0.88669                            \n",
      "[400]\ttrain-mlogloss:0.67151\tvalidate-mlogloss:0.90273                            \n",
      "[575]\ttrain-mlogloss:0.63325\tvalidate-mlogloss:0.91780                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.09388\tvalidate-mlogloss:1.09439                              \n",
      "[200]\ttrain-mlogloss:0.75141\tvalidate-mlogloss:0.84814                            \n",
      "[400]\ttrain-mlogloss:0.67346\tvalidate-mlogloss:0.82597                            \n",
      "[600]\ttrain-mlogloss:0.63043\tvalidate-mlogloss:0.82881                            \n",
      "[800]\ttrain-mlogloss:0.59795\tvalidate-mlogloss:0.83619                            \n",
      "[856]\ttrain-mlogloss:0.59031\tvalidate-mlogloss:0.83710                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8774860476038767                                              \n",
      "{'booster_params': {'alpha': 100, 'booster': 'gbtree', 'colsample_bylevel': 0.6, 'colsample_bynode': 0.8, 'colsample_bytree': 0.7, 'eval_metric': 'mlogloss', 'gamma': 7, 'lambda': 1, 'learning_rate': 0.17626798183644166, 'max_delta_step': 6, 'max_depth': 5, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.9, 'tree_method': 'hist'}, 'k_neighbors': 30, 'num_boost_round': 1088, 'num_feat': 60, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.05762\tvalidate-mlogloss:1.05811                              \n",
      "[200]\ttrain-mlogloss:0.73046\tvalidate-mlogloss:0.81985                            \n",
      "[400]\ttrain-mlogloss:0.72931\tvalidate-mlogloss:0.82249                            \n",
      "[481]\ttrain-mlogloss:0.72931\tvalidate-mlogloss:0.82249                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.05694\tvalidate-mlogloss:1.05974                              \n",
      "[200]\ttrain-mlogloss:0.73160\tvalidate-mlogloss:0.82337                            \n",
      "[400]\ttrain-mlogloss:0.73077\tvalidate-mlogloss:0.82277                            \n",
      "[506]\ttrain-mlogloss:0.73047\tvalidate-mlogloss:0.82244                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8224679930994713                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 0.7, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 4, 'lambda': 1, 'learning_rate': 0.07627895415090738, 'max_delta_step': 9, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 638, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.07188\tvalidate-mlogloss:1.07437                              \n",
      "[200]\ttrain-mlogloss:0.66895\tvalidate-mlogloss:0.78972                            \n",
      "[400]\ttrain-mlogloss:0.66021\tvalidate-mlogloss:0.78969                            \n",
      "[600]\ttrain-mlogloss:0.65645\tvalidate-mlogloss:0.78885                            \n",
      "[637]\ttrain-mlogloss:0.65594\tvalidate-mlogloss:0.78874                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.07215\tvalidate-mlogloss:1.07557                              \n",
      "[200]\ttrain-mlogloss:0.67190\tvalidate-mlogloss:0.78373                            \n",
      "[400]\ttrain-mlogloss:0.66493\tvalidate-mlogloss:0.78241                            \n",
      "[600]\ttrain-mlogloss:0.66049\tvalidate-mlogloss:0.78179                            \n",
      "[637]\ttrain-mlogloss:0.66004\tvalidate-mlogloss:0.78147                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.7851043216543907                                              \n",
      "{'booster_params': {'alpha': 1, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 0.7, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 4, 'lambda': 1, 'learning_rate': 0.07249902617085564, 'max_delta_step': 8, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 20, 'num_boost_round': 907, 'num_feat': 70, 'step': 0.2}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.07087\tvalidate-mlogloss:1.07747                              \n",
      "[200]\ttrain-mlogloss:0.64415\tvalidate-mlogloss:0.86691                            \n",
      "[400]\ttrain-mlogloss:0.63346\tvalidate-mlogloss:0.87768                            \n",
      "[486]\ttrain-mlogloss:0.63127\tvalidate-mlogloss:0.88157                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.07217\tvalidate-mlogloss:1.07587                              \n",
      "[200]\ttrain-mlogloss:0.64834\tvalidate-mlogloss:0.81054                            \n",
      "[400]\ttrain-mlogloss:0.63926\tvalidate-mlogloss:0.81095                            \n",
      "[494]\ttrain-mlogloss:0.63629\tvalidate-mlogloss:0.81008                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8458200755214198                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 0.7, 'colsample_bytree': 0.9, 'eval_metric': 'mlogloss', 'gamma': 4, 'lambda': 1, 'learning_rate': 0.09025935863913469, 'max_delta_step': 7, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 15, 'num_boost_round': 866, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.06505\tvalidate-mlogloss:1.06830                              \n",
      "[200]\ttrain-mlogloss:0.66413\tvalidate-mlogloss:0.79103                            \n",
      "[400]\ttrain-mlogloss:0.65600\tvalidate-mlogloss:0.79006                            \n",
      "[600]\ttrain-mlogloss:0.65197\tvalidate-mlogloss:0.79001                            \n",
      "[800]\ttrain-mlogloss:0.64988\tvalidate-mlogloss:0.79047                            \n",
      "[865]\ttrain-mlogloss:0.64913\tvalidate-mlogloss:0.79047                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.06423\tvalidate-mlogloss:1.06800                              \n",
      "[200]\ttrain-mlogloss:0.66864\tvalidate-mlogloss:0.78347                            \n",
      "[400]\ttrain-mlogloss:0.66152\tvalidate-mlogloss:0.78313                            \n",
      "[600]\ttrain-mlogloss:0.65767\tvalidate-mlogloss:0.78212                            \n",
      "[800]\ttrain-mlogloss:0.65525\tvalidate-mlogloss:0.78185                            \n",
      "[865]\ttrain-mlogloss:0.65450\tvalidate-mlogloss:0.78191                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.78618830429515                                                \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 0.7, 'colsample_bytree': 0.9, 'eval_metric': 'mlogloss', 'gamma': 4, 'lambda': 1, 'learning_rate': 0.08683535863618509, 'max_delta_step': 7, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 15, 'num_boost_round': 1937, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.06628\tvalidate-mlogloss:1.06942                              \n",
      "[200]\ttrain-mlogloss:0.66581\tvalidate-mlogloss:0.79551                            \n",
      "[400]\ttrain-mlogloss:0.65732\tvalidate-mlogloss:0.79676                            \n",
      "[552]\ttrain-mlogloss:0.65370\tvalidate-mlogloss:0.79765                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.06549\tvalidate-mlogloss:1.06913                              \n",
      "[200]\ttrain-mlogloss:0.66932\tvalidate-mlogloss:0.78219                            \n",
      "[400]\ttrain-mlogloss:0.66139\tvalidate-mlogloss:0.78215                            \n",
      "[600]\ttrain-mlogloss:0.65793\tvalidate-mlogloss:0.78181                            \n",
      "[800]\ttrain-mlogloss:0.65536\tvalidate-mlogloss:0.78126                            \n",
      "[1000]\ttrain-mlogloss:0.65382\tvalidate-mlogloss:0.78093                           \n",
      "[1200]\ttrain-mlogloss:0.65216\tvalidate-mlogloss:0.78095                           \n",
      "[1400]\ttrain-mlogloss:0.65077\tvalidate-mlogloss:0.78095                           \n",
      "[1600]\ttrain-mlogloss:0.64971\tvalidate-mlogloss:0.78111                           \n",
      "[1613]\ttrain-mlogloss:0.64966\tvalidate-mlogloss:0.78101                           \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.7893140961585055                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 0.7, 'colsample_bytree': 0.9, 'eval_metric': 'mlogloss', 'gamma': 4, 'lambda': 1, 'learning_rate': 0.12413260896225789, 'max_delta_step': 7, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 15, 'num_boost_round': 915, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.05302\tvalidate-mlogloss:1.05742                              \n",
      "[200]\ttrain-mlogloss:0.66096\tvalidate-mlogloss:0.80220                            \n",
      "[400]\ttrain-mlogloss:0.65401\tvalidate-mlogloss:0.80392                            \n",
      "[452]\ttrain-mlogloss:0.65244\tvalidate-mlogloss:0.80331                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.05191\tvalidate-mlogloss:1.05704                              \n",
      "[200]\ttrain-mlogloss:0.66405\tvalidate-mlogloss:0.78495                            \n",
      "[400]\ttrain-mlogloss:0.65771\tvalidate-mlogloss:0.78409                            \n",
      "[600]\ttrain-mlogloss:0.65487\tvalidate-mlogloss:0.78386                            \n",
      "[800]\ttrain-mlogloss:0.65217\tvalidate-mlogloss:0.78313                            \n",
      "[914]\ttrain-mlogloss:0.65099\tvalidate-mlogloss:0.78313                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.793219050410211                                               \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 0.7, 'colsample_bytree': 0.9, 'eval_metric': 'mlogloss', 'gamma': 4, 'lambda': 1, 'learning_rate': 0.14401833465005365, 'max_delta_step': 7, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 15, 'num_boost_round': 885, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.04610\tvalidate-mlogloss:1.05116                              \n",
      "[200]\ttrain-mlogloss:0.66055\tvalidate-mlogloss:0.79850                            \n",
      "[400]\ttrain-mlogloss:0.65304\tvalidate-mlogloss:0.79932                            \n",
      "[553]\ttrain-mlogloss:0.64945\tvalidate-mlogloss:0.80342                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.04482\tvalidate-mlogloss:1.05073                              \n",
      "[200]\ttrain-mlogloss:0.66406\tvalidate-mlogloss:0.78904                            \n",
      "[400]\ttrain-mlogloss:0.65770\tvalidate-mlogloss:0.78779                            \n",
      "[600]\ttrain-mlogloss:0.65311\tvalidate-mlogloss:0.78711                            \n",
      "[800]\ttrain-mlogloss:0.65093\tvalidate-mlogloss:0.78654                            \n",
      "[884]\ttrain-mlogloss:0.64981\tvalidate-mlogloss:0.78658                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.7950005328390626                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 0.7, 'colsample_bytree': 0.9, 'eval_metric': 'mlogloss', 'gamma': 4, 'lambda': 1, 'learning_rate': 0.04124001314062504, 'max_delta_step': 7, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 15, 'num_boost_round': 1276, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.08301\tvalidate-mlogloss:1.08453                              \n",
      "[200]\ttrain-mlogloss:0.68580\tvalidate-mlogloss:0.79572                            \n",
      "[400]\ttrain-mlogloss:0.66658\tvalidate-mlogloss:0.79356                            \n",
      "[600]\ttrain-mlogloss:0.66060\tvalidate-mlogloss:0.79358                            \n",
      "[800]\ttrain-mlogloss:0.65806\tvalidate-mlogloss:0.79330                            \n",
      "[865]\ttrain-mlogloss:0.65737\tvalidate-mlogloss:0.79315                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.08262\tvalidate-mlogloss:1.08438                              \n",
      "[200]\ttrain-mlogloss:0.68814\tvalidate-mlogloss:0.78486                            \n",
      "[400]\ttrain-mlogloss:0.67117\tvalidate-mlogloss:0.78100                            \n",
      "[600]\ttrain-mlogloss:0.66597\tvalidate-mlogloss:0.78024                            \n",
      "[800]\ttrain-mlogloss:0.66317\tvalidate-mlogloss:0.77957                            \n",
      "[1000]\ttrain-mlogloss:0.66091\tvalidate-mlogloss:0.77973                           \n",
      "[1200]\ttrain-mlogloss:0.65938\tvalidate-mlogloss:0.77960                           \n",
      "[1275]\ttrain-mlogloss:0.65868\tvalidate-mlogloss:0.77975                           \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.7864508160076652                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 0.7, 'colsample_bytree': 0.9, 'eval_metric': 'mlogloss', 'gamma': 4, 'lambda': 1, 'learning_rate': 0.10426652673329329, 'max_delta_step': 7, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 15, 'num_boost_round': 1439, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.06004\tvalidate-mlogloss:1.06377                              \n",
      "[200]\ttrain-mlogloss:0.66397\tvalidate-mlogloss:0.80004                            \n",
      "[400]\ttrain-mlogloss:0.65547\tvalidate-mlogloss:0.80047                            \n",
      "[499]\ttrain-mlogloss:0.65320\tvalidate-mlogloss:0.80265                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.05909\tvalidate-mlogloss:1.06343                              \n",
      "[200]\ttrain-mlogloss:0.66619\tvalidate-mlogloss:0.77986                            \n",
      "[400]\ttrain-mlogloss:0.65998\tvalidate-mlogloss:0.77949                            \n",
      "[600]\ttrain-mlogloss:0.65613\tvalidate-mlogloss:0.77948                            \n",
      "[800]\ttrain-mlogloss:0.65401\tvalidate-mlogloss:0.77885                            \n",
      "[1000]\ttrain-mlogloss:0.65178\tvalidate-mlogloss:0.77860                           \n",
      "[1200]\ttrain-mlogloss:0.65065\tvalidate-mlogloss:0.77819                           \n",
      "[1400]\ttrain-mlogloss:0.64954\tvalidate-mlogloss:0.77866                           \n",
      "[1438]\ttrain-mlogloss:0.64951\tvalidate-mlogloss:0.77861                           \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.7906284968678496                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 0.7, 'colsample_bytree': 0.9, 'eval_metric': 'mlogloss', 'gamma': 4, 'lambda': 1, 'learning_rate': 0.07790575157381553, 'max_delta_step': 9, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 15, 'num_boost_round': 1184, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.06951\tvalidate-mlogloss:1.07234                              \n",
      "[200]\ttrain-mlogloss:0.66896\tvalidate-mlogloss:0.80068                            \n",
      "[400]\ttrain-mlogloss:0.65869\tvalidate-mlogloss:0.80206                            \n",
      "[556]\ttrain-mlogloss:0.65494\tvalidate-mlogloss:0.80292                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.06880\tvalidate-mlogloss:1.07207                              \n",
      "[200]\ttrain-mlogloss:0.67145\tvalidate-mlogloss:0.78041                            \n",
      "[400]\ttrain-mlogloss:0.66252\tvalidate-mlogloss:0.78029                            \n",
      "[600]\ttrain-mlogloss:0.65805\tvalidate-mlogloss:0.78020                            \n",
      "[698]\ttrain-mlogloss:0.65677\tvalidate-mlogloss:0.77981                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.7913664685411375                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 0.7, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 4, 'lambda': 1, 'learning_rate': 0.09406504863816828, 'max_delta_step': 7, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1064, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.06585\tvalidate-mlogloss:1.06890                              \n",
      "[200]\ttrain-mlogloss:0.66487\tvalidate-mlogloss:0.78748                            \n",
      "[400]\ttrain-mlogloss:0.65677\tvalidate-mlogloss:0.78854                            \n",
      "[600]\ttrain-mlogloss:0.65372\tvalidate-mlogloss:0.78876                            \n",
      "[676]\ttrain-mlogloss:0.65252\tvalidate-mlogloss:0.78859                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.06619\tvalidate-mlogloss:1.07037                              \n",
      "[200]\ttrain-mlogloss:0.66928\tvalidate-mlogloss:0.78117                            \n",
      "[400]\ttrain-mlogloss:0.66215\tvalidate-mlogloss:0.77956                            \n",
      "[600]\ttrain-mlogloss:0.65870\tvalidate-mlogloss:0.77910                            \n",
      "[800]\ttrain-mlogloss:0.65578\tvalidate-mlogloss:0.77871                            \n",
      "[1000]\ttrain-mlogloss:0.65406\tvalidate-mlogloss:0.77827                           \n",
      "[1063]\ttrain-mlogloss:0.65339\tvalidate-mlogloss:0.77821                           \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.7834007690819744                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 0.7, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 8, 'lambda': 1, 'learning_rate': 0.05766833130753594, 'max_delta_step': 3, 'max_depth': 3, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 936, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.08371\tvalidate-mlogloss:1.08408                              \n",
      "[200]\ttrain-mlogloss:0.74394\tvalidate-mlogloss:0.81254                            \n",
      "[400]\ttrain-mlogloss:0.72625\tvalidate-mlogloss:0.80787                            \n",
      "[600]\ttrain-mlogloss:0.72308\tvalidate-mlogloss:0.80673                            \n",
      "[800]\ttrain-mlogloss:0.72104\tvalidate-mlogloss:0.80499                            \n",
      "[935]\ttrain-mlogloss:0.72017\tvalidate-mlogloss:0.80430                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.08288\tvalidate-mlogloss:1.08358                              \n",
      "[200]\ttrain-mlogloss:0.74179\tvalidate-mlogloss:0.80700                            \n",
      "[400]\ttrain-mlogloss:0.72405\tvalidate-mlogloss:0.80027                            \n",
      "[600]\ttrain-mlogloss:0.72118\tvalidate-mlogloss:0.79861                            \n",
      "[800]\ttrain-mlogloss:0.71969\tvalidate-mlogloss:0.79782                            \n",
      "[935]\ttrain-mlogloss:0.71851\tvalidate-mlogloss:0.79709                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8006995050949202                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 0.7, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 4, 'lambda': 1, 'learning_rate': 0.11468168924516958, 'max_delta_step': 2, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1664, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.05896\tvalidate-mlogloss:1.06265                              \n",
      "[200]\ttrain-mlogloss:0.66346\tvalidate-mlogloss:0.79026                            \n",
      "[400]\ttrain-mlogloss:0.65577\tvalidate-mlogloss:0.79011                            \n",
      "[600]\ttrain-mlogloss:0.65185\tvalidate-mlogloss:0.78930                            \n",
      "[747]\ttrain-mlogloss:0.64990\tvalidate-mlogloss:0.78903                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.05937\tvalidate-mlogloss:1.06444                              \n",
      "[200]\ttrain-mlogloss:0.66740\tvalidate-mlogloss:0.78199                            \n",
      "[400]\ttrain-mlogloss:0.66092\tvalidate-mlogloss:0.78048                            \n",
      "[600]\ttrain-mlogloss:0.65662\tvalidate-mlogloss:0.78011                            \n",
      "[800]\ttrain-mlogloss:0.65463\tvalidate-mlogloss:0.77997                            \n",
      "[1000]\ttrain-mlogloss:0.65266\tvalidate-mlogloss:0.77950                           \n",
      "[1200]\ttrain-mlogloss:0.65108\tvalidate-mlogloss:0.77957                           \n",
      "[1400]\ttrain-mlogloss:0.64968\tvalidate-mlogloss:0.77951                           \n",
      "[1600]\ttrain-mlogloss:0.64837\tvalidate-mlogloss:0.77983                           \n",
      "[1663]\ttrain-mlogloss:0.64772\tvalidate-mlogloss:0.77968                           \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.7843414992786213                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.5, 'colsample_bynode': 0.7, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 1, 'lambda': 1, 'learning_rate': 0.21364512949120215, 'max_delta_step': 2, 'max_depth': 4, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 1.0, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1540, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.05220\tvalidate-mlogloss:1.05198                              \n",
      "[200]\ttrain-mlogloss:0.64952\tvalidate-mlogloss:0.79950                            \n",
      "[400]\ttrain-mlogloss:0.64761\tvalidate-mlogloss:0.79923                            \n",
      "[535]\ttrain-mlogloss:0.64761\tvalidate-mlogloss:0.79923                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.04212\tvalidate-mlogloss:1.04536                              \n",
      "[200]\ttrain-mlogloss:0.65061\tvalidate-mlogloss:0.78093                            \n",
      "[400]\ttrain-mlogloss:0.65044\tvalidate-mlogloss:0.78098                            \n",
      "[599]\ttrain-mlogloss:0.65044\tvalidate-mlogloss:0.78098                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.7901045024392317                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 0.5, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 0, 'lambda': 1, 'learning_rate': 0.19853389991783216, 'max_delta_step': 2, 'max_depth': 11, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1403, 'num_feat': 50, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.02856\tvalidate-mlogloss:1.04945                              \n",
      "[200]\ttrain-mlogloss:0.34490\tvalidate-mlogloss:0.86519                            \n",
      "[400]\ttrain-mlogloss:0.24634\tvalidate-mlogloss:0.88960                            \n",
      "[447]\ttrain-mlogloss:0.23281\tvalidate-mlogloss:0.89350                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.02519\tvalidate-mlogloss:1.04025                              \n",
      "[200]\ttrain-mlogloss:0.35014\tvalidate-mlogloss:0.83429                            \n",
      "[400]\ttrain-mlogloss:0.25169\tvalidate-mlogloss:0.85955                            \n",
      "[474]\ttrain-mlogloss:0.23068\tvalidate-mlogloss:0.86771                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8807026436095762                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.9, 'colsample_bynode': 0.6, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 4, 'lambda': 1, 'learning_rate': 0.16633183586648836, 'max_delta_step': 2, 'max_depth': 6, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.8, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1003, 'num_feat': 80, 'step': 0.1}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.04503\tvalidate-mlogloss:1.05895                              \n",
      "[200]\ttrain-mlogloss:0.66037\tvalidate-mlogloss:0.85279                            \n",
      "[400]\ttrain-mlogloss:0.65521\tvalidate-mlogloss:0.85352                            \n",
      "[424]\ttrain-mlogloss:0.65518\tvalidate-mlogloss:0.85367                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.04201\tvalidate-mlogloss:1.04584                              \n",
      "[200]\ttrain-mlogloss:0.66460\tvalidate-mlogloss:0.79402                            \n",
      "[400]\ttrain-mlogloss:0.66164\tvalidate-mlogloss:0.79356                            \n",
      "[600]\ttrain-mlogloss:0.66007\tvalidate-mlogloss:0.79921                            \n",
      "[800]\ttrain-mlogloss:0.65885\tvalidate-mlogloss:0.79891                            \n",
      "[850]\ttrain-mlogloss:0.65874\tvalidate-mlogloss:0.79884                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.826245430171217                                               \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.8, 'colsample_bynode': 0.7, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 5, 'lambda': 1, 'learning_rate': 0.11255718149099364, 'max_delta_step': 2, 'max_depth': 5, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 5, 'num_boost_round': 1920, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.06535\tvalidate-mlogloss:1.06754                              \n",
      "[200]\ttrain-mlogloss:0.69680\tvalidate-mlogloss:0.79144                            \n",
      "[400]\ttrain-mlogloss:0.69029\tvalidate-mlogloss:0.79104                            \n",
      "[536]\ttrain-mlogloss:0.68820\tvalidate-mlogloss:0.79169                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.06362\tvalidate-mlogloss:1.06670                              \n",
      "[200]\ttrain-mlogloss:0.69411\tvalidate-mlogloss:0.78879                            \n",
      "[400]\ttrain-mlogloss:0.68955\tvalidate-mlogloss:0.78730                            \n",
      "[600]\ttrain-mlogloss:0.68656\tvalidate-mlogloss:0.78641                            \n",
      "[800]\ttrain-mlogloss:0.68469\tvalidate-mlogloss:0.78544                            \n",
      "[1000]\ttrain-mlogloss:0.68304\tvalidate-mlogloss:0.78528                           \n",
      "[1200]\ttrain-mlogloss:0.68175\tvalidate-mlogloss:0.78498                           \n",
      "[1400]\ttrain-mlogloss:0.68067\tvalidate-mlogloss:0.78473                           \n",
      "[1600]\ttrain-mlogloss:0.67991\tvalidate-mlogloss:0.78472                           \n",
      "[1800]\ttrain-mlogloss:0.67947\tvalidate-mlogloss:0.78477                           \n",
      "[1851]\ttrain-mlogloss:0.67941\tvalidate-mlogloss:0.78477                           \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.7882317622409434                                              \n",
      "{'booster_params': {'alpha': 100, 'booster': 'gbtree', 'colsample_bylevel': 0.6, 'colsample_bynode': 1.0, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 7, 'lambda': 100, 'learning_rate': 0.16283988149677284, 'max_delta_step': 2, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.9, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1664, 'num_feat': 60, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.05380\tvalidate-mlogloss:1.06003                              \n",
      "[200]\ttrain-mlogloss:0.73026\tvalidate-mlogloss:0.85894                            \n",
      "[400]\ttrain-mlogloss:0.72941\tvalidate-mlogloss:0.86651                            \n",
      "[438]\ttrain-mlogloss:0.72940\tvalidate-mlogloss:0.86654                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.05317\tvalidate-mlogloss:1.05578                              \n",
      "[200]\ttrain-mlogloss:0.72899\tvalidate-mlogloss:0.82177                            \n",
      "[400]\ttrain-mlogloss:0.72846\tvalidate-mlogloss:0.82166                            \n",
      "[600]\ttrain-mlogloss:0.72817\tvalidate-mlogloss:0.82128                            \n",
      "[800]\ttrain-mlogloss:0.72806\tvalidate-mlogloss:0.82113                            \n",
      "[1000]\ttrain-mlogloss:0.72789\tvalidate-mlogloss:0.82103                           \n",
      "[1200]\ttrain-mlogloss:0.72783\tvalidate-mlogloss:0.82095                           \n",
      "[1400]\ttrain-mlogloss:0.72714\tvalidate-mlogloss:0.82335                           \n",
      "[1600]\ttrain-mlogloss:0.72699\tvalidate-mlogloss:0.82319                           \n",
      "[1605]\ttrain-mlogloss:0.72699\tvalidate-mlogloss:0.82319                           \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8448678590406955                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 0.7, 'colsample_bytree': 0.6, 'eval_metric': 'mlogloss', 'gamma': 3, 'lambda': 1, 'learning_rate': 0.13001854024875983, 'max_delta_step': 5, 'max_depth': 10, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.8, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 919, 'num_feat': 70, 'step': 0.2}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.04672\tvalidate-mlogloss:1.06655                              \n",
      "[200]\ttrain-mlogloss:0.60724\tvalidate-mlogloss:0.83055                            \n",
      "[400]\ttrain-mlogloss:0.60306\tvalidate-mlogloss:0.82954                            \n",
      "[440]\ttrain-mlogloss:0.60299\tvalidate-mlogloss:0.82979                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.04881\tvalidate-mlogloss:1.05882                              \n",
      "[200]\ttrain-mlogloss:0.61439\tvalidate-mlogloss:0.80856                            \n",
      "[400]\ttrain-mlogloss:0.60983\tvalidate-mlogloss:0.81186                            \n",
      "[460]\ttrain-mlogloss:0.60942\tvalidate-mlogloss:0.81164                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8207219691026229                                              \n",
      "{'booster_params': {'alpha': 1, 'booster': 'gbtree', 'colsample_bylevel': 0.8, 'colsample_bynode': 0.8, 'colsample_bytree': 0.5, 'eval_metric': 'mlogloss', 'gamma': 2, 'lambda': 1, 'learning_rate': 0.12158413652106814, 'max_delta_step': 1, 'max_depth': 8, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 20, 'num_boost_round': 1288, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.05533\tvalidate-mlogloss:1.07046                              \n",
      "[200]\ttrain-mlogloss:0.53050\tvalidate-mlogloss:0.83388                            \n",
      "[400]\ttrain-mlogloss:0.50390\tvalidate-mlogloss:0.83378                            \n",
      "[481]\ttrain-mlogloss:0.49739\tvalidate-mlogloss:0.83394                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.05312\tvalidate-mlogloss:1.05987                              \n",
      "[200]\ttrain-mlogloss:0.53605\tvalidate-mlogloss:0.78902                            \n",
      "[400]\ttrain-mlogloss:0.50661\tvalidate-mlogloss:0.79017                            \n",
      "[580]\ttrain-mlogloss:0.49482\tvalidate-mlogloss:0.79081                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8123759949690201                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 0.5, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 1, 'lambda': 1, 'learning_rate': 0.14479447887484054, 'max_delta_step': 8, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.8, 'tree_method': 'hist'}, 'k_neighbors': 25, 'num_boost_round': 1488, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.04539\tvalidate-mlogloss:1.05047                              \n",
      "[200]\ttrain-mlogloss:0.54165\tvalidate-mlogloss:0.79551                            \n",
      "[400]\ttrain-mlogloss:0.49679\tvalidate-mlogloss:0.79992                            \n",
      "[503]\ttrain-mlogloss:0.48670\tvalidate-mlogloss:0.80085                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.04678\tvalidate-mlogloss:1.05255                              \n",
      "[200]\ttrain-mlogloss:0.54715\tvalidate-mlogloss:0.78302                            \n",
      "[400]\ttrain-mlogloss:0.50229\tvalidate-mlogloss:0.78603                            \n",
      "[553]\ttrain-mlogloss:0.48675\tvalidate-mlogloss:0.78684                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.7938520630854582                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.5, 'colsample_bynode': 0.6, 'colsample_bytree': 0.7, 'eval_metric': 'mlogloss', 'gamma': 4, 'lambda': 100, 'learning_rate': 0.10347331641435954, 'max_delta_step': 7, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 973, 'num_feat': 50, 'step': 0.1}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.07185\tvalidate-mlogloss:1.09303                              \n",
      "[200]\ttrain-mlogloss:0.71185\tvalidate-mlogloss:0.93859                            \n",
      "[400]\ttrain-mlogloss:0.70374\tvalidate-mlogloss:0.96574                            \n",
      "[439]\ttrain-mlogloss:0.70290\tvalidate-mlogloss:0.96615                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.06903\tvalidate-mlogloss:1.07100                              \n",
      "[200]\ttrain-mlogloss:0.70871\tvalidate-mlogloss:0.86089                            \n",
      "[400]\ttrain-mlogloss:0.70183\tvalidate-mlogloss:0.86468                            \n",
      "[453]\ttrain-mlogloss:0.70071\tvalidate-mlogloss:0.86474                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.9153775560571634                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.9, 'colsample_bynode': 1.0, 'colsample_bytree': 1.0, 'eval_metric': 'mlogloss', 'gamma': 8, 'lambda': 1, 'learning_rate': 0.1353824237639178, 'max_delta_step': 4, 'max_depth': 3, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.9, 'tree_method': 'hist'}, 'k_neighbors': 5, 'num_boost_round': 1606, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.06410\tvalidate-mlogloss:1.06471                              \n",
      "[200]\ttrain-mlogloss:0.71817\tvalidate-mlogloss:0.79288                            \n",
      "[400]\ttrain-mlogloss:0.71606\tvalidate-mlogloss:0.79169                            \n",
      "[600]\ttrain-mlogloss:0.71533\tvalidate-mlogloss:0.79168                            \n",
      "[800]\ttrain-mlogloss:0.71504\tvalidate-mlogloss:0.79162                            \n",
      "[1000]\ttrain-mlogloss:0.71466\tvalidate-mlogloss:0.79140                           \n",
      "[1200]\ttrain-mlogloss:0.71453\tvalidate-mlogloss:0.79138                           \n",
      "[1400]\ttrain-mlogloss:0.71440\tvalidate-mlogloss:0.79136                           \n",
      "[1600]\ttrain-mlogloss:0.71424\tvalidate-mlogloss:0.79132                           \n",
      "[1605]\ttrain-mlogloss:0.71424\tvalidate-mlogloss:0.79132                           \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.06346\tvalidate-mlogloss:1.06428                              \n",
      "[200]\ttrain-mlogloss:0.71525\tvalidate-mlogloss:0.79360                            \n",
      "[400]\ttrain-mlogloss:0.71376\tvalidate-mlogloss:0.79257                            \n",
      "[600]\ttrain-mlogloss:0.71349\tvalidate-mlogloss:0.79241                            \n",
      "[800]\ttrain-mlogloss:0.71293\tvalidate-mlogloss:0.79393                            \n",
      "[1000]\ttrain-mlogloss:0.71252\tvalidate-mlogloss:0.79370                           \n",
      "[1058]\ttrain-mlogloss:0.71242\tvalidate-mlogloss:0.79367                           \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.792509865802667                                               \n",
      "{'booster_params': {'alpha': 100, 'booster': 'gbtree', 'colsample_bylevel': 0.8, 'colsample_bynode': 0.7, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 1, 'lambda': 1, 'learning_rate': 0.24243382386308496, 'max_delta_step': 6, 'max_depth': 4, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 1.0, 'tree_method': 'hist'}, 'k_neighbors': 30, 'num_boost_round': 1367, 'num_feat': 80, 'step': 0.2}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.03919\tvalidate-mlogloss:1.04209                              \n",
      "[200]\ttrain-mlogloss:0.70789\tvalidate-mlogloss:0.80714                            \n",
      "[400]\ttrain-mlogloss:0.70789\tvalidate-mlogloss:0.80714                            \n",
      "[478]\ttrain-mlogloss:0.70789\tvalidate-mlogloss:0.80714                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.04188\tvalidate-mlogloss:1.04619                              \n",
      "[200]\ttrain-mlogloss:0.70405\tvalidate-mlogloss:0.81947                            \n",
      "[400]\ttrain-mlogloss:0.70405\tvalidate-mlogloss:0.81947                            \n",
      "[496]\ttrain-mlogloss:0.70405\tvalidate-mlogloss:0.81947                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8133027912903                                                 \n",
      "{'booster_params': {'alpha': 1, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 1.0, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 4, 'lambda': 1, 'learning_rate': 0.18143197575782552, 'max_delta_step': 2, 'max_depth': 11, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.8, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1058, 'num_feat': 60, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.01598\tvalidate-mlogloss:1.04585                              \n",
      "[200]\ttrain-mlogloss:0.58811\tvalidate-mlogloss:0.83017                            \n",
      "[400]\ttrain-mlogloss:0.58676\tvalidate-mlogloss:0.82924                            \n",
      "[418]\ttrain-mlogloss:0.58675\tvalidate-mlogloss:0.82941                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.01627\tvalidate-mlogloss:1.03649                              \n",
      "[200]\ttrain-mlogloss:0.58751\tvalidate-mlogloss:0.79854                            \n",
      "[400]\ttrain-mlogloss:0.58646\tvalidate-mlogloss:0.79940                            \n",
      "[427]\ttrain-mlogloss:0.58646\tvalidate-mlogloss:0.79932                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8143628642618469                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.6, 'colsample_bynode': 0.7, 'colsample_bytree': 0.6, 'eval_metric': 'mlogloss', 'gamma': 0, 'lambda': 100, 'learning_rate': 0.04683746091402628, 'max_delta_step': 7, 'max_depth': 6, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1662, 'num_feat': 70, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.08526\tvalidate-mlogloss:1.08639                              \n",
      "[200]\ttrain-mlogloss:0.70875\tvalidate-mlogloss:0.82701                            \n",
      "[400]\ttrain-mlogloss:0.66204\tvalidate-mlogloss:0.83234                            \n",
      "[600]\ttrain-mlogloss:0.63248\tvalidate-mlogloss:0.83450                            \n",
      "[647]\ttrain-mlogloss:0.62667\tvalidate-mlogloss:0.83483                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.08706\tvalidate-mlogloss:1.08773                              \n",
      "[200]\ttrain-mlogloss:0.71085\tvalidate-mlogloss:0.82831                            \n",
      "[400]\ttrain-mlogloss:0.66426\tvalidate-mlogloss:0.82147                            \n",
      "[600]\ttrain-mlogloss:0.63501\tvalidate-mlogloss:0.82343                            \n",
      "[800]\ttrain-mlogloss:0.61148\tvalidate-mlogloss:0.82506                            \n",
      "[840]\ttrain-mlogloss:0.60737\tvalidate-mlogloss:0.82506                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8299797849737145                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.8, 'colsample_bynode': 0.8, 'colsample_bytree': 0.5, 'eval_metric': 'mlogloss', 'gamma': 3, 'lambda': 1, 'learning_rate': 0.15937688113402915, 'max_delta_step': 7, 'max_depth': 5, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.6, 'tree_method': 'hist'}, 'k_neighbors': 20, 'num_boost_round': 1353, 'num_feat': 80, 'step': 0.1}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.05144\tvalidate-mlogloss:1.05596                              \n",
      "[200]\ttrain-mlogloss:0.65788\tvalidate-mlogloss:0.89272                            \n",
      "[400]\ttrain-mlogloss:0.64563\tvalidate-mlogloss:0.92185                            \n",
      "[451]\ttrain-mlogloss:0.64339\tvalidate-mlogloss:0.92181                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.05550\tvalidate-mlogloss:1.05977                              \n",
      "[200]\ttrain-mlogloss:0.66372\tvalidate-mlogloss:0.81494                            \n",
      "[400]\ttrain-mlogloss:0.65250\tvalidate-mlogloss:0.82192                            \n",
      "[572]\ttrain-mlogloss:0.64744\tvalidate-mlogloss:0.82039                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8712594168910153                                              \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 0.5, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 5, 'lambda': 1, 'learning_rate': 0.02987763120931391, 'max_delta_step': 5, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 1.0, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1211, 'num_feat': 50, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.08751\tvalidate-mlogloss:1.09146                              \n",
      "[200]\ttrain-mlogloss:0.70989\tvalidate-mlogloss:0.82191                            \n",
      "[400]\ttrain-mlogloss:0.69012\tvalidate-mlogloss:0.82314                            \n",
      "[600]\ttrain-mlogloss:0.69010\tvalidate-mlogloss:0.82317                            \n",
      "[663]\ttrain-mlogloss:0.69010\tvalidate-mlogloss:0.82317                            \n",
      "Predicting for fold 1...                                                          \n",
      "Start processing fold 2...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4542 14218 21946]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21946 21946 21946]                                                               \n",
      "Start training model for fold 2...                                                \n",
      "[0]\ttrain-mlogloss:1.08916\tvalidate-mlogloss:1.09038                              \n",
      "[200]\ttrain-mlogloss:0.70685\tvalidate-mlogloss:0.80486                            \n",
      "[400]\ttrain-mlogloss:0.68691\tvalidate-mlogloss:0.80079                            \n",
      "[600]\ttrain-mlogloss:0.68691\tvalidate-mlogloss:0.80078                            \n",
      "[719]\ttrain-mlogloss:0.68691\tvalidate-mlogloss:0.80078                            \n",
      "Predicting for fold 2...                                                          \n",
      "Average log loss: 0.8119777286282438                                              \n",
      "{'booster_params': {'alpha': 100, 'booster': 'gbtree', 'colsample_bylevel': 0.5, 'colsample_bynode': 1.0, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 1, 'lambda': 1, 'learning_rate': 0.009159107479952022, 'max_delta_step': 3, 'max_depth': 10, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.8, 'tree_method': 'hist'}, 'k_neighbors': 25, 'num_boost_round': 1769, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                        \n",
      "Class distribution before oversampling:                                           \n",
      "[ 4543 14218 21945]                                                               \n",
      "Class distribution after oversampling:                                            \n",
      "[21945 21945 21945]                                                               \n",
      "Start training model for fold 1...                                                \n",
      "[0]\ttrain-mlogloss:1.09556\tvalidate-mlogloss:1.09581                              \n",
      "[200]\ttrain-mlogloss:0.84032\tvalidate-mlogloss:0.87581                            \n",
      "[400]\ttrain-mlogloss:0.77465\tvalidate-mlogloss:0.83457                            \n",
      "[600]\ttrain-mlogloss:0.74462\tvalidate-mlogloss:0.82306                            \n",
      "[800]\ttrain-mlogloss:0.72736\tvalidate-mlogloss:0.81878                              \n",
      "[1000]\ttrain-mlogloss:0.71585\tvalidate-mlogloss:0.81775                             \n",
      "[1200]\ttrain-mlogloss:0.70770\tvalidate-mlogloss:0.81628                             \n",
      "[1400]\ttrain-mlogloss:0.70146\tvalidate-mlogloss:0.81615                             \n",
      "[1600]\ttrain-mlogloss:0.69653\tvalidate-mlogloss:0.81531                             \n",
      "[1768]\ttrain-mlogloss:0.69317\tvalidate-mlogloss:0.81509                             \n",
      "Predicting for fold 1...                                                            \n",
      "Start processing fold 2...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4542 14218 21946]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21946 21946 21946]                                                                 \n",
      "Start training model for fold 2...                                                  \n",
      "[0]\ttrain-mlogloss:1.09567\tvalidate-mlogloss:1.09596                                \n",
      "[200]\ttrain-mlogloss:0.83461\tvalidate-mlogloss:0.87578                              \n",
      "[400]\ttrain-mlogloss:0.76998\tvalidate-mlogloss:0.83279                              \n",
      "[600]\ttrain-mlogloss:0.74142\tvalidate-mlogloss:0.81747                              \n",
      "[800]\ttrain-mlogloss:0.72495\tvalidate-mlogloss:0.80979                              \n",
      "[1000]\ttrain-mlogloss:0.71373\tvalidate-mlogloss:0.80497                             \n",
      "[1200]\ttrain-mlogloss:0.70576\tvalidate-mlogloss:0.80194                             \n",
      "[1400]\ttrain-mlogloss:0.69973\tvalidate-mlogloss:0.79970                             \n",
      "[1600]\ttrain-mlogloss:0.69519\tvalidate-mlogloss:0.79812                             \n",
      "[1768]\ttrain-mlogloss:0.69206\tvalidate-mlogloss:0.79703                             \n",
      "Predicting for fold 2...                                                            \n",
      "Average log loss: 0.8060604004419865                                                \n",
      "{'booster_params': {'alpha': 1, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 0.6, 'colsample_bytree': 1.0, 'eval_metric': 'mlogloss', 'gamma': 2, 'lambda': 100, 'learning_rate': 0.08160659432305997, 'max_delta_step': 1, 'max_depth': 8, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1481, 'num_feat': 80, 'step': 0.2}\n",
      "Start processing fold 1...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4543 14218 21945]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21945 21945 21945]                                                                 \n",
      "Start training model for fold 1...                                                  \n",
      "[0]\ttrain-mlogloss:1.07187\tvalidate-mlogloss:1.07366                                \n",
      "[200]\ttrain-mlogloss:0.62480\tvalidate-mlogloss:0.79178                              \n",
      "[400]\ttrain-mlogloss:0.58680\tvalidate-mlogloss:0.79513                              \n",
      "[600]\ttrain-mlogloss:0.57268\tvalidate-mlogloss:0.79780                              \n",
      "[627]\ttrain-mlogloss:0.57119\tvalidate-mlogloss:0.79798                              \n",
      "Predicting for fold 1...                                                            \n",
      "Start processing fold 2...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4542 14218 21946]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21946 21946 21946]                                                                 \n",
      "Start training model for fold 2...                                                  \n",
      "[0]\ttrain-mlogloss:1.07135\tvalidate-mlogloss:1.07372                                \n",
      "[200]\ttrain-mlogloss:0.62789\tvalidate-mlogloss:0.80988                              \n",
      "[400]\ttrain-mlogloss:0.59022\tvalidate-mlogloss:0.81694                              \n",
      "[498]\ttrain-mlogloss:0.58095\tvalidate-mlogloss:0.81947                              \n",
      "Predicting for fold 2...                                                            \n",
      "Average log loss: 0.8087284189886972                                                \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.9, 'colsample_bynode': 0.7, 'colsample_bytree': 0.7, 'eval_metric': 'mlogloss', 'gamma': 7, 'lambda': 1, 'learning_rate': 0.14939821128378566, 'max_delta_step': 2, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.9, 'tree_method': 'hist'}, 'k_neighbors': 5, 'num_boost_round': 895, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4543 14218 21945]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21945 21945 21945]                                                                 \n",
      "Start training model for fold 1...                                                  \n",
      "[0]\ttrain-mlogloss:1.04565\tvalidate-mlogloss:1.05021                                \n",
      "[200]\ttrain-mlogloss:0.69137\tvalidate-mlogloss:0.78748                              \n",
      "[400]\ttrain-mlogloss:0.69083\tvalidate-mlogloss:0.78737                              \n",
      "[600]\ttrain-mlogloss:0.69016\tvalidate-mlogloss:0.78703                              \n",
      "[800]\ttrain-mlogloss:0.69000\tvalidate-mlogloss:0.78709                              \n",
      "[894]\ttrain-mlogloss:0.68991\tvalidate-mlogloss:0.78704                              \n",
      "Predicting for fold 1...                                                            \n",
      "Start processing fold 2...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4542 14218 21946]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21946 21946 21946]                                                                 \n",
      "Start training model for fold 2...                                                  \n",
      "[0]\ttrain-mlogloss:1.04814\tvalidate-mlogloss:1.05447                                \n",
      "[200]\ttrain-mlogloss:0.68951\tvalidate-mlogloss:0.78562                              \n",
      "[400]\ttrain-mlogloss:0.68831\tvalidate-mlogloss:0.78529                              \n",
      "[521]\ttrain-mlogloss:0.68831\tvalidate-mlogloss:0.78532                              \n",
      "Predicting for fold 2...                                                            \n",
      "Average log loss: 0.7861831283413259                                                \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.8, 'colsample_bynode': 1.0, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 4, 'lambda': 1, 'learning_rate': 0.06247083686206979, 'max_delta_step': 8, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 30, 'num_boost_round': 1953, 'num_feat': 60, 'step': 0.3}\n",
      "Start processing fold 1...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4543 14218 21945]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21945 21945 21945]                                                                 \n",
      "Start training model for fold 1...                                                  \n",
      "[0]\ttrain-mlogloss:1.07567\tvalidate-mlogloss:1.08027                                \n",
      "[200]\ttrain-mlogloss:0.67571\tvalidate-mlogloss:0.82600                              \n",
      "[400]\ttrain-mlogloss:0.66482\tvalidate-mlogloss:0.83760                              \n",
      "[506]\ttrain-mlogloss:0.66214\tvalidate-mlogloss:0.83974                              \n",
      "Predicting for fold 1...                                                            \n",
      "Start processing fold 2...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4542 14218 21946]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21946 21946 21946]                                                                 \n",
      "Start training model for fold 2...                                                  \n",
      "[0]\ttrain-mlogloss:1.07598\tvalidate-mlogloss:1.07807                                \n",
      "[200]\ttrain-mlogloss:0.67663\tvalidate-mlogloss:0.80171                              \n",
      "[400]\ttrain-mlogloss:0.66698\tvalidate-mlogloss:0.80162                              \n",
      "[600]\ttrain-mlogloss:0.66259\tvalidate-mlogloss:0.80335                              \n",
      "[694]\ttrain-mlogloss:0.66121\tvalidate-mlogloss:0.80497                              \n",
      "Predicting for fold 2...                                                            \n",
      "Average log loss: 0.8223596034487496                                                \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.6, 'colsample_bynode': 0.7, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 1, 'lambda': 1, 'learning_rate': 0.11895750793840458, 'max_delta_step': 4, 'max_depth': 3, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.6, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1916, 'num_feat': 70, 'step': 0.1}\n",
      "Start processing fold 1...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4543 14218 21945]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21945 21945 21945]                                                                 \n",
      "Start training model for fold 1...                                                  \n",
      "[0]\ttrain-mlogloss:1.07174\tvalidate-mlogloss:1.07142                                \n",
      "[200]\ttrain-mlogloss:0.70464\tvalidate-mlogloss:0.88067                              \n",
      "[400]\ttrain-mlogloss:0.67101\tvalidate-mlogloss:0.93090                              \n",
      "[568]\ttrain-mlogloss:0.65348\tvalidate-mlogloss:0.94270                              \n",
      "Predicting for fold 1...                                                            \n",
      "Start processing fold 2...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4542 14218 21946]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21946 21946 21946]                                                                 \n",
      "Start training model for fold 2...                                                  \n",
      "[0]\ttrain-mlogloss:1.07424\tvalidate-mlogloss:1.07405                                \n",
      "[200]\ttrain-mlogloss:0.70459\tvalidate-mlogloss:0.86946                              \n",
      "[400]\ttrain-mlogloss:0.67111\tvalidate-mlogloss:0.89703                              \n",
      "[525]\ttrain-mlogloss:0.65822\tvalidate-mlogloss:0.91291                              \n",
      "Predicting for fold 2...                                                            \n",
      "Average log loss: 0.927804961932104                                                 \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 0.8, 'colsample_bytree': 0.5, 'eval_metric': 'mlogloss', 'gamma': 8, 'lambda': 1, 'learning_rate': 0.29473629854190253, 'max_delta_step': 7, 'max_depth': 4, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.8, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1967, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4543 14218 21945]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21945 21945 21945]                                                                 \n",
      "Start training model for fold 1...                                                  \n",
      "[0]\ttrain-mlogloss:1.02215\tvalidate-mlogloss:1.02413                                \n",
      "[200]\ttrain-mlogloss:0.70990\tvalidate-mlogloss:0.79568                              \n",
      "[400]\ttrain-mlogloss:0.70890\tvalidate-mlogloss:0.79507                              \n",
      "[600]\ttrain-mlogloss:0.70737\tvalidate-mlogloss:0.79448                              \n",
      "[800]\ttrain-mlogloss:0.70699\tvalidate-mlogloss:0.79439                              \n",
      "[1000]\ttrain-mlogloss:0.70665\tvalidate-mlogloss:0.79436                             \n",
      "[1200]\ttrain-mlogloss:0.70661\tvalidate-mlogloss:0.79424                             \n",
      "[1400]\ttrain-mlogloss:0.70529\tvalidate-mlogloss:0.79344                             \n",
      "[1600]\ttrain-mlogloss:0.70507\tvalidate-mlogloss:0.79336                             \n",
      "[1800]\ttrain-mlogloss:0.70507\tvalidate-mlogloss:0.79349                             \n",
      "[1966]\ttrain-mlogloss:0.70505\tvalidate-mlogloss:0.79331                             \n",
      "Predicting for fold 1...                                                            \n",
      "Start processing fold 2...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4542 14218 21946]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21946 21946 21946]                                                                 \n",
      "Start training model for fold 2...                                                  \n",
      "[0]\ttrain-mlogloss:1.01997\tvalidate-mlogloss:1.02282                                \n",
      "[200]\ttrain-mlogloss:0.70740\tvalidate-mlogloss:0.79061                              \n",
      "[400]\ttrain-mlogloss:0.70666\tvalidate-mlogloss:0.79030                              \n",
      "[600]\ttrain-mlogloss:0.70587\tvalidate-mlogloss:0.78960                              \n",
      "[800]\ttrain-mlogloss:0.70553\tvalidate-mlogloss:0.78963                              \n",
      "[1000]\ttrain-mlogloss:0.70533\tvalidate-mlogloss:0.78963                             \n",
      "[1200]\ttrain-mlogloss:0.70530\tvalidate-mlogloss:0.78970                             \n",
      "[1221]\ttrain-mlogloss:0.70530\tvalidate-mlogloss:0.78976                             \n",
      "Predicting for fold 2...                                                            \n",
      "Average log loss: 0.7915313315450845                                                \n",
      "{'booster_params': {'alpha': 100, 'booster': 'gbtree', 'colsample_bylevel': 0.8, 'colsample_bynode': 1.0, 'colsample_bytree': 0.6, 'eval_metric': 'mlogloss', 'gamma': 4, 'lambda': 100, 'learning_rate': 0.09438799723338284, 'max_delta_step': 6, 'max_depth': 11, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 20, 'num_boost_round': 1302, 'num_feat': 50, 'step': 0.2}\n",
      "Start processing fold 1...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4543 14218 21945]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21945 21945 21945]                                                                 \n",
      "Start training model for fold 1...                                                  \n",
      "[0]\ttrain-mlogloss:1.07586\tvalidate-mlogloss:1.07774                                \n",
      "[200]\ttrain-mlogloss:0.74496\tvalidate-mlogloss:0.86072                              \n",
      "[400]\ttrain-mlogloss:0.73971\tvalidate-mlogloss:0.86504                              \n",
      "[497]\ttrain-mlogloss:0.73892\tvalidate-mlogloss:0.86476                              \n",
      "Predicting for fold 1...                                                            \n",
      "Start processing fold 2...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4542 14218 21946]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21946 21946 21946]                                                                 \n",
      "Start training model for fold 2...                                                  \n",
      "[0]\ttrain-mlogloss:1.07468\tvalidate-mlogloss:1.07572                                \n",
      "[200]\ttrain-mlogloss:0.74150\tvalidate-mlogloss:0.82404                              \n",
      "[400]\ttrain-mlogloss:0.73555\tvalidate-mlogloss:0.82286                              \n",
      "[600]\ttrain-mlogloss:0.73400\tvalidate-mlogloss:0.82237                              \n",
      "[800]\ttrain-mlogloss:0.73258\tvalidate-mlogloss:0.82228                              \n",
      "[1000]\ttrain-mlogloss:0.73182\tvalidate-mlogloss:0.82147                             \n",
      "[1200]\ttrain-mlogloss:0.73129\tvalidate-mlogloss:0.82170                             \n",
      "[1301]\ttrain-mlogloss:0.73108\tvalidate-mlogloss:0.82157                             \n",
      "Predicting for fold 2...                                                            \n",
      "Average log loss: 0.8431649779720758                                                \n",
      "{'booster_params': {'alpha': 1, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 0.7, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 0, 'lambda': 1, 'learning_rate': 0.1821875873959245, 'max_delta_step': 7, 'max_depth': 6, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 1.0, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1549, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4543 14218 21945]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21945 21945 21945]                                                                 \n",
      "Start training model for fold 1...                                                  \n",
      "[0]\ttrain-mlogloss:1.03616\tvalidate-mlogloss:1.04304                                \n",
      "[200]\ttrain-mlogloss:0.49337\tvalidate-mlogloss:0.80076                              \n",
      "[400]\ttrain-mlogloss:0.37633\tvalidate-mlogloss:0.82455                              \n",
      "[479]\ttrain-mlogloss:0.34022\tvalidate-mlogloss:0.82961                              \n",
      "Predicting for fold 1...                                                            \n",
      "Start processing fold 2...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4542 14218 21946]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21946 21946 21946]                                                                 \n",
      "Start training model for fold 2...                                                  \n",
      "[0]\ttrain-mlogloss:1.03622\tvalidate-mlogloss:1.04178                                \n",
      "[200]\ttrain-mlogloss:0.49460\tvalidate-mlogloss:0.78099                              \n",
      "[400]\ttrain-mlogloss:0.38040\tvalidate-mlogloss:0.79345                              \n",
      "[540]\ttrain-mlogloss:0.32247\tvalidate-mlogloss:0.80354                              \n",
      "Predicting for fold 2...                                                            \n",
      "Average log loss: 0.8165762712285847                                                \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.5, 'colsample_bynode': 0.5, 'colsample_bytree': 1.0, 'eval_metric': 'mlogloss', 'gamma': 4, 'lambda': 1, 'learning_rate': 0.07121327277294795, 'max_delta_step': 2, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.8, 'tree_method': 'hist'}, 'k_neighbors': 25, 'num_boost_round': 898, 'num_feat': 60, 'step': 0.3}\n",
      "Start processing fold 1...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4543 14218 21945]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21945 21945 21945]                                                                 \n",
      "Start training model for fold 1...                                                  \n",
      "[0]\ttrain-mlogloss:1.07576\tvalidate-mlogloss:1.07913                                \n",
      "[200]\ttrain-mlogloss:0.67666\tvalidate-mlogloss:0.82886                              \n",
      "[400]\ttrain-mlogloss:0.67161\tvalidate-mlogloss:0.83143                              \n",
      "[525]\ttrain-mlogloss:0.66968\tvalidate-mlogloss:0.83131                              \n",
      "Predicting for fold 1...                                                            \n",
      "Start processing fold 2...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4542 14218 21946]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21946 21946 21946]                                                                 \n",
      "Start training model for fold 2...                                                  \n",
      "[0]\ttrain-mlogloss:1.07592\tvalidate-mlogloss:1.07758                                \n",
      "[200]\ttrain-mlogloss:0.67967\tvalidate-mlogloss:0.81957                              \n",
      "[400]\ttrain-mlogloss:0.67392\tvalidate-mlogloss:0.81876                              \n",
      "[600]\ttrain-mlogloss:0.67205\tvalidate-mlogloss:0.81768                              \n",
      "[800]\ttrain-mlogloss:0.67107\tvalidate-mlogloss:0.81820                              \n",
      "[897]\ttrain-mlogloss:0.67071\tvalidate-mlogloss:0.81796                              \n",
      "Predicting for fold 2...                                                            \n",
      "Average log loss: 0.8246550944669895                                                \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.9, 'colsample_bynode': 0.6, 'colsample_bytree': 0.7, 'eval_metric': 'mlogloss', 'gamma': 5, 'lambda': 1, 'learning_rate': 0.19828471647549092, 'max_delta_step': 3, 'max_depth': 5, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.5, 'tree_method': 'hist'}, 'k_neighbors': 10, 'num_boost_round': 1951, 'num_feat': 80, 'step': 0.3}\n",
      "Start processing fold 1...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4543 14218 21945]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21945 21945 21945]                                                                 \n",
      "Start training model for fold 1...                                                  \n",
      "[0]\ttrain-mlogloss:1.04002\tvalidate-mlogloss:1.04332                                \n",
      "[200]\ttrain-mlogloss:0.68964\tvalidate-mlogloss:0.79608                              \n",
      "[400]\ttrain-mlogloss:0.68485\tvalidate-mlogloss:0.79473                              \n",
      "[600]\ttrain-mlogloss:0.68270\tvalidate-mlogloss:0.79504                              \n",
      "[800]\ttrain-mlogloss:0.68127\tvalidate-mlogloss:0.79420                              \n",
      "[1000]\ttrain-mlogloss:0.67935\tvalidate-mlogloss:0.79685                             \n",
      "[1196]\ttrain-mlogloss:0.67835\tvalidate-mlogloss:0.79642                             \n",
      "Predicting for fold 1...                                                            \n",
      "Start processing fold 2...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4542 14218 21946]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21946 21946 21946]                                                                 \n",
      "Start training model for fold 2...                                                  \n",
      "[0]\ttrain-mlogloss:1.04131\tvalidate-mlogloss:1.04402                                \n",
      "[200]\ttrain-mlogloss:0.69352\tvalidate-mlogloss:0.78858                              \n",
      "[400]\ttrain-mlogloss:0.68966\tvalidate-mlogloss:0.78690                              \n",
      "[600]\ttrain-mlogloss:0.68697\tvalidate-mlogloss:0.78598                              \n",
      "[800]\ttrain-mlogloss:0.68492\tvalidate-mlogloss:0.78597                              \n",
      "[1000]\ttrain-mlogloss:0.68362\tvalidate-mlogloss:0.78520                             \n",
      "[1200]\ttrain-mlogloss:0.68228\tvalidate-mlogloss:0.78560                             \n",
      "[1400]\ttrain-mlogloss:0.68166\tvalidate-mlogloss:0.78554                             \n",
      "[1428]\ttrain-mlogloss:0.68162\tvalidate-mlogloss:0.78551                             \n",
      "Predicting for fold 2...                                                            \n",
      "Average log loss: 0.7909713090002632                                                \n",
      "{'booster_params': {'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.8, 'colsample_bynode': 0.7, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 3, 'lambda': 1, 'learning_rate': 0.17058792506689038, 'max_delta_step': 7, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', 'subsample': 0.6, 'tree_method': 'hist'}, 'k_neighbors': 30, 'num_boost_round': 877, 'num_feat': 70, 'step': 0.1}\n",
      "Start processing fold 1...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4543 14218 21945]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21945 21945 21945]                                                                 \n",
      "Start training model for fold 1...                                                  \n",
      "[0]\ttrain-mlogloss:1.04050\tvalidate-mlogloss:1.06491                                \n",
      "[200]\ttrain-mlogloss:0.63041\tvalidate-mlogloss:0.92185                              \n",
      "[400]\ttrain-mlogloss:0.62092\tvalidate-mlogloss:0.93511                              \n",
      "[425]\ttrain-mlogloss:0.62017\tvalidate-mlogloss:0.93500                              \n",
      "Predicting for fold 1...                                                            \n",
      "Start processing fold 2...                                                          \n",
      "Class distribution before oversampling:                                             \n",
      "[ 4542 14218 21946]                                                                 \n",
      "Class distribution after oversampling:                                              \n",
      "[21946 21946 21946]                                                                 \n",
      "Start training model for fold 2...                                                  \n",
      "[0]\ttrain-mlogloss:1.03740\tvalidate-mlogloss:1.04566                                \n",
      "[200]\ttrain-mlogloss:0.63244\tvalidate-mlogloss:0.88325                              \n",
      "[400]\ttrain-mlogloss:0.62200\tvalidate-mlogloss:0.89083                              \n",
      "[422]\ttrain-mlogloss:0.62110\tvalidate-mlogloss:0.89022                              \n",
      "Predicting for fold 2...                                                            \n",
      "Average log loss: 0.9128027970125061                                                \n",
      "100%|| 100/100 [1:07:42<00:00, 40.63s/trial, best loss: 0.7790077492800599]\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "\n",
    "best_params = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the search, the best set of parameters are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1,\n",
       " 'colsample_bylevel': 3,\n",
       " 'colsample_bynode': 5,\n",
       " 'colsample_bytree': 3,\n",
       " 'gamma': 1,\n",
       " 'k_neighbors': 1,\n",
       " 'lambda': 0,\n",
       " 'learning_rate': 0.021239520998594802,\n",
       " 'max_delta_step': 7,\n",
       " 'max_depth': 4,\n",
       " 'num_boost_round': 1014,\n",
       " 'num_feat': 3,\n",
       " 'step': 2,\n",
       " 'subsample': 3}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for `hp.choice`, the returned values is not the actual values of the hyperparamter but simply the indices to the choice lists. \n",
    "\n",
    "* The `colsample_bynode` returns $5$, which corresponds to the sixth element of the choice list--- `[0.5, 0.6, 0.7, 0.8, 0.9, 1.0]`--- 1.0. \n",
    "\n",
    "* The `colsample_bytree` returns $3$, which correponds to  the fourth element of the choice list--- `[0.5, 0.6, 0.7, 0.8, 0.9, 1.0]`--- 0.8.\n",
    "\n",
    "We can simply use the dictionary from the standard output and modify as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'booster_params': {'alpha': 10,\n",
       "  'booster': 'gbtree',\n",
       "  'colsample_bylevel': 0.8,\n",
       "  'colsample_bynode': 1.0,\n",
       "  'colsample_bytree': 0.8,\n",
       "  'eval_metric': 'mlogloss',\n",
       "  'gamma': 1,\n",
       "  'lambda': 1,\n",
       "  'learning_rate': 0.021239520998594802,\n",
       "  'max_delta_step': 7,\n",
       "  'max_depth': 7,\n",
       "  'num_class': 3,\n",
       "  'objective': 'multi:softprob',\n",
       "  'predictor': 'cpu_predictor',\n",
       "  'subsample': 0.8,\n",
       "  'tree_method': 'hist'},\n",
       " 'k_neighbors': 10,\n",
       " 'num_boost_round': 10000,\n",
       " 'num_feat': 80,\n",
       " 'step': 0.3}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_params = {'booster_params': {\n",
    "    'alpha': 10, 'booster': 'gbtree', 'colsample_bylevel': 0.8, 'colsample_bynode': 1.0, 'colsample_bytree': 0.8, 'eval_metric': 'mlogloss', 'gamma': 1, \n",
    "    'lambda': 1, 'learning_rate': 0.021239520998594802, 'max_delta_step': 7, 'max_depth': 7, 'num_class': 3, 'objective': 'multi:softprob', 'predictor': 'cpu_predictor', \n",
    "    'subsample': 0.8, 'tree_method': 'hist'\n",
    "    }, \n",
    " 'k_neighbors': 10, \n",
    " 'num_boost_round': 10000, # Increase this to 2000 for better results\n",
    " 'num_feat': 80, \n",
    " 'step': 0.3}\n",
    "optimal_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this optimized set of parameters as a starting point for cross validation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with K-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "# Fold 1\n",
      "# Training set size 65129 Validation set size 16283\n",
      "################################################################################\n",
      "The training and validation sets are disjoint: True\n",
      "The preprocessor is not fitted yet. Fitting now...\n",
      "The label encoder is not fitted yet. Fitting now...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../outputs/pipeline/preprocessor_fold_1.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['../outputs/pipeline/label_encoder_fold_1.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/my_compute/lib/python3.9/site-packages/sklearn/base.py:402: UserWarning: X has feature names, but RFE was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature selector is not fitted yet. Fitting now...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../outputs/pipeline/feature_selector_fold_1.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before oversampling: [ 9085 28436 43891]\n",
      "The resampler is not fitted yet. Fitting now...\n",
      "Class distribution after oversampling: [43891 43891 43891]\n",
      "[0]\ttrain-mlogloss:1.08845\tvalidate-mlogloss:1.09113\n",
      "[1000]\ttrain-mlogloss:0.53024\tvalidate-mlogloss:0.80940\n",
      "[2000]\ttrain-mlogloss:0.47064\tvalidate-mlogloss:0.78136\n",
      "[3000]\ttrain-mlogloss:0.44456\tvalidate-mlogloss:0.76979\n",
      "[4000]\ttrain-mlogloss:0.43114\tvalidate-mlogloss:0.76326\n",
      "[5000]\ttrain-mlogloss:0.42237\tvalidate-mlogloss:0.75910\n",
      "[6000]\ttrain-mlogloss:0.41539\tvalidate-mlogloss:0.75606\n",
      "[7000]\ttrain-mlogloss:0.40996\tvalidate-mlogloss:0.75337\n",
      "[8000]\ttrain-mlogloss:0.40587\tvalidate-mlogloss:0.75141\n",
      "[9000]\ttrain-mlogloss:0.40194\tvalidate-mlogloss:0.74948\n",
      "[9999]\ttrain-mlogloss:0.39873\tvalidate-mlogloss:0.74785\n",
      "Multi-class logloss: 0.7478507008823111\n",
      "Balanced accuracy (Average recall): 0.5231483815217848\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.61      0.25      0.98      0.36      0.50      0.23      1817\n",
      "          1       0.72      0.38      0.92      0.50      0.59      0.33      5688\n",
      "          2       0.66      0.93      0.43      0.77      0.63      0.42      8778\n",
      "\n",
      "avg / total       0.67      0.67      0.66      0.63      0.60      0.37     16283\n",
      "\n",
      "################################################################################\n",
      "# Fold 2\n",
      "# Training set size 65129 Validation set size 16283\n",
      "################################################################################\n",
      "The training and validation sets are disjoint: True\n",
      "The preprocessor is not fitted yet. Fitting now...\n",
      "The label encoder is not fitted yet. Fitting now...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../outputs/pipeline/preprocessor_fold_2.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['../outputs/pipeline/label_encoder_fold_2.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/my_compute/lib/python3.9/site-packages/sklearn/base.py:402: UserWarning: X has feature names, but RFE was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature selector is not fitted yet. Fitting now...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../outputs/pipeline/feature_selector_fold_2.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before oversampling: [ 9085 28436 43891]\n",
      "The resampler is not fitted yet. Fitting now...\n",
      "Class distribution after oversampling: [43891 43891 43891]\n",
      "[0]\ttrain-mlogloss:1.08844\tvalidate-mlogloss:1.09078\n",
      "[1000]\ttrain-mlogloss:0.52905\tvalidate-mlogloss:0.81205\n",
      "[2000]\ttrain-mlogloss:0.46965\tvalidate-mlogloss:0.78548\n",
      "[3000]\ttrain-mlogloss:0.44314\tvalidate-mlogloss:0.77362\n",
      "[4000]\ttrain-mlogloss:0.42977\tvalidate-mlogloss:0.76731\n",
      "[5000]\ttrain-mlogloss:0.42130\tvalidate-mlogloss:0.76335\n",
      "[6000]\ttrain-mlogloss:0.41425\tvalidate-mlogloss:0.76015\n",
      "[7000]\ttrain-mlogloss:0.40870\tvalidate-mlogloss:0.75754\n",
      "[8000]\ttrain-mlogloss:0.40453\tvalidate-mlogloss:0.75559\n",
      "[9000]\ttrain-mlogloss:0.40107\tvalidate-mlogloss:0.75400\n",
      "[9999]\ttrain-mlogloss:0.39801\tvalidate-mlogloss:0.75246\n",
      "Multi-class logloss: 0.7524566794666495\n",
      "Balanced accuracy (Average recall): 0.5328806383228937\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.53      0.29      0.97      0.37      0.53      0.26      1817\n",
      "          1       0.70      0.39      0.91      0.50      0.60      0.34      5687\n",
      "          2       0.67      0.92      0.46      0.77      0.65      0.44      8779\n",
      "\n",
      "avg / total       0.66      0.66      0.67      0.63      0.62      0.39     16283\n",
      "\n",
      "################################################################################\n",
      "# Fold 3\n",
      "# Training set size 65130 Validation set size 16282\n",
      "################################################################################\n",
      "The training and validation sets are disjoint: True\n",
      "The preprocessor is not fitted yet. Fitting now...\n",
      "The label encoder is not fitted yet. Fitting now...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../outputs/pipeline/preprocessor_fold_3.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['../outputs/pipeline/label_encoder_fold_3.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/my_compute/lib/python3.9/site-packages/sklearn/base.py:402: UserWarning: X has feature names, but RFE was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature selector is not fitted yet. Fitting now...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../outputs/pipeline/feature_selector_fold_3.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before oversampling: [ 9085 28436 43891]\n",
      "The resampler is not fitted yet. Fitting now...\n",
      "Class distribution after oversampling: [43891 43891 43891]\n",
      "[0]\ttrain-mlogloss:1.08874\tvalidate-mlogloss:1.09098\n",
      "[1000]\ttrain-mlogloss:0.53139\tvalidate-mlogloss:0.83072\n",
      "[2000]\ttrain-mlogloss:0.47135\tvalidate-mlogloss:0.80426\n",
      "[3000]\ttrain-mlogloss:0.44529\tvalidate-mlogloss:0.79239\n",
      "[4000]\ttrain-mlogloss:0.43074\tvalidate-mlogloss:0.78547\n",
      "[5000]\ttrain-mlogloss:0.42167\tvalidate-mlogloss:0.78172\n",
      "[6000]\ttrain-mlogloss:0.41443\tvalidate-mlogloss:0.77853\n",
      "[7000]\ttrain-mlogloss:0.40911\tvalidate-mlogloss:0.77593\n",
      "[8000]\ttrain-mlogloss:0.40492\tvalidate-mlogloss:0.77388\n",
      "[9000]\ttrain-mlogloss:0.40142\tvalidate-mlogloss:0.77237\n",
      "[9999]\ttrain-mlogloss:0.39841\tvalidate-mlogloss:0.77092\n",
      "Multi-class logloss: 0.770919568089847\n",
      "Balanced accuracy (Average recall): 0.4961868231467715\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.57      0.23      0.98      0.33      0.47      0.21      1817\n",
      "          1       0.76      0.30      0.95      0.43      0.53      0.27      5687\n",
      "          2       0.63      0.96      0.35      0.76      0.58      0.36      8778\n",
      "\n",
      "avg / total       0.67      0.65      0.63      0.60      0.55      0.31     16282\n",
      "\n",
      "################################################################################\n",
      "# Fold 4\n",
      "# Training set size 65130 Validation set size 16282\n",
      "################################################################################\n",
      "The training and validation sets are disjoint: True\n",
      "The preprocessor is not fitted yet. Fitting now...\n",
      "The label encoder is not fitted yet. Fitting now...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../outputs/pipeline/preprocessor_fold_4.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['../outputs/pipeline/label_encoder_fold_4.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/my_compute/lib/python3.9/site-packages/sklearn/base.py:402: UserWarning: X has feature names, but RFE was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature selector is not fitted yet. Fitting now...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../outputs/pipeline/feature_selector_fold_4.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before oversampling: [ 9085 28436 43891]\n",
      "The resampler is not fitted yet. Fitting now...\n",
      "Class distribution after oversampling: [43891 43891 43891]\n",
      "[0]\ttrain-mlogloss:1.08889\tvalidate-mlogloss:1.09128\n",
      "[1000]\ttrain-mlogloss:0.53356\tvalidate-mlogloss:0.80626\n",
      "[2000]\ttrain-mlogloss:0.47320\tvalidate-mlogloss:0.77772\n",
      "[3000]\ttrain-mlogloss:0.44720\tvalidate-mlogloss:0.76519\n",
      "[4000]\ttrain-mlogloss:0.43358\tvalidate-mlogloss:0.75876\n",
      "[5000]\ttrain-mlogloss:0.42403\tvalidate-mlogloss:0.75426\n",
      "[6000]\ttrain-mlogloss:0.41704\tvalidate-mlogloss:0.75095\n",
      "[7000]\ttrain-mlogloss:0.41220\tvalidate-mlogloss:0.74846\n",
      "[8000]\ttrain-mlogloss:0.40783\tvalidate-mlogloss:0.74620\n",
      "[9000]\ttrain-mlogloss:0.40401\tvalidate-mlogloss:0.74432\n",
      "[9999]\ttrain-mlogloss:0.40082\tvalidate-mlogloss:0.74268\n",
      "Multi-class logloss: 0.7426773627769532\n",
      "Balanced accuracy (Average recall): 0.5322348848702568\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.61      0.26      0.98      0.37      0.51      0.24      1817\n",
      "          1       0.69      0.42      0.90      0.52      0.61      0.36      5687\n",
      "          2       0.67      0.92      0.47      0.77      0.65      0.45      8778\n",
      "\n",
      "avg / total       0.67      0.67      0.67      0.64      0.62      0.39     16282\n",
      "\n",
      "################################################################################\n",
      "# Fold 5\n",
      "# Training set size 65130 Validation set size 16282\n",
      "################################################################################\n",
      "The training and validation sets are disjoint: True\n",
      "The preprocessor is not fitted yet. Fitting now...\n",
      "The label encoder is not fitted yet. Fitting now...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../outputs/pipeline/preprocessor_fold_5.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['../outputs/pipeline/label_encoder_fold_5.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/my_compute/lib/python3.9/site-packages/sklearn/base.py:402: UserWarning: X has feature names, but RFE was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature selector is not fitted yet. Fitting now...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../outputs/pipeline/feature_selector_fold_5.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before oversampling: [ 9085 28436 43891]\n",
      "The resampler is not fitted yet. Fitting now...\n",
      "Class distribution after oversampling: [43891 43891 43891]\n",
      "[0]\ttrain-mlogloss:1.08903\tvalidate-mlogloss:1.09133\n",
      "[1000]\ttrain-mlogloss:0.53021\tvalidate-mlogloss:0.81383\n",
      "[2000]\ttrain-mlogloss:0.46897\tvalidate-mlogloss:0.79257\n",
      "[3000]\ttrain-mlogloss:0.44152\tvalidate-mlogloss:0.78082\n",
      "[4000]\ttrain-mlogloss:0.42662\tvalidate-mlogloss:0.77500\n",
      "[5000]\ttrain-mlogloss:0.41738\tvalidate-mlogloss:0.77083\n",
      "[6000]\ttrain-mlogloss:0.41053\tvalidate-mlogloss:0.76801\n",
      "[7000]\ttrain-mlogloss:0.40501\tvalidate-mlogloss:0.76551\n",
      "[8000]\ttrain-mlogloss:0.40045\tvalidate-mlogloss:0.76339\n",
      "[9000]\ttrain-mlogloss:0.39680\tvalidate-mlogloss:0.76182\n",
      "[9999]\ttrain-mlogloss:0.39374\tvalidate-mlogloss:0.76048\n",
      "Multi-class logloss: 0.7604833171024694\n",
      "Balanced accuracy (Average recall): 0.570179560010352\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.50      0.40      0.95      0.45      0.62      0.36      1817\n",
      "          1       0.70      0.41      0.91      0.52      0.61      0.35      5687\n",
      "          2       0.68      0.89      0.52      0.78      0.68      0.48      8778\n",
      "\n",
      "avg / total       0.67      0.67      0.70      0.65      0.65      0.42     16282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Out-of-fold prediction dictionary\n",
    "oof = {}\n",
    "# Feature importance container\n",
    "feat_imp_list = []\n",
    "# Classfication report container\n",
    "clf_report_list = []\n",
    "# CV splitter\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=rs)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_X, train_y)):\n",
    "    \n",
    "    # Print messages to console\n",
    "    print('#'* 80)\n",
    "    print('# Fold',fold + 1)\n",
    "    print('# Training set size', len(train_idx), 'Validation set size', len(val_idx))\n",
    "    print('#'* 80)\n",
    "    \n",
    "    # Check that the indices are indeed disjoint for training and validation set\n",
    "    print('The training and validation sets are disjoint:', set(train_idx).isdisjoint(set(val_idx)))\n",
    "    \n",
    "    # Train and validation sets\n",
    "    fold_train_X, fold_train_y = train_X.iloc[train_idx], train_y[train_idx]\n",
    "    fold_val_X, fold_val_y = train_X.iloc[val_idx], train_y[val_idx]\n",
    "        \n",
    "    # Preprocessing\n",
    "    preprocessor = joblib.load('../outputs/pipeline/preprocessor.joblib')\n",
    "    label_encoder = joblib.load('../outputs/pipeline/label_encoder.joblib')\n",
    "    try:\n",
    "        preprocessor.transform(train_X)\n",
    "    except NotFittedError:\n",
    "         print('The preprocessor is not fitted yet. Fitting now...')\n",
    "    try:\n",
    "        label_encoder.transform(train_y)\n",
    "    except NotFittedError:\n",
    "         print('The label encoder is not fitted yet. Fitting now...')\n",
    "    # Fit and transform on training data\n",
    "    fold_train_X = preprocessor.fit_transform(train_X)\n",
    "    fold_train_y = label_encoder.fit_transform(train_y)\n",
    "    # Transform validation data\n",
    "    fold_val_X = preprocessor.transform(fold_val_X)\n",
    "    fold_val_y = label_encoder.transform(fold_val_y)\n",
    "    # Store fitted preprocessor and label encoder\n",
    "    joblib.dump(preprocessor, f'../outputs/pipeline/preprocessor_fold_{fold + 1}.joblib')\n",
    "    joblib.dump(label_encoder, f'../outputs/pipeline/label_encoder_fold_{fold + 1}.joblib')\n",
    "    \n",
    "    # Feature selection\n",
    "    selector = RFE(estimator=DecisionTreeClassifier(random_state=rs), n_features_to_select=optimal_params['num_feat'], step=optimal_params['step'])\n",
    "    try:\n",
    "        selector.transform(fold_train_X)\n",
    "    except NotFittedError:\n",
    "         print('The feature selector is not fitted yet. Fitting now...')\n",
    "    fold_train_X = selector.fit_transform(fold_train_X, fold_train_y)\n",
    "    fold_val_X = selector.transform(fold_val_X)\n",
    "    # Store fitted feature selector\n",
    "    joblib.dump(selector, f'../outputs/pipeline/feature_selector_fold_{fold + 1}.joblib')\n",
    "    \n",
    "    # Oversampling training data\n",
    "    print('Class distribution before oversampling:', np.unique(fold_train_y, return_counts=True)[1])\n",
    "    smote = SMOTE(sampling_strategy='not majority', k_neighbors=optimal_params['k_neighbors'], random_state=rs)\n",
    "    try:\n",
    "        smote.get_feature_names_out()\n",
    "    except ValueError:\n",
    "        print('The resampler is not fitted yet. Fitting now...')\n",
    "    fold_train_X, fold_train_y = smote.fit_resample(fold_train_X, fold_train_y)\n",
    "    print('Class distribution after oversampling:', np.unique(fold_train_y, return_counts=True)[1])\n",
    "    \n",
    "    # Model \n",
    "    dtrain = xgb.DMatrix(data=fold_train_X, label=fold_train_y, feature_names=selector.get_feature_names_out().tolist())\n",
    "    dvalid = xgb.DMatrix(data=fold_val_X, label=fold_val_y, feature_names=selector.get_feature_names_out().tolist())\n",
    "    model = xgb.train(\n",
    "        params=optimal_params['booster_params'],\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=optimal_params['num_boost_round'],\n",
    "        early_stopping_rounds=400,\n",
    "        evals=[(dtrain, 'train'), (dvalid, 'validate')], \n",
    "        verbose_eval=1000\n",
    "    )\n",
    "    model.save_model(model_path + f'model_fold_{fold + 1}.xgb')\n",
    "    \n",
    "    # Feature importance for the current fold\n",
    "    # The booster object has a get_score method that returns a dictionary of feature names and their importance scores\n",
    "    feat_imp = model.get_score(importance_type='weight')\n",
    "    df = pd.DataFrame({'feature': feat_imp.keys(), f'importance_{fold + 1}': feat_imp.values()})\n",
    "    # Add the current fold's feature importance to the container\n",
    "    feat_imp_list.append(df)\n",
    "    \n",
    "    # Out-of-fold prediction, matrix of probabilities with shape (n_samples, n_classes)\n",
    "    oof_pred = model.predict(dvalid)\n",
    "    pred_classes = np.argmax(oof_pred, axis=1)\n",
    "    # Add the current fold's out-of-fold prediction to the dictionary with target values\n",
    "    oof[f'fold_{fold + 1}'] = {'target': fold_val_y, 'prediction': oof_pred}\n",
    "    # Add report to container\n",
    "    clf_report_list.append(classification_report_imbalanced(y_true=fold_val_y, y_pred=pred_classes, output_dict=True))\n",
    "    # Print multi-class logloss and balanced accuracy (average recall (i.e., the ability of the classifier to find all the positive samples) for each class)\n",
    "    print('Multi-class logloss:', log_loss(y_true=fold_val_y, y_pred=oof_pred, labels=[0, 1, 2]))\n",
    "    print('Balanced accuracy (Average recall):', balanced_accuracy_score(y_true=fold_val_y, y_pred=pred_classes))\n",
    "    print(classification_report_imbalanced(y_true=fold_val_y, y_pred=pred_classes))\n",
    "    \n",
    "    del fold_train_X, fold_train_y, fold_val_X, fold_val_y, dtrain, dvalid, model, feat_imp, oof_pred, pred_classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on Validation Set\n",
    "\n",
    "One quick way to benchmark our test prediction is using the multi-class log-loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7478507008823111\n",
      "0.7524566794666495\n",
      "0.770919568089847\n",
      "0.7426773627769532\n"
     ]
    }
   ],
   "source": [
    "for fold in range(4):\n",
    "    print(log_loss(y_true=oof[f'fold_{fold + 1}']['target'], y_pred=oof[f'fold_{fold + 1}']['prediction'], labels=[0, 1, 2]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random guess model, when the dataset is imbalanced, simply uses the overall proportions of the traning examples belonging to each class as the predicted probabilities of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_guess_log_loss(class_ratio, sample_size):\n",
    "    \n",
    "    if sum(class_ratio) != 1.0:\n",
    "        class_ratio[-1] += 1 - sum(class_ratio) # Adjust last class ratio to make the sum equal to 1\n",
    "    \n",
    "    true_y = []\n",
    "    for ith_class_label, ith_class_ratio in enumerate(class_ratio):\n",
    "        true_y = true_y + [ith_class_label for val in range(int(ith_class_ratio * sample_size))]\n",
    "        \n",
    "    # The 'random guess' predictions simply predict the class ratios\n",
    "    preds=[]\n",
    "    for i in range(sample_size):\n",
    "        preds += [class_ratio]\n",
    "\n",
    "    return log_loss(true_y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11158877, 0.34932138, 0.53908985])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_ratios = np.unique(oof[f'fold_1']['target'], return_counts=True)[1] / len(oof[f'fold_1']['target'])\n",
    "class_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9451992596686541"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_guess_log_loss(class_ratios, sample_size=len(oof[f'fold_1']['target']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, our log loss values are all lower than this benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<30', '>30', 'NO'], dtype=object)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<30', '>30', 'NO'], dtype=object)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f6740d3ac40>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f67408204f0>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f67406ac3a0>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f6740a3e370>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for fold in range(4):\n",
    "    # Compute predictions for the current fold\n",
    "    ConfusionMatrixDisplay(confusion_matrix(y_true=oof[f'fold_{fold + 1}']['target'], y_pred=np.argmax(oof[f'fold_{fold + 1}']['prediction'], axis=1), labels=[0, 1, 2], normalize='true')).plot();\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the nature of the problem, we would likely wish to focus on the top row. Specifically, we would want to maximize the top left corner, which corresponds to cases where the model correctly classified patient as a member of the '<30' group, i.e., the patient was readmitted into the hospital within 30 days. This is because the cost of miss-classifying a patient who is a high-risk in terms of within 30-day readmission is very high. \n",
    "\n",
    "* Even though our model out performs the random guessing model. It still remains suboptimal in terms of our objective--- maximizing the classifier's ability to correctly identify patients who are high-risk of readmission. \n",
    "\n",
    "* For the first fold, the classifier correctly classified class 0 ('<30') patients $26\\%$ of the time; however, its false negative rate is $73\\%$, which is too high. This performance is consistent across all folds."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the feature importance for each fold\n",
    "for i in range(5):\n",
    "    feat_imp_list[i].sort_values(by=f'importance_{i + 1}', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_1</th>\n",
       "      <th>importance_2</th>\n",
       "      <th>importance_3</th>\n",
       "      <th>importance_4</th>\n",
       "      <th>importance_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>number_inpatient_max_by_patient_nbr</td>\n",
       "      <td>10015.0</td>\n",
       "      <td>9602.0</td>\n",
       "      <td>9626.0</td>\n",
       "      <td>9908.0</td>\n",
       "      <td>9836.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>admission_source_id</td>\n",
       "      <td>10282.0</td>\n",
       "      <td>10191.0</td>\n",
       "      <td>10436.0</td>\n",
       "      <td>10657.0</td>\n",
       "      <td>10056.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>number_inpatient_mean_by_patient_nbr</td>\n",
       "      <td>10711.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10625.0</td>\n",
       "      <td>10683.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>time_in_hospital_mean_by_patient_nbr</td>\n",
       "      <td>10764.0</td>\n",
       "      <td>10896.0</td>\n",
       "      <td>10737.0</td>\n",
       "      <td>10908.0</td>\n",
       "      <td>10554.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>time_in_hospital_sum_by_patient_nbr</td>\n",
       "      <td>10828.0</td>\n",
       "      <td>10234.0</td>\n",
       "      <td>10831.0</td>\n",
       "      <td>10597.0</td>\n",
       "      <td>10812.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>num_medications_median_by_patient_nbr</td>\n",
       "      <td>10859.0</td>\n",
       "      <td>10815.0</td>\n",
       "      <td>10832.0</td>\n",
       "      <td>10785.0</td>\n",
       "      <td>10984.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>insulin</td>\n",
       "      <td>11258.0</td>\n",
       "      <td>10972.0</td>\n",
       "      <td>11422.0</td>\n",
       "      <td>11505.0</td>\n",
       "      <td>11268.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>admission_type_id</td>\n",
       "      <td>11397.0</td>\n",
       "      <td>10999.0</td>\n",
       "      <td>11419.0</td>\n",
       "      <td>11315.0</td>\n",
       "      <td>11013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>num_medications_mean_by_patient_nbr</td>\n",
       "      <td>12050.0</td>\n",
       "      <td>11707.0</td>\n",
       "      <td>11982.0</td>\n",
       "      <td>12315.0</td>\n",
       "      <td>11982.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>number_diagnoses_sum_by_patient_nbr</td>\n",
       "      <td>12113.0</td>\n",
       "      <td>12008.0</td>\n",
       "      <td>12356.0</td>\n",
       "      <td>12365.0</td>\n",
       "      <td>12327.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>num_medications_max_by_patient_nbr</td>\n",
       "      <td>12234.0</td>\n",
       "      <td>11623.0</td>\n",
       "      <td>11547.0</td>\n",
       "      <td>11724.0</td>\n",
       "      <td>12200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>number_inpatient</td>\n",
       "      <td>13067.0</td>\n",
       "      <td>12133.0</td>\n",
       "      <td>12541.0</td>\n",
       "      <td>12768.0</td>\n",
       "      <td>12785.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>time_in_hospital</td>\n",
       "      <td>13160.0</td>\n",
       "      <td>13119.0</td>\n",
       "      <td>13357.0</td>\n",
       "      <td>13331.0</td>\n",
       "      <td>13238.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>num_lab_procedures_median_by_patient_nbr</td>\n",
       "      <td>13660.0</td>\n",
       "      <td>13553.0</td>\n",
       "      <td>13856.0</td>\n",
       "      <td>13542.0</td>\n",
       "      <td>14085.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>num_medications_sum_by_patient_nbr</td>\n",
       "      <td>14325.0</td>\n",
       "      <td>13920.0</td>\n",
       "      <td>14273.0</td>\n",
       "      <td>14417.0</td>\n",
       "      <td>14673.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>payer_code</td>\n",
       "      <td>14539.0</td>\n",
       "      <td>14353.0</td>\n",
       "      <td>14724.0</td>\n",
       "      <td>14403.0</td>\n",
       "      <td>14862.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>num_medications</td>\n",
       "      <td>15344.0</td>\n",
       "      <td>15033.0</td>\n",
       "      <td>15390.0</td>\n",
       "      <td>15226.0</td>\n",
       "      <td>15551.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>num_lab_procedures_mean_by_patient_nbr</td>\n",
       "      <td>15422.0</td>\n",
       "      <td>14943.0</td>\n",
       "      <td>15206.0</td>\n",
       "      <td>15064.0</td>\n",
       "      <td>15651.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>num_lab_procedures_max_by_patient_nbr</td>\n",
       "      <td>15673.0</td>\n",
       "      <td>16001.0</td>\n",
       "      <td>15720.0</td>\n",
       "      <td>15855.0</td>\n",
       "      <td>15835.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>num_lab_procedures_sum_by_patient_nbr</td>\n",
       "      <td>17968.0</td>\n",
       "      <td>17845.0</td>\n",
       "      <td>18051.0</td>\n",
       "      <td>18399.0</td>\n",
       "      <td>18795.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>discharge_disposition_id</td>\n",
       "      <td>19304.0</td>\n",
       "      <td>19040.0</td>\n",
       "      <td>19726.0</td>\n",
       "      <td>19994.0</td>\n",
       "      <td>19529.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>num_lab_procedures</td>\n",
       "      <td>19421.0</td>\n",
       "      <td>19006.0</td>\n",
       "      <td>19619.0</td>\n",
       "      <td>19650.0</td>\n",
       "      <td>20318.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>diag_2</td>\n",
       "      <td>30650.0</td>\n",
       "      <td>30177.0</td>\n",
       "      <td>30412.0</td>\n",
       "      <td>30330.0</td>\n",
       "      <td>32092.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>diag_3</td>\n",
       "      <td>30700.0</td>\n",
       "      <td>30799.0</td>\n",
       "      <td>30780.0</td>\n",
       "      <td>30828.0</td>\n",
       "      <td>31664.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>diag_1</td>\n",
       "      <td>32147.0</td>\n",
       "      <td>31763.0</td>\n",
       "      <td>32236.0</td>\n",
       "      <td>32361.0</td>\n",
       "      <td>33420.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     feature  importance_1  importance_2  \\\n",
       "0        number_inpatient_max_by_patient_nbr       10015.0        9602.0   \n",
       "1                        admission_source_id       10282.0       10191.0   \n",
       "2       number_inpatient_mean_by_patient_nbr       10711.0           NaN   \n",
       "3       time_in_hospital_mean_by_patient_nbr       10764.0       10896.0   \n",
       "4        time_in_hospital_sum_by_patient_nbr       10828.0       10234.0   \n",
       "5      num_medications_median_by_patient_nbr       10859.0       10815.0   \n",
       "6                                    insulin       11258.0       10972.0   \n",
       "7                          admission_type_id       11397.0       10999.0   \n",
       "8        num_medications_mean_by_patient_nbr       12050.0       11707.0   \n",
       "9        number_diagnoses_sum_by_patient_nbr       12113.0       12008.0   \n",
       "10        num_medications_max_by_patient_nbr       12234.0       11623.0   \n",
       "11                          number_inpatient       13067.0       12133.0   \n",
       "12                          time_in_hospital       13160.0       13119.0   \n",
       "13  num_lab_procedures_median_by_patient_nbr       13660.0       13553.0   \n",
       "14        num_medications_sum_by_patient_nbr       14325.0       13920.0   \n",
       "15                                payer_code       14539.0       14353.0   \n",
       "16                           num_medications       15344.0       15033.0   \n",
       "17    num_lab_procedures_mean_by_patient_nbr       15422.0       14943.0   \n",
       "18     num_lab_procedures_max_by_patient_nbr       15673.0       16001.0   \n",
       "19     num_lab_procedures_sum_by_patient_nbr       17968.0       17845.0   \n",
       "20                  discharge_disposition_id       19304.0       19040.0   \n",
       "21                        num_lab_procedures       19421.0       19006.0   \n",
       "22                                    diag_2       30650.0       30177.0   \n",
       "23                                    diag_3       30700.0       30799.0   \n",
       "24                                    diag_1       32147.0       31763.0   \n",
       "\n",
       "    importance_3  importance_4  importance_5  \n",
       "0         9626.0        9908.0        9836.0  \n",
       "1        10436.0       10657.0       10056.0  \n",
       "2            NaN       10625.0       10683.0  \n",
       "3        10737.0       10908.0       10554.0  \n",
       "4        10831.0       10597.0       10812.0  \n",
       "5        10832.0       10785.0       10984.0  \n",
       "6        11422.0       11505.0       11268.0  \n",
       "7        11419.0       11315.0       11013.0  \n",
       "8        11982.0       12315.0       11982.0  \n",
       "9        12356.0       12365.0       12327.0  \n",
       "10       11547.0       11724.0       12200.0  \n",
       "11       12541.0       12768.0       12785.0  \n",
       "12       13357.0       13331.0       13238.0  \n",
       "13       13856.0       13542.0       14085.0  \n",
       "14       14273.0       14417.0       14673.0  \n",
       "15       14724.0       14403.0       14862.0  \n",
       "16       15390.0       15226.0       15551.0  \n",
       "17       15206.0       15064.0       15651.0  \n",
       "18       15720.0       15855.0       15835.0  \n",
       "19       18051.0       18399.0       18795.0  \n",
       "20       19726.0       19994.0       19529.0  \n",
       "21       19619.0       19650.0       20318.0  \n",
       "22       30412.0       30330.0       32092.0  \n",
       "23       30780.0       30828.0       31664.0  \n",
       "24       32236.0       32361.0       33420.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_imp = reduce(lambda x, y: pd.merge(x.iloc[-top_num_features:], y.iloc[-top_num_features:], on='feature', how='left'), feat_imp_list)\n",
    "feat_imp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, all five folds agree on these feature importances except for `number_inpatient_mean_by_patient_nbr`, which are not in the top 25 for fold two and three. We can extract its importance weights from thoes folds and replace the nan's in the data frame above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp['importance_2'].fillna(feat_imp_list[1].loc[feat_imp_list[1]['feature'] == 'number_inpatient_mean_by_patient_nbr', 'importance_2'].values[0], inplace=True)\n",
    "feat_imp['importance_3'].fillna(feat_imp_list[2].loc[feat_imp_list[2]['feature'] == 'number_inpatient_mean_by_patient_nbr', 'importance_3'].values[0], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the average importance across all five folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_1</th>\n",
       "      <th>importance_2</th>\n",
       "      <th>importance_3</th>\n",
       "      <th>importance_4</th>\n",
       "      <th>importance_5</th>\n",
       "      <th>avg_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>number_inpatient_max_by_patient_nbr</td>\n",
       "      <td>10015.0</td>\n",
       "      <td>9602.0</td>\n",
       "      <td>9626.0</td>\n",
       "      <td>9908.0</td>\n",
       "      <td>9836.0</td>\n",
       "      <td>9797.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>admission_source_id</td>\n",
       "      <td>10282.0</td>\n",
       "      <td>10191.0</td>\n",
       "      <td>10436.0</td>\n",
       "      <td>10657.0</td>\n",
       "      <td>10056.0</td>\n",
       "      <td>10324.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>number_inpatient_mean_by_patient_nbr</td>\n",
       "      <td>10711.0</td>\n",
       "      <td>8387.0</td>\n",
       "      <td>8156.0</td>\n",
       "      <td>10625.0</td>\n",
       "      <td>10683.0</td>\n",
       "      <td>9712.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>time_in_hospital_mean_by_patient_nbr</td>\n",
       "      <td>10764.0</td>\n",
       "      <td>10896.0</td>\n",
       "      <td>10737.0</td>\n",
       "      <td>10908.0</td>\n",
       "      <td>10554.0</td>\n",
       "      <td>10771.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>time_in_hospital_sum_by_patient_nbr</td>\n",
       "      <td>10828.0</td>\n",
       "      <td>10234.0</td>\n",
       "      <td>10831.0</td>\n",
       "      <td>10597.0</td>\n",
       "      <td>10812.0</td>\n",
       "      <td>10660.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>num_medications_median_by_patient_nbr</td>\n",
       "      <td>10859.0</td>\n",
       "      <td>10815.0</td>\n",
       "      <td>10832.0</td>\n",
       "      <td>10785.0</td>\n",
       "      <td>10984.0</td>\n",
       "      <td>10855.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>insulin</td>\n",
       "      <td>11258.0</td>\n",
       "      <td>10972.0</td>\n",
       "      <td>11422.0</td>\n",
       "      <td>11505.0</td>\n",
       "      <td>11268.0</td>\n",
       "      <td>11285.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>admission_type_id</td>\n",
       "      <td>11397.0</td>\n",
       "      <td>10999.0</td>\n",
       "      <td>11419.0</td>\n",
       "      <td>11315.0</td>\n",
       "      <td>11013.0</td>\n",
       "      <td>11228.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>num_medications_mean_by_patient_nbr</td>\n",
       "      <td>12050.0</td>\n",
       "      <td>11707.0</td>\n",
       "      <td>11982.0</td>\n",
       "      <td>12315.0</td>\n",
       "      <td>11982.0</td>\n",
       "      <td>12007.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>number_diagnoses_sum_by_patient_nbr</td>\n",
       "      <td>12113.0</td>\n",
       "      <td>12008.0</td>\n",
       "      <td>12356.0</td>\n",
       "      <td>12365.0</td>\n",
       "      <td>12327.0</td>\n",
       "      <td>12233.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>num_medications_max_by_patient_nbr</td>\n",
       "      <td>12234.0</td>\n",
       "      <td>11623.0</td>\n",
       "      <td>11547.0</td>\n",
       "      <td>11724.0</td>\n",
       "      <td>12200.0</td>\n",
       "      <td>11865.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>number_inpatient</td>\n",
       "      <td>13067.0</td>\n",
       "      <td>12133.0</td>\n",
       "      <td>12541.0</td>\n",
       "      <td>12768.0</td>\n",
       "      <td>12785.0</td>\n",
       "      <td>12658.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>time_in_hospital</td>\n",
       "      <td>13160.0</td>\n",
       "      <td>13119.0</td>\n",
       "      <td>13357.0</td>\n",
       "      <td>13331.0</td>\n",
       "      <td>13238.0</td>\n",
       "      <td>13241.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>num_lab_procedures_median_by_patient_nbr</td>\n",
       "      <td>13660.0</td>\n",
       "      <td>13553.0</td>\n",
       "      <td>13856.0</td>\n",
       "      <td>13542.0</td>\n",
       "      <td>14085.0</td>\n",
       "      <td>13739.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>num_medications_sum_by_patient_nbr</td>\n",
       "      <td>14325.0</td>\n",
       "      <td>13920.0</td>\n",
       "      <td>14273.0</td>\n",
       "      <td>14417.0</td>\n",
       "      <td>14673.0</td>\n",
       "      <td>14321.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>payer_code</td>\n",
       "      <td>14539.0</td>\n",
       "      <td>14353.0</td>\n",
       "      <td>14724.0</td>\n",
       "      <td>14403.0</td>\n",
       "      <td>14862.0</td>\n",
       "      <td>14576.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>num_medications</td>\n",
       "      <td>15344.0</td>\n",
       "      <td>15033.0</td>\n",
       "      <td>15390.0</td>\n",
       "      <td>15226.0</td>\n",
       "      <td>15551.0</td>\n",
       "      <td>15308.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>num_lab_procedures_mean_by_patient_nbr</td>\n",
       "      <td>15422.0</td>\n",
       "      <td>14943.0</td>\n",
       "      <td>15206.0</td>\n",
       "      <td>15064.0</td>\n",
       "      <td>15651.0</td>\n",
       "      <td>15257.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>num_lab_procedures_max_by_patient_nbr</td>\n",
       "      <td>15673.0</td>\n",
       "      <td>16001.0</td>\n",
       "      <td>15720.0</td>\n",
       "      <td>15855.0</td>\n",
       "      <td>15835.0</td>\n",
       "      <td>15816.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>num_lab_procedures_sum_by_patient_nbr</td>\n",
       "      <td>17968.0</td>\n",
       "      <td>17845.0</td>\n",
       "      <td>18051.0</td>\n",
       "      <td>18399.0</td>\n",
       "      <td>18795.0</td>\n",
       "      <td>18211.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>discharge_disposition_id</td>\n",
       "      <td>19304.0</td>\n",
       "      <td>19040.0</td>\n",
       "      <td>19726.0</td>\n",
       "      <td>19994.0</td>\n",
       "      <td>19529.0</td>\n",
       "      <td>19518.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>num_lab_procedures</td>\n",
       "      <td>19421.0</td>\n",
       "      <td>19006.0</td>\n",
       "      <td>19619.0</td>\n",
       "      <td>19650.0</td>\n",
       "      <td>20318.0</td>\n",
       "      <td>19602.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>diag_2</td>\n",
       "      <td>30650.0</td>\n",
       "      <td>30177.0</td>\n",
       "      <td>30412.0</td>\n",
       "      <td>30330.0</td>\n",
       "      <td>32092.0</td>\n",
       "      <td>30732.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>diag_3</td>\n",
       "      <td>30700.0</td>\n",
       "      <td>30799.0</td>\n",
       "      <td>30780.0</td>\n",
       "      <td>30828.0</td>\n",
       "      <td>31664.0</td>\n",
       "      <td>30954.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>diag_1</td>\n",
       "      <td>32147.0</td>\n",
       "      <td>31763.0</td>\n",
       "      <td>32236.0</td>\n",
       "      <td>32361.0</td>\n",
       "      <td>33420.0</td>\n",
       "      <td>32385.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     feature  importance_1  importance_2  \\\n",
       "0        number_inpatient_max_by_patient_nbr       10015.0        9602.0   \n",
       "1                        admission_source_id       10282.0       10191.0   \n",
       "2       number_inpatient_mean_by_patient_nbr       10711.0        8387.0   \n",
       "3       time_in_hospital_mean_by_patient_nbr       10764.0       10896.0   \n",
       "4        time_in_hospital_sum_by_patient_nbr       10828.0       10234.0   \n",
       "5      num_medications_median_by_patient_nbr       10859.0       10815.0   \n",
       "6                                    insulin       11258.0       10972.0   \n",
       "7                          admission_type_id       11397.0       10999.0   \n",
       "8        num_medications_mean_by_patient_nbr       12050.0       11707.0   \n",
       "9        number_diagnoses_sum_by_patient_nbr       12113.0       12008.0   \n",
       "10        num_medications_max_by_patient_nbr       12234.0       11623.0   \n",
       "11                          number_inpatient       13067.0       12133.0   \n",
       "12                          time_in_hospital       13160.0       13119.0   \n",
       "13  num_lab_procedures_median_by_patient_nbr       13660.0       13553.0   \n",
       "14        num_medications_sum_by_patient_nbr       14325.0       13920.0   \n",
       "15                                payer_code       14539.0       14353.0   \n",
       "16                           num_medications       15344.0       15033.0   \n",
       "17    num_lab_procedures_mean_by_patient_nbr       15422.0       14943.0   \n",
       "18     num_lab_procedures_max_by_patient_nbr       15673.0       16001.0   \n",
       "19     num_lab_procedures_sum_by_patient_nbr       17968.0       17845.0   \n",
       "20                  discharge_disposition_id       19304.0       19040.0   \n",
       "21                        num_lab_procedures       19421.0       19006.0   \n",
       "22                                    diag_2       30650.0       30177.0   \n",
       "23                                    diag_3       30700.0       30799.0   \n",
       "24                                    diag_1       32147.0       31763.0   \n",
       "\n",
       "    importance_3  importance_4  importance_5  avg_importance  \n",
       "0         9626.0        9908.0        9836.0          9797.4  \n",
       "1        10436.0       10657.0       10056.0         10324.4  \n",
       "2         8156.0       10625.0       10683.0          9712.4  \n",
       "3        10737.0       10908.0       10554.0         10771.8  \n",
       "4        10831.0       10597.0       10812.0         10660.4  \n",
       "5        10832.0       10785.0       10984.0         10855.0  \n",
       "6        11422.0       11505.0       11268.0         11285.0  \n",
       "7        11419.0       11315.0       11013.0         11228.6  \n",
       "8        11982.0       12315.0       11982.0         12007.2  \n",
       "9        12356.0       12365.0       12327.0         12233.8  \n",
       "10       11547.0       11724.0       12200.0         11865.6  \n",
       "11       12541.0       12768.0       12785.0         12658.8  \n",
       "12       13357.0       13331.0       13238.0         13241.0  \n",
       "13       13856.0       13542.0       14085.0         13739.2  \n",
       "14       14273.0       14417.0       14673.0         14321.6  \n",
       "15       14724.0       14403.0       14862.0         14576.2  \n",
       "16       15390.0       15226.0       15551.0         15308.8  \n",
       "17       15206.0       15064.0       15651.0         15257.2  \n",
       "18       15720.0       15855.0       15835.0         15816.8  \n",
       "19       18051.0       18399.0       18795.0         18211.6  \n",
       "20       19726.0       19994.0       19529.0         19518.6  \n",
       "21       19619.0       19650.0       20318.0         19602.8  \n",
       "22       30412.0       30330.0       32092.0         30732.2  \n",
       "23       30780.0       30828.0       31664.0         30954.2  \n",
       "24       32236.0       32361.0       33420.0         32385.4  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_imp['avg_importance'] = feat_imp.iloc[:, 1:].apply(lambda x: x.mean(), axis=1)\n",
    "feat_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feat_imp_plot = feat_imp.sort_values(by='avg_importance').plot(kind='barh', x='feature', y='avg_importance', figsize=(15, 12), title='Average feature importance across five folds')\n",
    "feat_imp_plot.figure.savefig('../outputs/plots/feature_importance.pdf', bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions on Test Data\n",
    "\n",
    "Take the average of each model's predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20354, 49), (20354,))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X, test_y = pd.read_parquet(data_path + 'test_X.parquet'), pd.read_parquet(data_path + 'test_y.parquet').to_numpy().reshape(-1,)\n",
    "test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y = joblib.load('../outputs/pipeline/label_encoder.joblib').fit_transform(test_y)\n",
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.zeros(shape=(test_X.shape[0], 3))\n",
    "for i in range(5):\n",
    "    # Initialize the model\n",
    "    model = xgb.Booster()\n",
    "    # Load the model\n",
    "    model.load_model(model_path + f'model_fold_{i + 1}.xgb')\n",
    "    # Process test set\n",
    "    fold_test_X = joblib.load(f'../outputs/pipeline/preprocessor_fold_{i + 1}.joblib').transform(test_X)\n",
    "    fold_test_X = joblib.load(f'../outputs/pipeline/feature_selector_fold_{i + 1}.joblib').transform(fold_test_X)\n",
    "    # Make predictions on the test set and sum the matrices of probabilities with shape (n_samples, n_classes) element-wise\n",
    "    pred += model.predict(xgb.DMatrix(data=fold_test_X))\n",
    "# Average the predictions\n",
    "pred /= 5\n",
    "pred = pred.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f6740a46bb0>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_true=test_y, y_pred=np.argmax(pred, axis=1), labels=[0, 1, 2], normalize='true')).plot();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8513914379491603"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_true=test_y, y_pred=pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_compute",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "330f1c154db79336c59a6eb23bf2343cc0a94603c32539a12551042b9df9fca8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
