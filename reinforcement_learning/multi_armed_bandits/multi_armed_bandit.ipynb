{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "1. $k$ — number of actions (arms)  \n",
    "2. $t$ — discrete time step (or play number)  \n",
    "3. $q_*(a)$ — true (expected) reward of action $a$  \n",
    "4. $Q_t(a)$ — estimate at time $t$ of $q_*(a)$  \n",
    "5. $N_t(a)$ — number of times action $a$ has been selected up to (but not including) time $t$  \n",
    "6. $H_t(a)$ — learned preference for selecting action $a$ at time $t$  \n",
    "7. $\\pi_t(a)$ — probability of selecting action $a$ at time $t$  \n",
    "8. $\\overline{R}_t$ — estimate at time $t$ of the expected reward under $\\pi_t$  \n",
    "9. $R_i$ — reward received on the $i$th selection of a particular action  \n",
    "10. $Q_n$ — value estimate of an action after it has been chosen $n-1$ times (i.e., the average of its first $n-1$ rewards)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. The $k$-Armed Bandit Problem\n",
    "\n",
    "- We have $k$ possible actions (arms).  \n",
    "- On each time step $t$:\n",
    "  - We choose an action $A_t$.  \n",
    "  - We receive a numerical reward $R_t$.  \n",
    "- Each action $a$ has a true but unknown expected reward $q_*(a)$.  \n",
    "- Our **goal**: choose actions over many trials to **maximize** total reward. This requires balancing:\n",
    "  - **Exploitation**: picking the action that currently appears best.\n",
    "  - **Exploration**: trying out other actions to refine estimates.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Action-Value Methods\n",
    "\n",
    "We maintain a running **action-value estimate** $Q_t(a)$ for each action $a$. One simple approach is the **sample-average** method:\n",
    "\n",
    "$$\n",
    "Q_t(a)=\\frac{\\sum_{i=1}^{t-1} R_i \\,\\mathbf{1}_{\\{A_i = a\\}}}\n",
    "     {\\sum_{i=1}^{t-1} \\mathbf{1}_{\\{A_i = a\\}}} \\quad (\\text{if never chosen, use a default like }0)\n",
    "$$\n",
    "\n",
    "As the number of samples grows large, by the law of large numbers, $Q_t(a)$ converges to $q_*(a)$.\n",
    "\n",
    "### 2.1 Greedy and $\\varepsilon$-Greedy Action Selection\n",
    "\n",
    "1. **Greedy**: always pick\n",
    "\n",
    "   $$\n",
    "   A_t=\\arg\\max_a \\, Q_t(a)\n",
    "   $$\n",
    "   \n",
    "   This **exploits** our current knowledge but does not explore less-tried actions.\n",
    "\n",
    "2. **$\\varepsilon$-Greedy**:  \n",
    "   - With probability $1-\\varepsilon$, select the greedy action (max $Q_t(a)$).  \n",
    "   - With probability $\\varepsilon$, select an action **at random**.  \n",
    "   - Ensures **infinite exploration** of each action in the limit.  \n",
    "   - Probability of picking the true optimal arm $\\to 1-\\varepsilon$ as $t\\to\\infty$.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Incremental Implementation of Sample Averages\n",
    "\n",
    "Rather than storing **all** past rewards, we can update the average **incrementally** in constant time. Let:\n",
    "\n",
    "- $Q_n$ be the average of the first $n-1$ rewards of one particular action.\n",
    "- $R_n$ be the **new** (i.e., $n$-th) observed reward for that action.\n",
    "- $Q_{n+1}$ be the updated average (now covering $n$ rewards).\n",
    "\n",
    "### 3.1 Derivation\n",
    "\n",
    "1. By definition of $Q_{n+1}$:\n",
    "   $$\n",
    "   \\begin{align*}\n",
    "   Q_{n+1}&=\\frac{1}{n}\\sum_{i=1}^{n} R_i \\\\\n",
    "          &=\\frac{1}{n}(R_n + \\sum_{i=1}^{n-1}R_i)\n",
    "   \\end{align*}\n",
    "   $$\n",
    "\n",
    "2. Note that:\n",
    "   $$\n",
    "   \\begin{align*}\n",
    "   \\sum_{i=1}^{n-1} R_i&=(n-1)\\,\\frac{1}{n-1}\\sum_{i=1}^{n-1} R_i \\\\\n",
    "                       &=(n-1)\\,Q_n\n",
    "   \\end{align*}\n",
    "   $$\n",
    "\n",
    "3. Substitute back:\n",
    "   $$\n",
    "   Q_{n+1}=\\frac{1}{n}(R_n + (n-1)Q_n)\n",
    "   $$\n",
    "\n",
    "4. Distribute $\\frac{1}{n}$:\n",
    "   $$\n",
    "   \\begin{align*}\n",
    "   Q_{n+1}&=\\frac{1}{n}(R_n + nQ_n - Q_n) \\\\\n",
    "         &=\\frac{R_n}{n} + \\frac{\\cancel{n}Q_n}{\\cancel{n}} - \\frac{Q_n}{n} \\\\\n",
    "         &=Q_n + \\frac{R_n - Q_n}{n} \\\\\n",
    "         &=Q_n + \\frac{1}{n}(R_n - Q_n)\n",
    "   \\end{align*}\n",
    "   $$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "Q_{n+1}=Q_n + \\frac{1}{n}(R_n - Q_n)\n",
    "$$\n",
    "\n",
    "This shows that **only the old estimate, the new reward, and the count $n$ are needed** to update the average in $O(1)$ time.\n",
    "\n",
    "<center>\n",
    "    <img src=\"diagrams/simple_bandit_algorithm.png\" width=\"70%\">\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
