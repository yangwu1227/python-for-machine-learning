{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta-Bernoulli Model\n",
    "\n",
    "### 1. Model Definition\n",
    "\n",
    "#### **Notation**\n",
    "\n",
    "- $N$: Total number of Bernoulli trials\n",
    "- $y_i$: Outcome of the $i$-th trial, where $y_i \\in \\{0,1\\}$\n",
    "- $k = \\sum_{i=1}^N y_i$: Total number of successes observed\n",
    "- $\\theta \\in [0,1]$: Probability of success on each trial\n",
    "\n",
    "#### **Definition**\n",
    "\n",
    "We observe $N$ Bernoulli trials, resulting in $k$ successes. We assume each trial’s probability of success is $\\theta$. Our goal is to infer $\\theta$.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Prior\n",
    "\n",
    "#### **Notation**\n",
    "\n",
    "- $\\alpha, \\beta > 0$: Shape parameters of the Beta prior distribution.\n",
    "\n",
    "#### **Beta Prior**\n",
    "\n",
    "We place a Beta prior on $\\theta$:\n",
    "$$\n",
    "  p(\\theta) \n",
    "  \\;=\\; \\mathrm{Beta}(\\theta \\mid \\alpha, \\beta) \n",
    "  \\;=\\; \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\,\\Gamma(\\beta)}\n",
    "        \\,\\theta^{\\alpha - 1} \\,(1-\\theta)^{\\beta - 1}\n",
    "$$\n",
    "\n",
    "This choice of prior is conjugate to the Bernoulli likelihood, ensuring the posterior also follows a Beta distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Likelihood\n",
    "\n",
    "#### **Notation**\n",
    "\n",
    "- $p(D \\mid \\theta)$: Probability of data $D$ (i.e., the observed outcomes) given $\\theta$.\n",
    "- $k$: Number of successes in $N$ trials.\n",
    "\n",
    "#### **Bernoulli Likelihood**\n",
    "\n",
    "For $k$ successes and $N-k$ failures, the Bernoulli likelihood is\n",
    "$$\n",
    "  p(D \\mid \\theta) \n",
    "  \\;=\\; \\theta^{k}\\,(1-\\theta)^{(N - k)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Posterior\n",
    "\n",
    "#### **Notation**\n",
    "\n",
    "- $p(\\theta \\mid D)$: Posterior distribution of $\\theta$ after observing data $D$.\n",
    "- $p(D)$: Marginal likelihood (normalizing constant).\n",
    "- $\\propto$: “Proportional to,” omitting constant factors that do not depend on $\\theta$.\n",
    "\n",
    "#### **Bayes’ Theorem and Posterior Form**\n",
    "\n",
    "By Bayes’ Theorem,\n",
    "\n",
    "$$\n",
    "  p(\\theta \\mid D) \n",
    "  \\;=\\; \\frac{p(D \\mid \\theta)\\,p(\\theta)}{p(D)}\n",
    "$$\n",
    "Substitute the Bernoulli likelihood and the Beta prior:\n",
    "$$\n",
    "  p(\\theta \\mid D) \n",
    "  \\;\\propto\\; \\theta^{k}\\,(1-\\theta)^{(N - k)} \n",
    "               \\;\\times\\; \\theta^{\\alpha - 1}\\,(1-\\theta)^{\\beta - 1}\n",
    "$$\n",
    "Combine the exponents of $\\theta$ and $(1-\\theta)$:\n",
    "$$\n",
    "  p(\\theta \\mid D) \n",
    "  \\;\\propto\\; \\theta^{(\\alpha - 1) + k}\n",
    "              \\,(1-\\theta)^{(\\beta - 1) + (N - k)}\n",
    "$$\n",
    "Recognizing the Beta kernel, we conclude:\n",
    "$$\n",
    "  p(\\theta \\mid D) \n",
    "  = \\mathrm{Beta}\\Bigl(\\theta \\;\\bigm|\\; \\alpha + k,\\;\\beta + (N - k)\\Bigr)\n",
    "$$\n",
    "\n",
    "Hence, the posterior is another Beta distribution with **updated** shape parameters $\\alpha + k$ and $\\beta + (N - k)$.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Sympy Verification\n",
    "\n",
    "Below, we symbolically represent the **unnormalized** posterior and numerically confirm that the integral over $[0,1]$ matches the Beta function normalization.\n",
    "\n",
    "#### **Notation**\n",
    "\n",
    "- $\\alpha, \\beta > 0$: Beta prior parameters.\n",
    "- $N$: Number of trials.\n",
    "- $k$: Number of observed successes.\n",
    "- $\\theta$: Success probability.\n",
    "\n",
    "#### **Unnormalized Posterior**\n",
    "\n",
    "$$\n",
    "  \\text{posterior}_\\text{unnorm}(\\theta) \n",
    "  \\;=\\; \\bigl[\\theta^{\\alpha - 1}\\,(1 - \\theta)^{\\beta - 1}\\bigr]\n",
    "         \\;\\times\\;\n",
    "         \\bigl[\\theta^{k}\\,(1 - \\theta)^{(N - k)}\\bigr].\n",
    "$$\n",
    "\n",
    "When integrated over $\\theta\\in[0,1]$, it should yield the normalizing constant that turns this expression into a valid Beta density with parameters $\\alpha + k$ and $\\beta + (N-k)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnormalized integral = 0.00198412698412698\n"
     ]
    }
   ],
   "source": [
    "import mpmath\n",
    "import sympy\n",
    "\n",
    "theta = sympy.Symbol(\"theta\", positive=True)\n",
    "alpha_v, beta_v = 2, 3  # Values for alpha and beta\n",
    "N_v, k_v = 5, 2  # Observed 5 trials, 2 successes\n",
    "\n",
    "# Unnormalized posterior expression\n",
    "expr_symbolic = (\n",
    "    theta ** (alpha_v - 1)\n",
    "    * (1 - theta) ** (beta_v - 1)\n",
    "    * theta ** (k_v)\n",
    "    * (1 - theta) ** (N_v - k_v)\n",
    ")\n",
    "\n",
    "f = sympy.lambdify(theta, expr_symbolic, \"mpmath\")\n",
    "\n",
    "# Numeric integration from 0 to 1\n",
    "res = mpmath.quad(f, [0, 1])\n",
    "print(\"Unnormalized integral =\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Result**  \n",
    "\n",
    "From the Beta conjugacy, the posterior’s normalizing constant for $\\alpha_v=2, \\beta_v=3, N_v=5, k_v=2$ is:\n",
    "\n",
    "$$\n",
    "  \\mathrm{B}(\\alpha_v + k_v,\\, \\beta_v + (N_v - k_v)) \n",
    "  \\;=\\; \\mathrm{B}(4,6)\n",
    "$$\n",
    "reduces to $1/504$. For concreteness, we use the fact that:\n",
    "\n",
    "$$\n",
    "  \\mathrm{B}(x,y) \\;=\\; \\frac{\\Gamma(x)\\,\\Gamma(y)}{\\Gamma(x+y)},\n",
    "  \\quad \\text{and} \n",
    "  \\quad \\Gamma(n+1)=n! \\text{ for positive integers } n.\n",
    "$$\n",
    "\n",
    "Substitute $\\mathrm{B}(4,6)$:\n",
    "\n",
    "$$\n",
    "\\mathrm{B}(4,6) \n",
    "\\;=\\; \\frac{\\Gamma(4)\\,\\Gamma(6)}{\\Gamma(4+6)}\n",
    "\\;=\\; \\frac{\\Gamma(4)\\,\\Gamma(6)}{\\Gamma(10)}.\n",
    "$$\n",
    "\n",
    "Convert gamma functions to factorials:\n",
    "\n",
    "- $\\Gamma(4) = 3! = 3 \\times 2 \\times 1 = 6.$  \n",
    "- $\\Gamma(6) = 5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 120.$  \n",
    "- $\\Gamma(10) = 9! = 9 \\times 8 \\times 7 \\times 6 \\times 5 \\times 4 \\times 3 \\times 2 \\times 1 = 362{,}880.$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "  \\frac{\\Gamma(4)\\,\\Gamma(6)}{\\Gamma(10)} \n",
    "  \\;=\\; \\frac{3!\\,5!}{9!} \n",
    "  \\;=\\; \\frac{6 \\times 120}{362{,}880}.\n",
    "$$\n",
    "\n",
    "Simplify numerically:\n",
    "\n",
    "$$\n",
    "  \\frac{6 \\times 120}{362{,}880} \n",
    "  \\;=\\; \\frac{720}{362{,}880}.\n",
    "$$\n",
    "\n",
    "Recognizing $362{,}880 / 720 = 504$, we have\n",
    "\n",
    "$$\n",
    "  \\frac{720}{362{,}880} \n",
    "  \\;=\\; \\frac{1}{504}.\n",
    "$$\n",
    "\n",
    "Hence, when we numerically integrate the **unnormalized** posterior over $\\theta \\in [0,1]$ and get $0.001984...$, we match the exact fraction $\\tfrac{1}{504}$. This confirms the **Beta conjugacy** result:\n",
    "\n",
    "$$\n",
    "  \\int_0^1 \\text{posterior}_\\text{unnorm}(\\theta)\\,d\\theta \n",
    "  \\;=\\; \\frac{1}{504}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of Different Priors on the Posterior in a Beta-Bernoulli Model\n",
    "\n",
    "### Model Setup\n",
    "\n",
    "- **Prior**: Beta distribution with various parameterizations.\n",
    "- **Likelihood**: Bernoulli trials with observed successes and failures.\n",
    "- **Posterior**: Updated Beta distribution via conjugacy.\n",
    "\n",
    "### 🔢 Priors Explored\n",
    "\n",
    "<center>\n",
    "\n",
    "| Prior        | Description                              |\n",
    "|--------------|------------------------------------------|\n",
    "| Beta(1,1)    | Uniform, non-informative                 |\n",
    "| Beta(2,2)    | Weakly informative, centered at 0.5      |\n",
    "| Beta(0.5,0.5)| Informative, favoring extremes (0 and 1) |\n",
    "| Beta(5,1)    | Informative, favoring high success prob. |\n",
    "| Beta(1,5)    | Informative, favoring low success prob.  |\n",
    "| Beta(10,10)  | Strongly centered around 0.5             |\n",
    "\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "<center>\n",
    "<img src=\"diagrams/prior_posterior_plot.png\" alt=\"Prior and Posterior Distributions\" width=\"90%\" />\n",
    "</center>\n",
    "\n",
    "1. **Uniform Prior (Beta(1,1))**:  \n",
    "   - This prior represents no prior knowledge and allows the posterior to be driven mainly by the data.\n",
    "   - As data accumulates, the posterior becomes more concentrated around the observed proportion of successes.\n",
    "\n",
    "2. **Weakly Informative Prior (Beta(2,2))**:  \n",
    "   - Slightly informative but still relatively neutral, this prior pulls the posterior slightly toward 0.5 when data is sparse.\n",
    "   - With more data, the influence of the prior diminishes.\n",
    "\n",
    "3. **Informative Prior Favoring Extremes (Beta(0.5,0.5))**:  \n",
    "   - This prior assumes high variance, favoring probabilities near 0 or 1.\n",
    "   - When data is limited, the posterior also exhibits a preference for extremes.\n",
    "\n",
    "4. **Informative Prior Favoring High Probabilities (Beta(5,1))**:  \n",
    "   - This prior assumes a strong prior belief that success probability is high.\n",
    "   - When data contradicts this belief, the posterior updates but remains somewhat biased toward higher values, especially with small datasets.\n",
    "\n",
    "5. **Informative Prior Favoring Low Probabilities (Beta(1,5))**:  \n",
    "   - This prior assumes the success probability is low.\n",
    "   - The posterior updates based on observed data but remains skewed toward lower probabilities when data is scarce.\n",
    "\n",
    "### General Takeaways\n",
    "\n",
    "- **More Data Reduces Prior Influence**: As more trials are observed, the posterior relies more on data and less on the prior.\n",
    "- **Stronger Priors Require More Data to Overcome**: Highly informative priors can significantly shape the posterior when the sample size is small.\n",
    "- **Prior Selection Matters in Low-Data Regimes**: When data is limited, the choice of prior can heavily influence conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
