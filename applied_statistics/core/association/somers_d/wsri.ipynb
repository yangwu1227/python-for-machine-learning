{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac902345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "import tracemalloc\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "from wsri import somers_d_roc_auc, somers_d_two_pointers, wsri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1997d2",
   "metadata": {},
   "source": [
    "## Benchmark vs. Monitoring Setup\n",
    "\n",
    "* **Benchmark**: A fixed reference period (e.g., 2023 data) that we treat as the \"gold standard\" or baseline distribution of scores and outcomes.\n",
    "* **Monitoring**: A newer period (e.g., Q1 2024 data) that we want to evaluate for stability relative to the benchmark.\n",
    "\n",
    "The purpose of **wSRI** here is to ask: *if the score-label relationship in monitoring looks different from benchmark, how much of that difference reflects population shift rather than true deterioration in model rank-ordering?*\n",
    "\n",
    "---\n",
    "\n",
    "### Why Weight the Benchmark\n",
    "\n",
    "Weighting only the **benchmark** population makes sense because:\n",
    "\n",
    "* **Benchmark as reference**: We want monitoring data to be evaluated *as is*, because it reflects the actual, current population distribution.\n",
    "* **Benchmark rescaled**: By reweighting benchmark bins to match the marginal distribution of scores in monitoring, we are forcing the benchmark to look like e.g. \"what 2023 would have looked like under the 2024 score distribution.\"\n",
    "* **Apples-to-apples comparability**: Without reweighting, Somers’ D differences could be confounded by the fact that the monitoring period simply has more/less accounts in certain score ranges (not necessarily because the rank-order power of the model changed).\n",
    "\n",
    "So the weighting isolates the effect of **conditional rank performance** (score <-> label relationship) rather than **population score mix**.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Not Weight Monitoring\n",
    "\n",
    "If we weighted **monitoring** to benchmark:\n",
    "\n",
    "* We’d be discarding real, current information about the new population distribution.\n",
    "* The monitoring Somers’ D would be artificially “distorted back” to the 2023 distribution, making it less diagnostic for drift.\n",
    "* We’d risk underestimating deterioration if population shift itself is part of what we want to detect.\n",
    "\n",
    "---\n",
    "\n",
    "### Covariate Shift\n",
    "\n",
    "Formally, weighting only benchmark approximates a **covariate-shift correction** problem:\n",
    "\n",
    "* Assume conditional outcome distributions *P(y|score)* are stable in both periods (or at least what we want to test).\n",
    "* But the marginal score distribution *P(score)* has shifted.\n",
    "* Reweighting benchmark to monitoring ensures we compare *P(y|score)* fairly across periods.\n",
    "\n",
    "This is consistent with the idea of “importance weighting” in domain adaptation:\n",
    "\n",
    "$$\n",
    "w(x) = \\frac{P_{\\text{monitoring}}(x)}{P_{\\text{benchmark}}(x)}\n",
    "$$\n",
    "\n",
    "where here *x* = score bins.\n",
    "\n",
    "It makes sense to weight the **benchmark** and not the monitoring because we want to transform the reference into the monitoring population’s shape, ensuring comparability. Monitoring stays unweighted so it reflects reality. This lets the ratio (monitoring Somers’ D / reweighted benchmark Somers’ D) isolate *rank-order stability*, not population differences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f470a7",
   "metadata": {},
   "source": [
    "## Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5be3d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng: np.random.Generator = np.random.default_rng(seed=1227)\n",
    "\n",
    "sample_sizes: np.typing.NDArray[np.int64] = np.array(\n",
    "    [5_000, 50_000, 500_000, 5_000_000, 50_000_000]\n",
    ")\n",
    "\n",
    "score_column: str = \"score\"\n",
    "label_column: str = \"label\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a2a45f",
   "metadata": {},
   "source": [
    "## JIT Warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "278107ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WSRI: 4.687046070922117\n"
     ]
    }
   ],
   "source": [
    "# Warm-up for JIT since the first call to a numba JIT function includes a one-time compilation overhead\n",
    "warmup_sample_size: int = 1_000\n",
    "benchmark: pl.DataFrame = pl.DataFrame(\n",
    "    {\n",
    "        score_column: rng.uniform(0, 1, warmup_sample_size).astype(np.float64),\n",
    "        label_column: rng.choice([0, 1], warmup_sample_size).astype(np.int8),\n",
    "    }\n",
    ").lazy()\n",
    "\n",
    "monitoring: pl.DataFrame = pl.DataFrame(\n",
    "    {\n",
    "        score_column: rng.uniform(0, 1, warmup_sample_size).astype(np.float64),\n",
    "        label_column: rng.choice([0, 1], warmup_sample_size).astype(np.int8),\n",
    "    }\n",
    ").lazy()\n",
    "\n",
    "wsri_value: float = wsri(\n",
    "    benchmark=benchmark,\n",
    "    monitoring=monitoring,\n",
    "    score_column=score_column,\n",
    "    label_column=label_column,\n",
    "    quantiles=20,\n",
    "    callback=somers_d_two_pointers,\n",
    ")\n",
    "\n",
    "print(f\"WSRI: {wsri_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af584e1c",
   "metadata": {},
   "source": [
    "## Wall Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9791127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 5000\n",
      "Sample size: 50000\n",
      "Sample size: 500000\n",
      "Sample size: 5000000\n",
      "Sample size: 50000000\n"
     ]
    }
   ],
   "source": [
    "wall_time_results: List[Dict[str, float]] = []\n",
    "data_sets: List[Dict[str, pl.LazyFrame]] = []\n",
    "\n",
    "for sample_size in sample_sizes:\n",
    "    print(f\"Sample size: {sample_size}\")\n",
    "\n",
    "    benchmark: pl.LazyFrame = pl.DataFrame(\n",
    "        {\n",
    "            score_column: rng.uniform(0, 1, sample_size).astype(np.float64),\n",
    "            label_column: rng.choice([0, 1], sample_size).astype(np.int8),\n",
    "        }\n",
    "    ).lazy()\n",
    "\n",
    "    monitoring: pl.LazyFrame = pl.DataFrame(\n",
    "        {\n",
    "            score_column: rng.uniform(0, 1, sample_size).astype(np.float64),\n",
    "            label_column: rng.choice([0, 1], sample_size).astype(np.int8),\n",
    "        }\n",
    "    ).lazy()\n",
    "\n",
    "    data_sets.append({\"benchmark\": benchmark, \"monitoring\": monitoring})\n",
    "\n",
    "    start_time_two_pointers: float = time.perf_counter()\n",
    "    wsri_two_pointers: float = wsri(\n",
    "        benchmark=benchmark,\n",
    "        monitoring=monitoring,\n",
    "        score_column=score_column,\n",
    "        label_column=label_column,\n",
    "        quantiles=20,\n",
    "        callback=somers_d_two_pointers,\n",
    "    )\n",
    "    time_two_pointers: float = time.perf_counter() - start_time_two_pointers\n",
    "\n",
    "    start_time_roc_auc: float = time.perf_counter()\n",
    "    wsri_roc_auc: float = wsri(\n",
    "        benchmark=benchmark,\n",
    "        monitoring=monitoring,\n",
    "        score_column=score_column,\n",
    "        label_column=label_column,\n",
    "        quantiles=20,\n",
    "        callback=somers_d_roc_auc,\n",
    "    )\n",
    "    time_roc_auc: float = time.perf_counter() - start_time_roc_auc\n",
    "\n",
    "    wall_time_results.append(\n",
    "        {\n",
    "            \"sample_size\": sample_size,\n",
    "            \"wsri_two_pointers\": wsri_two_pointers,\n",
    "            \"wsri_roc_auc\": wsri_roc_auc,\n",
    "            \"time_two_pointers\": time_two_pointers,\n",
    "            \"time_roc_auc\": time_roc_auc,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25af32e3",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "794ce449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 5000\n",
      "Sample size: 50000\n",
      "Sample size: 500000\n",
      "Sample size: 5000000\n",
      "Sample size: 50000000\n"
     ]
    }
   ],
   "source": [
    "memory_results: List[Dict[str, float]] = []\n",
    "\n",
    "for sample_size, data_set in zip(sample_sizes, data_sets):\n",
    "    print(f\"Sample size: {sample_size}\")\n",
    "\n",
    "    benchmark: pl.LazyFrame = data_set[\"benchmark\"]\n",
    "    monitoring: pl.LazyFrame = data_set[\"monitoring\"]\n",
    "\n",
    "    tracemalloc.start()\n",
    "    _ = wsri(\n",
    "        benchmark=benchmark,\n",
    "        monitoring=monitoring,\n",
    "        score_column=score_column,\n",
    "        label_column=label_column,\n",
    "        quantiles=20,\n",
    "        callback=somers_d_two_pointers,\n",
    "    )\n",
    "    _, peak_mem_two_pointers = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    tracemalloc.start()\n",
    "    _ = wsri(\n",
    "        benchmark=benchmark,\n",
    "        monitoring=monitoring,\n",
    "        score_column=score_column,\n",
    "        label_column=label_column,\n",
    "        quantiles=20,\n",
    "        callback=somers_d_roc_auc,\n",
    "    )\n",
    "    _, peak_mem_roc_auc = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    memory_results.append(\n",
    "        {\n",
    "            \"sample_size\": sample_size,\n",
    "            \"peak_mem_two_pointers_mb\": peak_mem_two_pointers / 10**6,\n",
    "            \"peak_mem_roc_auc_mb\": peak_mem_roc_auc / 10**6,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7d1dc1",
   "metadata": {},
   "source": [
    "## Combine Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bad522f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sample_size</th><th>wsri_two_pointers</th><th>wsri_roc_auc</th><th>time_two_pointers</th><th>time_roc_auc</th><th>peak_mem_two_pointers_mb</th><th>peak_mem_roc_auc_mb</th><th>speedup_two_pointers_vs_roc_auc</th><th>memory_two_pointers_vs_roc_auc</th></tr><tr><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>5000</td><td>-0.3375</td><td>-0.3375</td><td>0.0068</td><td>0.0083</td><td>0.1305</td><td>0.502</td><td>1.2172</td><td>3.8457</td></tr><tr><td>50000</td><td>1.5518</td><td>1.5518</td><td>0.0219</td><td>0.0259</td><td>1.2534</td><td>4.5766</td><td>1.1857</td><td>3.6515</td></tr><tr><td>500000</td><td>-0.3074</td><td>-0.3074</td><td>0.2069</td><td>0.2441</td><td>12.5036</td><td>45.0766</td><td>1.18</td><td>3.6051</td></tr><tr><td>5000000</td><td>0.868</td><td>0.868</td><td>2.1829</td><td>2.7142</td><td>125.0035</td><td>450.0768</td><td>1.2434</td><td>3.6005</td></tr><tr><td>50000000</td><td>-0.529</td><td>-0.529</td><td>26.1408</td><td>31.7893</td><td>1250.0041</td><td>4500.0769</td><td>1.2161</td><td>3.6</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 9)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ sample_si ┆ wsri_two_ ┆ wsri_roc_ ┆ time_two_ ┆ … ┆ peak_mem_ ┆ peak_mem_ ┆ speedup_t ┆ memory_t │\n",
       "│ ze        ┆ pointers  ┆ auc       ┆ pointers  ┆   ┆ two_point ┆ roc_auc_m ┆ wo_pointe ┆ wo_point │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ers_mb    ┆ b         ┆ rs_vs_roc ┆ ers_vs_r │\n",
       "│ i64       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ ---       ┆ ---       ┆ _au…      ┆ oc_auc   │\n",
       "│           ┆           ┆           ┆           ┆   ┆ f64       ┆ f64       ┆ ---       ┆ ---      │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ f64       ┆ f64      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 5000      ┆ -0.3375   ┆ -0.3375   ┆ 0.0068    ┆ … ┆ 0.1305    ┆ 0.502     ┆ 1.2172    ┆ 3.8457   │\n",
       "│ 50000     ┆ 1.5518    ┆ 1.5518    ┆ 0.0219    ┆ … ┆ 1.2534    ┆ 4.5766    ┆ 1.1857    ┆ 3.6515   │\n",
       "│ 500000    ┆ -0.3074   ┆ -0.3074   ┆ 0.2069    ┆ … ┆ 12.5036   ┆ 45.0766   ┆ 1.18      ┆ 3.6051   │\n",
       "│ 5000000   ┆ 0.868     ┆ 0.868     ┆ 2.1829    ┆ … ┆ 125.0035  ┆ 450.0768  ┆ 1.2434    ┆ 3.6005   │\n",
       "│ 50000000  ┆ -0.529    ┆ -0.529    ┆ 26.1408   ┆ … ┆ 1250.0041 ┆ 4500.0769 ┆ 1.2161    ┆ 3.6      │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wall_time_data: pl.DataFrame = pl.from_dicts(wall_time_results)\n",
    "memory_data: pl.DataFrame = pl.from_dicts(memory_results)\n",
    "\n",
    "combined_results: pl.DataFrame = wall_time_data.join(\n",
    "    other=memory_data, on=\"sample_size\", how=\"inner\"\n",
    ").with_columns(\n",
    "    (pl.col(\"time_roc_auc\") / pl.col(\"time_two_pointers\")).alias(\n",
    "        \"speedup_two_pointers_vs_roc_auc\"\n",
    "    ),\n",
    "    (pl.col(\"peak_mem_roc_auc_mb\") / pl.col(\"peak_mem_two_pointers_mb\")).alias(\n",
    "        \"memory_two_pointers_vs_roc_auc\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "combined_results.with_columns(cs.float().round(4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-for-machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
