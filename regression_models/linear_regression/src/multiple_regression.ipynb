{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6394f0d0",
   "metadata": {},
   "source": [
    "# Multiple Variable Linear Regression\n",
    "\n",
    "# Outline\n",
    "- [&nbsp;&nbsp;1.1 Goals](#toc_15456_1.1)\n",
    "- [&nbsp;&nbsp;1.2 Tools](#toc_15456_1.2)\n",
    "- [&nbsp;&nbsp;1.3 Notation](#toc_15456_1.3)\n",
    "- [2 Problem Statement](#toc_15456_2)\n",
    "- [&nbsp;&nbsp;2.1 Matrix X containing our examples](#toc_15456_2.1)\n",
    "- [&nbsp;&nbsp;2.2 Parameter vector w, b](#toc_15456_2.2)\n",
    "- [3 Model Prediction With Multiple Variables](#toc_15456_3)\n",
    "- [&nbsp;&nbsp;3.1 Single Prediction element by element](#toc_15456_3.1)\n",
    "- [&nbsp;&nbsp;3.2 Single Prediction, vector](#toc_15456_3.2)\n",
    "- [4 Compute Cost With Multiple Variables](#toc_15456_4)\n",
    "- [5 Gradient Descent With Multiple Variables](#toc_15456_5)\n",
    "- [&nbsp;&nbsp;5.1 Compute Gradient with Multiple Variables](#toc_15456_5.1)\n",
    "- [&nbsp;&nbsp;5.2 Gradient Descent With Multiple Variables](#toc_15456_5.2)\n",
    "- [6 Congratulations](#toc_15456_6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf103c1",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_1.1\"></a>\n",
    "## 1.1 Goals\n",
    "- Extend our regression model  routines to support multiple features\n",
    "    - Extend data structures to support multiple features\n",
    "    - Utilize NumPy `np.dot` to vectorize their implementations for speed and simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbffe648",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_1.2\"></a>\n",
    "## 1.2 Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "210524c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "np.set_printoptions(precision=2)  # reduced display precision on numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c33b85",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_1.3\"></a>\n",
    "## 1.3 Notation\n",
    "\n",
    "|General <img width=70/> <br />  Notation  <img width=70/> | Description<img width=350/>| Python (if applicable) |\n",
    "|: ------------|: ------------------------------------------------------------||\n",
    "| $a$ | scalar, non bold                                                      ||\n",
    "| $\\mathbf{a}$ | vector, bold                                                 ||\n",
    "| $\\mathbf{A}$ | matrix, bold capital                                         ||\n",
    "| **Regression** |         |    |     |\n",
    "|  $\\mathbf{X}$ | training example matrix                  | `X_train` |   \n",
    "|  $\\mathbf{y}$  | training example  targets                | `y_train` \n",
    "|  $\\mathbf{x}^{(i)}$, $y^{(i)}$ | $i_{th}$Training Example | `X[i]`, `y[i]`|\n",
    "| m | number of training examples | `m`|\n",
    "| n | number of features in each example | `n`|\n",
    "|  $\\mathbf{w}$  |  parameter: weight,                       | `w`    |\n",
    "|  $b$           |  parameter: bias                                           | `b`    |     \n",
    "| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | The result of the model evaluation at $\\mathbf{x^{(i)}}$ parameterized by $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$  | `f_wb` | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a5010d",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_2\"></a>\n",
    "# 2 Problem Statement\n",
    "\n",
    "The training dataset contains three examples (rows) with four features (size, bedrooms, floors and, age) shown in the table below.  Note that, unlike the earlier labs, size is in sqft rather than 1000 sqft, so we need to use feature scaling.\n",
    "\n",
    "| Size (sqft) | Number of Bedrooms  | Number of floors | Age of  Home | Price (1000s dollars)  |   \n",
    "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
    "| 2104            | 5                   | 1                | 45           | 460           |  \n",
    "| 1416            | 3                   | 2                | 40           | 232           |  \n",
    "| 852             | 2                   | 1                | 35           | 178           |  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "861848e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e5fdc8",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_2.1\"></a>\n",
    "## 2.1 Matrix X containing our examples\n",
    "Similar to the table above, examples are stored in a NumPy matrix `X_train`. Each row of the matrix represents one example. When we have $m$ training examples ( $m$ is three in our example), and there are $n$ features (four in our example), $\\mathbf{X}$ is a matrix with dimensions ($m$, $n$) (m rows, n columns).\n",
    "\n",
    "\n",
    "$$\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    " x^{(0)}_0 & x^{(0)}_1 & \\cdots & x^{(0)}_{n-1} \\\\ \n",
    " x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_{n-1} \\\\\n",
    " \\cdots \\\\\n",
    " x^{(m-1)}_0 & x^{(m-1)}_1 & \\cdots & x^{(m-1)}_{n-1} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "notation:\n",
    "- $\\mathbf{x}^{(i)}$ is vector containing example i. $\\mathbf{x}^{(i)}$ $ = (x^{(i)}_0, x^{(i)}_1, \\cdots,x^{(i)}_{n-1})$\n",
    "- $x^{(i)}_j$ is element j in example i. The superscript in parenthesis indicates the example number while the subscript represents an element.  \n",
    "\n",
    "Display the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d0abf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: (3, 4), X Type:<class 'numpy.ndarray'>)\n",
      "[[2104    5    1   45]\n",
      " [1416    3    2   40]\n",
      " [ 852    2    1   35]]\n",
      "y Shape: (3,), y Type:<class 'numpy.ndarray'>)\n",
      "[460 232 178]\n"
     ]
    }
   ],
   "source": [
    "# data is stored in numpy array/matrix\n",
    "print(f\"X Shape: {X_train.shape}, X Type:{type(X_train)})\")\n",
    "print(X_train)\n",
    "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d2b435",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_2.2\"></a>\n",
    "## 2.2 Parameter vector w, b\n",
    "\n",
    "* $\\mathbf{w}$ is a vector with $n$ elements.\n",
    "  - Each element contains the parameter associated with one feature.\n",
    "  - in our dataset, n is 4.\n",
    "  - notionally, we draw this as a column vector\n",
    "\n",
    "$$\\mathbf{w} = \\begin{pmatrix}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\\n",
    "\\cdots\\\\\n",
    "w_{n-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "* $b$ is a scalar parameter, which is the intercept. When an intercept term is included in the model, the residuals must sum to zero. This is because the error vector $\\vec{\\sigma}$ is perpendicular to the vector of 1's (phat $\\vec{1}$). The dot product between these two vectors--- $\\vec{\\sigma}$ and $\\vec{1}$ is zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852537a2",
   "metadata": {},
   "source": [
    "For demonstration, $\\mathbf{w}$ and $b$ will be loaded with some initial selected values that are near the optimal. $\\mathbf{w}$ is a 1-D NumPy vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f21acf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_init shape: (4,), b_init type: <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "print(f\"w_init shape: {w_init.shape}, b_init type: {type(b_init)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17d082a",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_3\"></a>\n",
    "# 3 Model Prediction With Multiple Variables\n",
    "The model's prediction with multiple variables is given by the linear model:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "or in vector notation:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \n",
    "where $\\cdot$ is a vector `dot product`\n",
    "\n",
    "To demonstrate the dot product, we will implement prediction using (1) and (2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffdf07d",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_3.1\"></a>\n",
    "## 3.1 Single Prediction element by element\n",
    "Our previous prediction multiplied one feature value by one parameter and added a bias parameter. A direct extension of our previous implementation of prediction to multiple features would be to implement (1) above using loop over each element, performing the multiply with its parameter and then adding the bias parameter at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "036e8b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_loop(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters    \n",
    "      b (scalar):  model parameter     \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    n = x.shape[0] # Number of features in row\n",
    "    p = 0\n",
    "    for i in range(n):\n",
    "        p_i = x[i] * w[i]  \n",
    "        p = p + p_i         \n",
    "    p = p + b                \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "610fb681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec shape (4,), x_vec value: [2104    5    1   45]\n",
      "f_wb shape (), prediction: 459.9999976194083\n"
     ]
    }
   ],
   "source": [
    "# Get a row from our training data\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "# Make a prediction using the row \n",
    "f_wb = predict_single_loop(x_vec, w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b273d31c",
   "metadata": {},
   "source": [
    "Note the shape of `x_vec`. It is a 1-D NumPy vector with 4 elements, (4,). The result, `f_wb` is a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc4cf18",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_3.2\"></a>\n",
    "## 3.2 Single Prediction, vector\n",
    "\n",
    "Noting that equation (1) above can be implemented using the dot product as in (2) above. We can make use of vector operations to speed up predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "749030e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters   \n",
    "      b (scalar):             model parameter \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    p = np.dot(x, w) + b     \n",
    "    return p    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69b09c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec shape (4,), x_vec value: [2104    5    1   45]\n",
      "f_wb shape (), prediction: 459.9999976194083\n"
     ]
    }
   ],
   "source": [
    "# get a row from our training data\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "# make a prediction\n",
    "f_wb = predict(x_vec,w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45037a4",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_4\"></a>\n",
    "# 4 Compute Cost With Multiple Variables\n",
    "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ \n",
    "\n",
    "* $\\mathbf{x}^{(i)}$ is vector containing example i. $\\mathbf{x}^{(i)}$ $ = (x^{(i)}_0, x^{(i)}_1, \\cdots,x^{(i)}_{n-1})$ and $i \\in 0, 1, 2, ..., m - 1$\n",
    "\n",
    "In contrast to previous labs, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46d4c58",
   "metadata": {},
   "source": [
    "Below is an implementation of equations (3) and (4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4740b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b): \n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0] # Number of examples\n",
    "    cost = 0.0\n",
    "    for i in range(m):                                \n",
    "        f_wb_i = np.dot(X[i], w) + b           # The variable f_wb_i is a single y_hat for the ith training example, since (n,)(n,) = scalar\n",
    "        cost = cost + (f_wb_i - y[i])**2       # Scalar\n",
    "    cost = cost / (2 * m)                      # Scalar    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5b3fda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at optimal w : 1.5578904428966628e-12\n"
     ]
    }
   ],
   "source": [
    "# Compute and display cost using our pre-chosen optimal parameters. \n",
    "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
    "print(f'Cost at optimal w : {cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Result**: Cost at optimal w : 1.5578904045996674e-12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a28e12",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_5\"></a>\n",
    "# 5 Gradient Descent With Multiple Variables\n",
    "Gradient descent for multiple variables:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously. Here *simultaniously* means that we calculate the partial derivatives for all the parameters before updating any of the parameters.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "* m is the number of training examples in the data set\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be874643",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_5.1\"></a>\n",
    "## 5.1 Compute Gradient with Multiple Variables\n",
    "An implementation for calculating the equations (6) and (7) is below. In this version, there is an\n",
    "- outer loop over all m examples. \n",
    "    - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ for the example can be computed directly and accumulated\n",
    "    - in a second loop over all n features:\n",
    "        - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$ is computed for each $w_j$.\n",
    "  \n",
    "There will be a total of $m\\times n$ iterations.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a157566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape           # (number of examples, number of features)\n",
    "    dj_dw = np.zeros((n,))   # Partial derivative of cost w.r.t. w, a vector containing n elements\n",
    "    dj_db = 0.               # Partial derivative of cost w.r.t. b, a scalar\n",
    "\n",
    "    # For each training example, 0, 1, ..., m-1\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(X[i], w) + b) - y[i] # Compute the error (deviation y_hat - y) for the ith training example\n",
    "        for j in range(n):                         \n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j] # For each error computed above, use it to compute the partial derivative of the cost w.r.t. each feature w[j], where j = 0, 1, ..., n-1\n",
    "        dj_db = dj_db + err # For each error computed above, add it to the initialized dj_db, since the partial derivative dj_db is just the sum of errors for all training examples divided by m\n",
    "    \n",
    "    # For both partial derivatives, divide by m \n",
    "    dj_dw = dj_dw / m                           \n",
    "    dj_db = dj_db / m                                \n",
    "        \n",
    "    return dj_db, dj_dw # Return the partial derivatives (scalar, vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68adb3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db at initial w,b: -1.6739251501955248e-06\n",
      "dj_dw at initial w,b: \n",
      " [-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]\n"
     ]
    }
   ],
   "source": [
    "# Compute and display gradient \n",
    "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Result**:   \n",
    "dj_db at initial w,b: -1.6739251122999121e-06  \n",
    "dj_dw at initial w,b:   \n",
    " [-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_5.2\"></a>\n",
    "## 5.2 Gradient Descent With Multiple Variables\n",
    "The routine below implements equation (5) above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f7ea94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w and b. Updates w and b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "      \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  # Avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db,dj_dw = gradient_function(X, y, w, b)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient, simultaneously\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # Prevent resource exhaustion \n",
    "            J_history.append(cost_function(X, y, w, b))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \") # 4d formats an integer 'd' to a field of minimum width 4 with zero paddings on the left\n",
    "        \n",
    "    return w, b, J_history # Return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21005b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost  2529.46   \n",
      "Iteration  100: Cost   695.99   \n",
      "Iteration  200: Cost   694.92   \n",
      "Iteration  300: Cost   693.86   \n",
      "Iteration  400: Cost   692.81   \n",
      "Iteration  500: Cost   691.77   \n",
      "Iteration  600: Cost   690.73   \n",
      "Iteration  700: Cost   689.71   \n",
      "Iteration  800: Cost   688.70   \n",
      "Iteration  900: Cost   687.69   \n",
      "b,w found by gradient descent: -0.00,[ 0.2   0.   -0.01 -0.07] \n",
      "prediction: 426.19, target value: 460\n",
      "prediction: 286.17, target value: 232\n",
      "prediction: 171.47, target value: 178\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "# Some gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "# Run gradient descent \n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m,_ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Result**:    \n",
    "b,w found by gradient descent: -0.00,[ 0.2   0.   -0.01 -0.07]   \n",
    "prediction: 426.19, target value: 460  \n",
    "prediction: 286.17, target value: 232  \n",
    "prediction: 171.47, target value: 178  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f29243d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2gAAAEoCAYAAAAt0dJ4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABFOklEQVR4nO3deZhU1bX+8e/LrIKtMogDKo6IKDIoDohGRMVZUQEvSUzMYK6a+0s0icONMSY3Ro2JGmNMTK5eowFEHOOIokZFwRmZhThHxAFaGWVYvz/OaSiqeoTqqurq9/M856Fq71PnrDravXvV3meVIgIzMzMzMzMrvhbFDsDMzMzMzMwSTtDMzMzMzMxKhBM0MzMzMzOzEuEEzczMzMzMrEQ4QTMzMzMzMysRTtDMzMzMzMxKhBM0MzMzM7MNJKmdpKmSHpLUKs/H3klSSBqYPt9Z0keSfprP81hpcYJmZU1SR0lXSpopabmkzyS9LOkXkrrm6RxnSlqVj2Pli6TLJEXW87kFjuFWSY9X0x6SLitkLGZmjcXjzHrPm+s4cyWwGXBaRKyqLba6SDosjf+w6voj4l/A6cClkgZseMhWyvKa5ZuVEknbA88By4H/AV4j+VDiYOBc4FPg2iKF19iuBW4tcgw16Q4sKnYQZmYby+OMxxlJ3YHvAcMiYkkeDvkCSfzza9ohIp6WNIbk/7kj8nBOKzUR4c1bWW7AfcD7QMdq+loDO6ePWwI/B94DVgBvAKdn7X8syS/NL4CPgInAfsBhQGRtT9UQzzfT43fMav8p8A7QIn1+ATAbWAa8C9xZ3Xuo473/N/B2+vjMamK8NeM6XJ6efxkwFRiecZyq93cGMAH4HBgLdAUmAx+n7+l94Cagffq6y6o552Vp3yrgzIxzdAL+BnwGLAWeAPpUE8OZaV8l8K/MOL158+atGJvHGY8zJLNnc7Laaovt2vRaLE/jmQgckPHagen+O6XPd0qfD8w6R/+0fc9i/xx4y/9W9AC8eWuMDdgSWA1cXI99fwEsAIYDewMXpr/cB6f9ewArgYuAHiSfVv02/UXeLh3oVqW/RHcCutZwns3SX/r/ldU+F7g0fXxWus8wYE/gRGB8Q38Bs/7A2T4dEN7OiLFT2ncryR8KRwM9ST7xXQUMSvurBq1ZaSy7ArsB25AMuIOA3YET0mv4h/R1WwB3Ac9mnHOLtC974Pwn8GJ6rn7A7emg1SkrhjeAk4G9gBtIBvptiv3/mjdv3prn5nHG40z62jdIk6+MttpiuwA4Jb0W+wH3k8y0tk3765WgpX1zgB8W+2fBW/63ogfgzVtjbMD+6S+0oXXst0n6C/isrPY7SD+hBE5Nj7VtDcc4E1hVz7huAl7PeD4oHUi2S5/fAMwAWm7k+187cKbPLwPmZu3THVhD1qAMPAjcnD6uGrR2rMc5fwe8lvH8VuDxavZbO3ACX0mPv0tGfyuSQf6yrBi6ZezTJW07rtj/r3nz5q15bh5nPM6QLGddTZpoZ/VVG1s1+/VOz7Nv+rwhCdqtwC3F+hnw1nibi4RYc7cryaeTL2S1TyL5lBOS5Q7zgOmS7pR0gaQeG3i+m4F9JPVLn38DeCgiPkif3wHsAMyV9BdJ35bUeQPPVZf+gIBX0xvbl0taDhxFMiBkWp35RFKL9Kb1f0iaK2kBcA7Jp6gN0Qv4JCLmVTVEcoP1i6y7/mu7Mh5/nP67RQPPZ2ZWaB5nynec6UiSpL1f32Ak9Umv++uSPiCZZYOGvy+AD0kSSSszTtCsXM0j+UXbr479lP4b1bQHQEQsJPkF/12S+wLOAKZJOr2hQUXEy8CrwDcltSf51PTPGf3PAzuTrGlvRfIJ5WxJuzb0XPVQ9fM/ENg3Y+tFsgSmNr8G/kCytOMcYCjJHwUNtfY617MdgIiosc/MrEA8ztSt3MeZqv+2resViLQvSaK+PfBLkmWmp9bntbWc3+NhGXKCZmUpIj4ludn4fEnbZPdLapmWP36T5Objg7J2OQiYlu7bOiKWR8SdEXFeRPRNjz0i3XcZ0FJSm3qGdzPJ4PtVkipTD2fE1ToiFkTETRFxJsk6/BbAMfU8dk2WAZtmtb2W/tstImZlbe/WcbzjgD9GxEUR8Wj6B8HHWftUd85sbwCdM/8wSL9DZj/S629mVoo8zuRojuPMJyRLOLtV01ddbEeS3P83NCLGRcQLJMVaNtR25F4TKwNO0KycnQN8Cbwk6TuSekvaW9I3SZY2HB0Ry4BrgF9JGi6pl6SfkHzHyC/T43wvXXIyRNJuko4iWTM+Je2v+gX/E0n7VH2ZZC3uANoAVwF/jYjMZR23SrpK0gGSdicZXNsDLwFI2lHSKkmXNvBavAFsI+ms9Dr0j4jZwGjgz+kSl70k9Zd0iaQL6zjeDOCINM59JX0f+E415+wn6URJfSX1yj5IRDxJUqJ6dPrdL31J1tRvTvLJqZlZKfM4s06zG2ciYg0wndylkjXFNgPYCjhDUg9JJwJ/3YgQBpJUxbRyU+yb4Lx5a8yNpArU70kqWK0AFpMMeBcCW6X7tCKpsPU+yUA7HRiZcYx9gHvS/uXpsX5Gxg3WwE9IvrNkOfC3esR1C8l6+25Z7ScDT5N8KrcEeJmMUsysu1n4sjqOv97N22nbdSSVopYAv0rbWpOUX56TvvePSD5pPTTtPyw93/ZZx9oWeCg91kfA/5J8Yjs3Y592JH8kVJKUTf7PtD27ulaXdL+FJJ84Pgn0y+ivKYYARhX7/zFv3rw1783jzHptzW6cIVmK+Xo17TXFdmn6fpYAzwBnk1EEhPqX2T84be9R7J8Bb/nflP5HNjMzMzOzBpC0E0nyeVQks3WFOu94oENEHFmoc1rhOEEzMzMzM9tAkn5H8t11fSNiZQHOdxTJVxUcGBEvNvb5rPB8D5qZmZmZ2Ya7iKRYyG2SGvVva0k9Se7r+7mTs/LlGTQzMzMzM7MS4Rk0MzMzMzOzEtGq2AHkU2VlpacDzcyasYqKCtW9V+nwuGVm1rxVN255Bs3MzMzMzKxEOEEzMzMzMzMrEWW1xDFTRUVFsUMwM7MCqKysLHYIeeFxy8yseahr3PIMmpmZmZmZWYlwgmZmZmZmZlYinKCZmZmZmZmVCCdoZmZmZmZmJcIJmpmZWT1Jai3pGklzJb0naUtJfSRNkjRD0n2SOmbsv3XaNk/Sk8WM3czMmgYnaGZmZvV3I7AE2A3YAVgEjAbOjoiewETglwCSWgITgJsiYhfgiMYMbNkqCH/ttZlZk+cEzczMrB4kdQUOAi6LFNARaB0RU9Pd/gicmD4+CZgSEQ8DRMTqxozv7Kdh8P3w5qLGPIuZmTW2Rk/QJLWT9Hi6vGOOpIvT9jMlLZQ0K91eznjNJZJmS5omaWhGe39Jr6bHuV5SXuN/+WP4+YtwWbo98HY+j25mZk1cLyCAiekYdQewDGgjab90n65Ah/RxH6C7pBfTceuSxgpswntw2xx48t+wz51wxSuwslHTQTMzayyFmkG7Ml3e0RsYLmnftP2OiOiRbv0AJA0ChgI9gSHAdZJaV+0PjIqI3YHOJJ9O5s3LH8NlL8HP080JmpmZZegCzAGOIhmjPgJ+BgwHbpA0DbgK+Cxj/4cjYj+gL3CMpGPyHdTSlXD2P9c9X74aLp4M/cfDlI/yfTYzM2tsjZ6gRcTyiJiQPl4GzAW2ruUlg4FxEbE6Ij4EpgMDJHUHlkbE9HS/MUDeB7pMXspvZmYZFgJLImJFulzxXmDPiJgUEQMiohdwA/B6xv4LASJiMfAosGe+g/p8JeyyeW771E/hgLvh/z0Li1fm+6xmZtZYCnoPmqStgQOAyWnTGZLelDRBUs+0bVtgQcbLPiZZMlJTe/7iy+fBzMys3DwHDJK0U/p8KDC5arm9pM7A1ekG8DDwLUmbSGoDHA5MyXdQXTeFR4+D2w6Hju3W7wvgujdgrzHw0Dv5PrOZmTWGgiVoktoC44BLImIRSdWrjhGxG3AzcGvG7muyXt6mjnYzM7NGFRGfA2cB90maQbKE8WrgbElzgceByyPimXT/p4C7gFeB14B7q/ryTYKv7gEzR8Co3XP7310Mxz4EIyfAR0sbIwIzM8uXgiRo6SeH40nW4t8KkC4RqVpFeBdJyWKA+UCnjJd3Tttqam80LldsZmaZIuLxiOgdET0j4qx0LLsxInZN2x/O2v+a9D7rnhFxbWPH13kT+NtgeORY2KlDbv+YubDnGLhllsc4M7NSVYgqjpsCDwDPRMQVGe2DJG2SPh0GvJQ+fgI4TVJLSduQ3Fg9JSLmARWSeqT7jUj3zV+s+TyYmZlZkRy1A0wbDuf3hhZZg9vCFfDNJ+GIB2BuZXHiMzOzmhViBm1/4DDgGxkl9a8ADgZmSpoFnA18F9YuCXkSmEGSgJ2T3lwNMAoYI+lNkipZtzdm4P5w0czMmqrNWsNvDoIpw6BPp9z+iR/A3mNdkt/MrNQoymiNQ2Vl5do3U1FR0eDX/3UmfOupdc+/2QP++pV8RGZmZo2lsnLdNFBFRUWTWgyxseNWfa1aA797HX72Eixbldu/T0e4+VDYv7Yay2Zmlhd1jVsFreLY1JRP6mpmZs1Zqxbwoz7Jsscjts/tn/opHHiPS/KbmZUCJ2gZmtTHrmZmZg208+bw2HHwf9WU5F8TLslvZlYKnKDVooxWf5qZmQFJSf6vpSX5/2O33P7MkvwLXJLfzKzgnKBlkKfQzMysmei8Cdx+RO0l+Xu4JL+ZWcE5QTMzM2vGqkry/9Al+c3MSoITtFr4A0MzM2sONmsN19SjJP+vXZLfzKzROUHL4BWOZmbWnPXrnCRpVx0Am7Rav2/5arhoMuw3Hl5cUJz4zMyaAydotfCaezMza26qSvK/cXr1Jflf/xQOuBt+8JxL8puZNQYnaBk8g2ZmZpbYpWJdSf6t2q7ftybg2qnQayw87JL8ZmZ55QStFp5AMzOz5qyqJP+skdWX5H/nCzjmITjDJfnNzPLGCVoGl9k3MzPLVVWS/+FjYcdqSvKPngt7joFbXZLfzGyjOUGrhccYMzOzdY7eAabXUJL/sxXwjSdhiEvym5ltFCdoZmZmVm9VJfknnwL7VlOS/4m0JP+Vr7okv5nZhnCClsErHM3MzOqnfxeYcgpceQC0a7l+3/LVcOELSUn+l1yS38ysQZyg1cLr6M3MzGrWuiX8uA9MGw6Dt8vtf/1TGHA3/NAl+c3M6s0JWgYXCTEzM2u4XSpgwvFw61eqL8n/u7Qk/yPvFic+M7OmxAlaLTyBZmZmVj8SfL0HzBwBZ9RQkn/og/Afj7skv5lZbZygZfAEmpmZ2cbpsinccQQ8dEz1Jfn//mZSkv8Wl+Q3M6uWE7RaeOAwMzPbMEN3rL0k/zefhMPvhzmLihKemVnJcoKWwTNoZmZm+VNXSf6n/g373Am/eAm+dEl+MzOgAAmapHaSHpc0T9IcSRen7RdImitplqSHJXXOeM2atL1qOzJt7y/p1fQ410tygmlmZlbiMkvyb9Jq/b4Vq+HSF2HfcfDsh8WJz8yslBQqwbkyInYBegPDJe0LvAbsExE9gGeAizP2XxoRPTK2x9L2O4BREbE70Bk4qTGD9gpHMzOz/MgsyX9kt9z+mQvhkHvhu0/DohUFD8/MrGQ0eoIWEcsjYkL6eBkwF9g6Ih6PiKo6Tm8AXWs7jqTuJInb9LRpDHBMPmN1mX0zM7PGtfPm8MixcMdg6Nwut//PM5IiInfO9b3gZtY8FXSJoKStgQOAyVldo4AnMp63k/SmpDckfTtt2xZYkLHPx9SR1G0sjwtmZmb5J8EZu8OskfDNHrn985fC8Alw/MNJeX4zs+akYAmapLbAOOCSiFiU0f6fQEfglozd20fEbsDRwAWS9krb12Qdtk1eY8znwczMzKxWW7WDv34FnjwBdq/I7X/wHeg5Bn77OqzK/gvAzKxMFSRBk9QGGA88HBG3ZrR/DfgqMCwi1tZviojl6b8fAM8BPYD5QGYNqM5pW6Px0gozM7PGd9h28PrpcGk/aJ31l8nSVXD+JBgwHl75uDjxmZkVUiGqOG4KPAA8ExFXZLR/B/g2MDQiKjPad5G0a/q4MzAQeDki5gEVkqoWQ4xg/WWReYg1n0czMzOz+mrXCn6+P7x2Ggys5gaGVz6B/cYnydrilYWPz8ysUAoxg7Y/cBjwjYyy+VeQVG3sBrxQ1Z7uXwHcLWku8BRweUS8nfaNAsZIehP4DLi9MQP3BJqZmVlh9dwKnj4J/nwoVGTdyLAmkuWOe41Jlj+amZWjVnXvsnEi4imgbTVdF9Ww/yvAPjX0TQH2zVds2TyBZmZmVnwtBN/uCcfvBP/vORg7d/3+dxfDcQ/B6bvAdQOh66ZFCdPMrFH4i57NzMysJHXdFMYMgQePgR075PbfOQ96jE5K86/xshczKxNO0GrhIiFmZmbFd8yOMH04nN87mV3LVPll8uXWg+6FGZ8VJTwzs7xygpbBRULMzMxK02at4TcHwYvDoF/n3P7n5sO+4+DSKbB8VeHjMzPLFydotfAEmpmZWWnp2xleOAV+exBslnUn/co18IuXofed8OQHxYnPzGxjOUHL4Ak0MzOz0teqBfygN0wfAcfumNs/pxIOvx+++SR8urzw8ZmZbQwnaLXwPWhmZpZJUmtJ10iaK+k9SVtK6iNpkqQZku6T1DHrNZL0gKQbihV3udqxAzwwFO48svpKjrfMSoqI3D7HY7qZNR1O0DJ4Bs3MzOpwI7AE2A3YAVgEjAbOjoiewETgl1mv+THQvYAxNisSnLYLzBwB3+2Z2//JcvjqE3DUP2BeZeHjMzNrKCdoZmZm9SCpK3AQcFmkgI5A64iYmu72R+DEjNcMBA4DflvgcJudLdrCTYfCMydBzy1z+ye8D73GwpWvwsrVBQ/PzKzenKDVwqshzMwsQy+SoWGipNmS7gCWAW0k7Zfu0xXoACCpM/A74OvAmiLE2ywN3AZePQ1+sT+0bbl+3/LVcOEL0H88TP6oOPGZmdXFCVoGl9k3M7NadAHmAEcBPYGPgJ8Bw4EbJE0DrgKqvo3rT8AFEbGgCLE2a21awn/3g6mnw2Hb5vZP/RQOvBvOewY+/7Lw8ZmZ1cYJWi08g2ZmZhkWAksiYkVErAbuBfaMiEkRMSAiegE3AK+n+3cD/iRpFnAF8B+SrilG4M3V7lvAxBPglq/AVm3X7wvghmnQcwzc+1YxojMzq54TtAyeQDMzs1o8BwyStFP6fCgwWVILWLuk8ep0IyL2i4geEdEDuAi4IyLOL3zYzZsEZ/aAWSNh1O65/R8sgZMfSbb3Fxc+PjOzbE7QauGSvGZmViUiPgfOAu6TNINkyePVwNmS5gKPA5dHxDNFDNNq0HkT+NtgeOw42Hnz3P5730pm0254A1b7jkEzKyInaBl8D5qZmdUmIh6PiN4R0TMizkqXO94YEbum7Q/X8LpbI+LcQsdruYZ0gzdOhwv7QMuscf+LlXDes3DwPcl9amZmxeAErRaeQDMzMys/m7aGKw6AV06DAV1y+ycvgH53JRUfl64sfHxm1rw5QcvgCTQzM7PmY5+O8NzJ8PuB0KH1+n2r1iTfmdZrLDz6bnHiM7PmyQmamZmZNVstW8C5e8OMEXBy99z+t76Aox+EkRNg/tLCx2dmzY8TtFq4SIiZmVnzsH17uPtouOdo2G6z3P4xc6HHaPjTdFjjvw/MrBE5QcvgJY5mZmbN20ndk9m0c3vl/l1Q+SWc/U8YeA+84SIiZtZInKDVwh+QmZmZNT+bt4HfHwIvnAL7dsrtf/4j6HsXXOQiImbWCBo9QZPUTtLjkuZJmiPp4rS9o6RH0rZHJG2V8ZpLJM2WNE3S0Iz2/pJeTV9zfdWXg+Yv1nwezczMzJqy/beGF4fBNQfBpq3W71u1Bn6dFhF5xEVEzCyPCjWDdmVE7AL0BoZL2pfkyz3viYjdgXuAywAkDQKGAj2BIcB1kqpqK90BjEpf0xk4qTGD9gyamZlZ89aqBfywd7Ls8fgdc/vf+gKGPggjJsCHSwofn5mVn0ZP0CJieURMSB8vA+YCWwODgbHpbmOAY9LHg4FxEbE6Ij4EpgMDJHUHlkbE9GpekxeeQDMzM7Pq7NgB7hsKdx9VfRGRsXNhzzFwk4uImNlGKug9aJK2Bg4AJgMdI2IRQERUAlVLHLcFFmS87GOgay3tjcZVHM3MzKyKBCfvnMymfX/v6ouIfO+fcLCLiJjZRihYgiapLTAOuCRNzLLTnzYZj9fU0FdTe174HjQzMzOry+Zt4LqBMHkY9KmmiMgLH0GfcfCT52GJi4iYWQMVJEGT1AYYDzwcEbemzQsltU/7K4DP0vb5QOavu85pW03tZmZmZgW3XxeYMgx+exBsllVEZHXAVa8lRUQefqco4ZlZE1WIKo6bAg8Az0TEFRldE4Hh6eMRwBPp4yeA0yS1lLQN0BeYEhHzgApJPap5TaPwCkczMzOrTasW8IO0iMgJO+X2v/0FHPMQDH/MRUTMrH4KMYO2P3AY8A1Js9LtCuBHJBUd5wDDgB8DRMRTwJPADJIE7JyIWJweaxQwRtKbJDNut+czUK9wNDMzsw2xQ1pE5J6jqy8icuc86DEG/jjNRUTMrHaKMqqEUVlZufbNVFRUNPj1D70Dxz607vnR3eDh4/ISmpmZNZLKysq1jysqKprUZ20bO25ZafriS/jpFPh9DcnYgC7wp0OhdzX3r5lZ+atr3CpoFcdS16RGdTMzMytJHdrAtQNh8inQt5okbPIC6HcX/NhFRMysGk7QalE+c4tmZmZWaP27JJUerz0Y2rdev291wNWvwV5jkxU8ZmZVnKBlcJl9MzMzy6dWLeC/9oGZI+Dk7rn973yR3F5x+mPwbxcRMTOcoJmZmZk1uu3bw91Hw71Hw/bVFBEZNw/2HAN/mAars7/11cyaFSdotfASRzMzM8unE7snJfl/sA+0yFq58/mXcO4zcNA98NonxYnPzIrPCVoGr3A0MzOzxtahDfz2YHhxGPTrnNs/ZQH0vwt+NMlFRMyaIydotSijbyAwMzOzEtO3c1Lp8fqB1RcR+c3r0HMM/OPtooRnZkXiBC2Di4SYmZlZIbVsAeftnRQROaWaIiLvLobjH4ZTH4UPFhc+PjMrPCdotfAEmpmZmRXC9u1h/NFw39HQrX1u//h/JUVEbnjDRUTMyp0TtAyeQDMzM7NiOiEtIvLD3rlFRL5YCec9Cwe6iIhZWXOCVgvfg2ZmZmaF1r41XHMQvDQM9uuS2/9iWkTkgkmw2EVEzMqOE7QMvgfNzMzMSkWfzvD8yfD7gdChmiIi16RFRB54uyjhmVkjcYJmZmZmVqJatoBz0yIiw3bO7X9vMZzwMAx7xEVEzMqFE7RaeIWjmZmZlYLt2sNdR8H9Q2GHaoqI3P0W9BgD102FVS4iYtakOUHL4BWOZmZmVsqO3wmmj4ALekPLrD9cFq+E//ccDBif3KdmZk2TE7RauEiImZmZlZr2reHqg+ClU2H/aoqIvPJJkqSd+wxUrih8fGa2cZygZfAMmpmZmTUV+3aCSSfDDYfA5m3W7wvgD9OSZY9j5/pDZ7OmxAlaLfy7zMzMzEpZyxZwTi+YNQJG7JrbP38pjJgARz8IcysLH5+ZNZwTtAwus29mZmZN0Tabwegh8OhxsMvmuf2PvQe9xsIvXoIVqwsfn5nVnxO0WngGzczMzJqSI7vBG8Php/2gddZfeStWw6UvQu874ckPihOfmdWtYAmapL6SpmY8f0jSrIxtiaSd0r41WX1Hpu39Jb0qaY6k6yU5wTQzMzPLsEkruHx/mHo6fGXb3P7Zi+Dw++FrT8CCpQUPz8zqUJAER9I1wITM80XEMRHRIyJ6AAcBHwAfpt1Lq/rS7bG0/Q5gVETsDnQGTsprnPk8mJmZmVkR9dgSnjgB/jYYOrfL7f/bnKSIyM0zYI2XDZmVjIIkaBFxPtCvll1+ANwcETUWg5XUnSRxm542jQGOyV+UuVzxyMzMMklqLekaSXMlvSdpS0l9JE2SNEPSfZI6pvv2lvSKpDclvSGpUccss+pIMGp3mDUSvtMzt3/hCvjO0zDwHpj6aeHjM7NcRV8iKGkL4D+AP2Y0t8sY0L6dtm0LZH7t4sdA1/zGks+jmZlZGboRWALsBuwALAJGA2dHRE9gIvDLdN9lwMiI2A04Afir5JHGimOrdvCnQ+G5k2HvrXL7n/8I+o6DH02CJSsLH5+ZrVP0BA34PnBbRCzOaGufDmhHAxdI2ittX5P12qxv/cgvT6CZmVkVSV1JluRfFimgI9A6Iqrusf4jcCJARMyJiNnp47eAVkDbwkduts5BXeHlU+HqA2HTVuv3rQ74zevQcwzc/1Zx4jOzIidokjoAZwHXZ7ZHxPL03w+A54AewHygU8ZundO2/MWTz4OZmVm56UXy2d1ESbMl3UEyS9ZG0n7pPl2BDtkvlDQUmF01vpkVU+uWcMG+MHMEnLBTbv+7i+HER+Ckh+HdLwodnZkVewbtHODOiPisqkHSLpJ2TR93BgYCL0fEPKBCUo901xHAE40ZnO9BMzOzDF2AOcBRQE/gI+BnwHDgBknTgKuAzzJflN5DfT1wdkGjNavDDh3gvqFw79HQrX1u/31vJ7Np17wGK/3daWYFU6gqjpcD9wO7SHpJ0qGSNgX+E/ht1u4VwN2S5gJPAZdHxNtp3yhgjKQ3SQbA2/MaZz4PZmZm5WYhsCQiVkTEauBeYM+ImBQRAyKiF3AD8HrVCyR1Ax4AvhsR04oRtFldTuwOM0bABb2hZdYfQ0tWwQXPQ//x8EJe1y2ZWU0UZTRNVFlZufbNVFRUNPj1z/wbBt237vnArvDMyXkJzczMGkllZeXaxxUVFY32WZukzYE3gEMj4m1JV5AUDPlVRKxJV33cD/w4Ip5JV4PcC3w/IibWEPtGjVtm+fb6J3D2P+GFj3L7RFIJ8ooDYEvfTWm2weoat+o1gybpW9W0bSnptI2KrsSVT+pqZmawceNZRHxOct/0fZJmkCx5vBo4O1318TjJqo9n0peMArYDbpQ0K938sZ+VtN6dkkqPfzoUtsgqxRbAn2ZAj9Fw+xzfCmLWWOo1gybpy4hok9XWHpgfEdWsWi6Ojf0k8tkP4ZB71z0/uCs866HUzKykNWQGrdTGM8+gWSlbsDRZ3vi3OdX3H74d3HgI7LFlYeMya+rqGrdaZTdkkrTzuofqzrrbtERSAv/z/IRZmvzBkJlZeWju45nZhuiyKdw2GM7cA773T5hTuX7/xA9gnzvhwj5wUV9oV+tflWZWX3X9KM0lyVMEzMtoXwO8C5zXSHEVhYuEmJmVrWY1npnl0+Hbw9ThcNWr8D+vwIqMio5froHLX4a/z01m04Z0K16cZuWi1nvQIqJFRLQEJqaPq7ZWEbFzRIwvUJxF4bXVZmblobmPZ2Ybq21L+Gl/mDYchmyf2z+3Eo78B5wxAeYvLXx8ZuWkvmX2T5a0VdUTSUdnfCln2ZCn0MzMyl2zGM/MGsuuFfDocTD6CNh6k9z+0XOTIiI3ToPVawofn1k5qG+CdjvwdQBJvwbuAO6VdE5jBVYKPIFmZlZ2muV4ZpZPEozYDWaNhP/cK/cWkcov4Zxn4KB74NWPixKiWZNW3wTtQGBs+ngUcAQwEPhhYwRVLJ5AMzMre81iPDMrhC3awh8GwQunwL6dcvunLEi+4PoHz8EXXxY+PrOmqr4JWitgsaRuwOYR8Srwb6Bro0VmZmaWfx7PzPJs/63hxWHwu4Ogfev1+9YEXDsV9hwDd//L9/eb1Ud9E7SJwDXAH4HH0rb9gfcbI6hS4V8iZmZlp1mOZ2aNrVUL+H+9YeYIGLZzbv8HS2DYo3D8w/C2v9TCrFb1TdDOTv/9HPhB+vgo4E95j6iIvMTRzKzsNYvxzKxYtm8Pdx0F/zgGduqQ2//gO9BzLPz6FfhydW6/mYGijKaJKisr176ZioqKBr/+hflw4D3rnu/fBSYPy0toZmbWSCor1317bkVFRZP6rG1jxy2zUrZ0JfzyZbj6dVhVTUXHnlvCjYPg0G0LH5tZMdU1btVrBk1SK0mXSponaXn6708lldV3xrvMvplZeWsu45lZKdi0NfzqAHjtNBhYzV2eMxbCYffB15+ABf7uNLO16rvE8UpgMPAdYB/guySVr65opLhKQvnMLZqZWapZjmdmxbTXVvD0SfC/X4GO7XL7b5sDe4yGm6YnRUXMmrt6LXGU9AHQJyIWZLR1BV6JiJKZmN7YpSJTPoIBd697vl8XmOIljmZmJa0hSxxLbTzzEkdrbj5ZBj95Af53VvX9+3eBmwZBn86FjcuskPKyxLEGXhBoZmblwOOZWYF02gT++hV49iTYe6vc/qrvTvuvZ6FyRcHDMysJ9U3QxgDjJA2W1EPSEOBO4O+NF1rxlVH9FDMzSzTL8cys1By8Dbx8KvzmQNgs6w7QNQHXv5F8d9qYN/33mDU/9U3QLiT57pibgFeAP5B8f8yFjRRXUbhIiJlZ2WsW45lZU9C6JZy/L8wcCad0z+3/cCmMfByO/AfMWVTo6MyKp9YETdIISROBiIifR8RuEbEp0APYCzi+EEEWiz+wMTMrD819PDMrZd3aw/ij4cFjoHs13532+Puw91i4dAosW1X4+MwKra4ZtG8Bf4mI9X4cImINcBfwvcYKrBg8gWZmVraa1Xhm1hQdsyNMHwH/3Q9aZ/2F+uUa+MXL0GssPPxOceIzK5S6ErTewD9q6HsE2C+/4ZQWr3k2MysbzXo8M2sqNmkFv9gf3jgdBm+X2/+vz+GYh+DUR+H9xYWPz6wQ6krQNgVW1tC3Bmhb3xNJ6itpasbzMyUtlDQr3V7O6LtE0mxJ0yQNzWjvL+lVSXMkXS9pY6pQVhNjPo9mZmYlJG/jmZk1vj22hAnHw9+PgK6b5vaP/1dSROS3r8OqNYWPz6wx1ZXgTAO+UkPfEGB2fU4i6RpgQjXnuyMieqRbv3TfQcBQoGd6juskta7aHxgVEbsDnYGT6nP+DeUJNDOzspGX8czMCkeCkbvBrBFw3t7QIuuD9MUr4fxJ0O8umDS/ODGaNYa6ErTfAjelSdNakg4HbgSur89JIuJ8oF89YxoMjIuI1RHxITAdGCCpO7A0Iqan+40BjqnnMevFE2hmZmUrL+OZmRVeRVu4fiBMGQb7dcntn/opHHwPfOtJ+HR54eMzy7daE7SIGAtcAzwo6d+SXpA0D3gAuDEibtnI858h6U1JEyT1TNu2BRZk7PMx0LWWdjMzs1oVYDwzs0bWrzM8fzLceAhUtMnt/+ss2GM0/O/M5LvUzJqqVnXtEBHXSboFOADoBHwGvBARizby3KOB/4uIkHQ6cCuwf9qXvZq4TR3tjcI/22Zm5aMRxzMzK5CWLeB7veCUneFHz8Pf5qzf/+lyOOupJFn74yDYp2MxojTbOPUqshERn0fEYxHx94h4JB+DWUSsiFhbJ/EuYLf08XySgbNK57Stpva88RJHM7Py1hjjmZkV3tabwm2D4ckTYM8tc/snzYe+45J71L74svDxmW2MvFZBbAhJgyRtkj4dBryUPn4COE1SS0nbAH2BKRExD6iQ1CPdb0S6b6NxmX0zMzOz0nXYdvDaaXDFgKREf6bVkVR53HMMjJ/nv+us6ShIgibpcuB+YBdJL0k6FDgYmClpFnA28F2AiHgKeBKYQZKAnRMRVd90MQoYI+lNkqUpt+c3znwezczMzMwaW5uWcGFfmDEcjt8xt/+DJXDqY3DsQ8n3qJmVOkUZfZxQWVm59s1UVFQ0+PWvfQJ9xq17vk9HeP30vIRmZmaNpLKycu3jioqKJvVR28aOW2aW6/634Lxn4d1qvsi6XUu4uC/8uA+0bVn42Myg7nGraEscS1GTGtXNzMzMLMcJ3WHGCPhJH2iV9Zfu8tVw6Yuwz1h4/P3ixGdWFydotSijyUUzMzOzZmOz1vDrA5L70wZtk9s/pxKGPAAjJ8CHSwofn1ltnKCZmZmZWVnaayt46kT4v8Ohc7vc/jFzoccY+P0bsDr7y5zMisQJWgYXCTEzMzMrLxJ8bQ+YNRK+2zP3lpbPv4TvPwv7jYcpHxUlRLP1OEGrhVc4mpmZmZWHrdrBTYfC86dAn065/a9+AgfcDd97GhauKHx8ZlWcoGXwBJqZmZlZeRuwNUwZBtcdDB1ar98XwE0zoMdo+Nts1yOw4nCCVgv/UJqZmZmVn1Yt4Pv7wOyRMGLX3P4Fy+BrE+Er98OMzwofnzVvTtAyeAbNzMzMrPnYZjMYPQQeOw52q+arCJ/+N/QeBxe+AEtWFj4+a56coNXCE2hmZmZm5W9IN5h6Oly+X+4XWK9aA1e+CnuOgbv/5RVW1vicoGVwFUczM6uNpNaSrpE0V9J7kraU1EfSJEkzJN0nqWPG/pdImi1pmqShxYzdzGrXrhX8tD9MHw5Hd8vtf28xDHsUjn0I5lYWPj5rPpygmZmZ1d+NwBJgN2AHYBEwGjg7InoCE4FfAkgaBAwFegJDgOskta7mmGZWQnapgIeOhXFHwnab5fY//C70GguXvQjLVhU+Pit/TtBq4RlsMzOrIqkrcBBwWaSAjkDriJia7vZH4MT08WBgXESsjogPgenAgELHbWYNJ8Gpu8DMEXBB76SoSKYVq+HnLyWJ2kPvFCdGK19O0DJ4haOZmdWiF8lndxPTZYt3AMuANpL2S/fpCnRIH28LLMh4/cdpv5k1ER3awNUHwWunwaBtcvv/9Xmy5PHkR+CdLwofn5UnJ2i18E2gZmaWoQswBziKZNniR8DPgOHADZKmAVcBmUW512Qdo00B4jSzPNtrK3jqRPjbYNh6k9z+e99Kioj8+hX4cnXh47Py4gQtg4uEmJlZLRYCSyJiRUSsBu4F9oyISRExICJ6ATcAr6f7zwc6Zby+c9pmZk2QBKN2h1kj4dxe0CLr78Zlq+CiydD7Tpj4fnFitPLgBK0WnkAzM7MMzwGDJO2UPh8KTJbUAkBSZ+DqdAN4AjhNUktJ2wB9gSmFDdnM8m2LtvD7Q+DFYTCgS27/rEUw+AEYOQH+vaTg4VkZcIKWwRNoZmZWk4j4HDgLuE/SDJIlj1cDZ0uaCzwOXB4Rz6T7PwU8CcwgSdbOiYjFxYjdzPKvb2eYdArcfChs1Ta3f8xc6DEarn09+S41s/pSlNGNVpWVlWvfTEVFNV8HX4dZC5P1w1V2r4DZZ+QlNDMzaySVleu+kKiioqJJfda2seOWmZWGT5Ylyxv/MrP6/n06wo2HwMHVFBqx5qeuccszaBma1KhuZmZmZiWh0yZw82Hw/MnQp1Nu/9RPYeC98I2J8PGyQkdnTY0TNDMzMzOzPDiga3Jv2u8HQkU1NVtvnQ17jIabpsNqL3u0GhQsQZPUV9LUjOcXSJoraZakh9Obq6v61qTtVduRaXt/Sa9KmiPp+qobsxtL+Sz+NDMzM7NCaNkCzt0bZo+Er+6e279wBXzvn3DA3fDSgtx+s4IkaJKuASZkne81YJ+I6AE8A1yc0bc0InpkbI+l7XcAoyJid5JyxSflN858Hs3MzMzMmqutN4XbBiffn9Zzy9z+lz6G/cfD955OkjazKgVJ0CLifKBfVtvjEbE0ffoG0LW2Y0jqTpK4TU+bxgDH5DvWTJ5BMzMzM7ONcei28NppcPWBsFmr9fsCuGkG7P53uGUWrPEfn0bp3IM2iqQEcZV2kt6U9Iakb6dt2wKZE8EfU0dS11CeQDMzMzOzfGvdEi7YN/mS61N3zu3/ZDl880kYdG9SUMSat6InaJL+E+gI3JLR3D4idgOOBi6QtFfann07ZTW3X+ZPGX0DgZmZmZkV2fbtYdxR8OhxsFs136zx3HzoOw5+8Bx8/mXh47PSUNQETdLXgK8CwyJidVV7RCxP//0AeA7oAcwHMguXdk7b8hhPPo9mZmZmZpbryG7wxnD4xf7QruX6fasDrp2afMn16Dc9YdAcFS1Bk/Qd4NvA0IiozGjfRdKu6ePOwEDg5YiYB1RI6pHuOoL1l0WamZmZmTUJbVvCf/eDGSPguB1z+z9cCmc8Dkc8ADMXFj4+K55CVXG8HLgf2EXSS5IOJana2A14oaqcfrp7BXC3pLnAU8DlEfF22jcKGCPpTeAz4PbGjNsfWJiZmZlZY+q+OTxwDNx3NOzYIbd/4gfQ+0646AVYsrLw8VnhKcpo3rSysnLtm6moqGZhbx3mVcKuf1/3fOfNYd5/5CU0MzNrJJWVaxdhUFFR0aQWq2/suGVm5WXpSvjVK3DVa7Cymi+y3qE9XHswnNTdt+Y0ZXWNW0UvElLKyih3NTMzM7MSt2lr+OUAeON0OGL73P53F8Mpj8KxDyUTC1aenKBl8CcRZmZmZlZse2wJjx0HY4fAtpvl9j/8Luw1Fn7+IixfVfj4rHE5QauFJ9DMzMzMrBgkOH1XmDUCzu8NLbMmElashstegl5j4eF3ihOjNQ4naBk8gWZmZmZmpaRDG/jNQfDqaXDINrn98z6HYx6CUx6Bd78ofHyWf07QauEZNDMzMzMrBXt3hKdPhP87HLpsktt/z1uw5xj49Svw5ercfms6nKBl8AyamZmZmZUqCb62B8weCef0ghZZf7wuXQUXTYZ97oTH3y9OjLbxnKCZmZmZmTUhW7SFGw6BF4fB/l1y+2cvgiEPwOmPwfuLCx6ebSQnaLVwmX0zMzMzK1V9O8Pzp8CfDoWt2ub2j5sHPUbDla962WNT4gQtg8vsm5mZmVlT0kLwnZ7Jssdv75l7y86SVXDhC1722JQ4QauFJ9DMzMzMrCnotAn8+TB44RTo3zm338semw4naBk8gWZmZmZmTdn+WydJ2k2DYMtalj1e5WWPJcsJWi18D5qZmZmZNTUtW8B394I5tSx7/MkL0NvLHkuSE7QMvgfNzMzMzMpF1bLH50+BftUse5y1KFn2ONzLHkuKE7RaeALNzMzMzJq6AVvD5FqWPd7pZY8lxQmamZmZmVmZy1z2+K09c/szlz0+4WWPReUELYNXOJqZmZlZOeu0Cdx8WFJIpKZlj0d42WNROUGrhZc4mpmZmVk5qlr2+Ecveyw5TtAyeAbNzMzMzJqLli3gbC97LDlO0GrhMvtmZmZmVu687LG0OEHL4DL7ZmZmZtZc1XfZ49Ve9tioCpagSeoraWrG846SHpE0J/13q4y+SyTNljRN0tCM9v6SXk1fc72kRo3fE2hmZmZm1pzUZ9njj73ssVEVJEGTdA0wIet8VwP3RMTuwD3AZem+g4ChQE9gCHCdpNbpa+4ARqWv6QyclNc483kwMzMzM7MmqmrZ4/MnQ99Ouf1Vyx5HTIAPvOwxrwqSoEXE+UC/rObBwNj08RjgmIz2cRGxOiI+BKYDAyR1B5ZGxPRqXmNmZmZmZnl2QFeYMqzmZY9j58IeXvaYV8W8B61jRCwCiIhKoGqJ47bAgoz9Pga61tLeaFwkxMzMzMyau6plj7NHwlk9cvurlj3uOw4metnjRitmgpad/rTJeLymhr6a2vPCRULMzMzMzKrXeRP4y1dqXvY4cyEM9rLHjVbMBG2hpPYAkiqAz9L2+UDmf/LOaVtN7Y3GE2hmZmZmZuurWvZ44yFe9tgYipmgTQSGp49HAE+kj58ATpPUUtI2QF9gSkTMAyok9ajmNXnhCTQzM6uNpNaSrpE0V9J7kraUtKukJyXNTCsNH5LuK0m/kzRL0puSfl7s+M3M8qVlC/heLy97bAyFquJ4OXA/sIuklyQdCvwIGC5pDjAM+DFARDwFPAnMIEnAzomIqknSUcAYSW+SzLjd3phx+x40MzPLciOwBNgN2AFYBFwLXBsRewJnA39J9x2W7tMT2Bs4XtKBBY7XzKxRedlj/rUqxEki4lLg0mq6jqxh/58DOZ80RsQUYN+8BpfBM2hmZlYTSV2Bg4C9I9Z9hCepHcmye4APgS/Tx+2ALYBWEbFc0qKMPjOzslK17PHPM+DiybAo67fd2Lnw4Dvws/7wX3tD65bFibMpKOYSx5LnCTQzM8vQi2RomChptqQ7JG0GnAdcKekRktmzb6X7jwGWArMk3Qq8HBEvFyFuM7OCqFr2OOeM6pc9Ll4JP3oeenvZY62coGVwFUczM6tFF2AOcBTJssWPgJ8B3wEuAS4CPgW+n+7fj2SlyqHAVOAESTsUOGYzs4LLXPbYp5ZljyO97LFaTtDMzMzqZyGwJCJWRMRq4F5gT+AbwJ8i4tWIGAkMltQJ+Crw94h4LyJ+m+5/enFCNzMrvAO6wotptcctqvlyrDFptcerXO1xPU7QauEljmZmluE5YJCkndLnQ4HJwDvA8QCSdgdWkBSymgccJ6mVpNZAb2BWoYM2MyumupY9LlkFP3kB9rkTHnuv8PGVIidoGbzC0czMahIRnwNnAfdJmkGy5PFq4GvARZJmkVQXPiMi1gB/AL4AZpIscXw+Iv5RlODNzIqsatnjpBqWPc5eBEf9A4Y9Au98UfDwSoqijGrJV1ZWrn0zFRUVDX79J8ug863rnm/VFj79Zj4iMzOzxlJZWbn2cUVFRZP6rG1jxy0zs6Zo9Zqk2uMlU2Dhitz+di3h4r7wo32hXUFqzhdWXeOWZ9AyuEiImZmZmVnjWrvscSR8p2fuKrblq+HSF2GvsfDA28WIsLicoNWifOYWzczMzMxKS6dN4E+HJt+fNqBLbv+/PocTHobjHoK5lbn95coJWgZPoJmZmZmZFVb/LjDpFPjrYdC5XW7/g+/AXmPgvyfDkpUFD6/gnKDVooxuzzMzMzMzK1ktBN/cE2afAeftnTzP9OUa+J9XYM8xcNe88v473QmamZmZmZmVhC3bwvUD4ZVT4ZBtcvvfWwynPQZDHki+8LocOUHL4CIhZmZmZmbF17sTPH0i3DEYttk0t/+JD5LvTrtgEnz+ZeHja0xO0GpRxjOnZmZmZmYlTYIzdodZI+GC3tAqK3NZtQaueR16jIY75pTPskcnaBk8gWZmZmZmVlo2bwNXHwRTT4fB2+X2f7gURj0Bh94Hr39S+PjyzQlaLcokCTczMzMza/L23BImHA93HQnd2uf2P/Mh9L0Lznum+i/AbiqcoGXwDJqZmZmZWemSYNguMHMEXNIX2mRlM2sCbpgGe/wd/ndm8rypcYJWi3JZx2pmZmZmVk42aw2/HADTR8AxO+T2f7wcznoKDrwbXlpQ6Og2jhO0DK7iaGZmZmbWdOxaAQ8eCw8MhZ03z+2fsgD2Hw/feQo+WVbw8DaIEzQzMzMzM2vSjtsJpg+Hy/eDdi3X7wvg5pmw+2j44zRYvaYYEdafE7RaeIWjmZmZmVnT0K4V/LR/cn/aKd1z+xeugP98BvqPh0nzCx9ffRUtQZPUW9KsjG2upKcknSlpYUb7yxmvuUTSbEnTJA3Ne0z5PqCZmZmZmRXUTpvD+KPh0eNgjy1y+1/7BA6+B77+BMxfWvDw6lS0BC0iXo+IHlUbcBXwatp9R0ZfPwBJg4ChQE9gCHCdpNb5jKlFVoa2qsSnP83MzMzMrHpHdku+O+3KA2CzVrn9t82B3f8Ov3sdVq4ufHw1KYkljpJaAT8EflPLboOBcRGxOiI+BKYDA/IZxyZZ/+GWr26apTnNzMzMzAzatIQf94HZI2Hkrrn9X6yEH06CPuPgyQ8KH191SiJBA74K/DMiqi7LGZLelDRBUs+0bVsgs0jmx0DXfAbRQrBpVpK2dFU+z2BmZmZmZoW2XXv4+xB46kTotVVu//SFcPj9MGICvL+48PFlKnqCJqkl8GPgyrRpNNAxInYDbgZuzdg9e9Fhm3zHkz39uXhlvs9gZmZmZmbFcOi28OppcN3BsHk1mcTYubDHaPj1K7CiSMsei56gASOAVyJiHkBErIhY+xXRdwG7pY/nA50yXtc5bcur9ll3tS1xgmZmZmZmVjZatYDv7wNzRsKZe+T2L10FF02GvcfCI+8WPr6iJmiSWgAXAVdktA2StEn6dBjwUvr4CeA0SS0lbQP0BabkO6bNshI0z6CZmZmZmZWfrTeFWw6HSSdD3065/W9WwtAH4eRH4K3PCxdXNfVMCmoYMC8ipmW0HQzcJmk58AHwbYCIeErSk8AMYDVwTkTkfYVo9gzaGY/DwG2SEvwCpHX/kt2W1V9VFLK69gYfYyPOkXOMGo619jg17NugY2zgOao9xoZes+qOsaHXbAP+uxT7mpmZmZlZ3Q7sClOGwV9mwsWT4bMV6/ff+1Yyk3Zhn6TgSHZhwXzTutWETV9lZeXaN1NRUbFBxzjifniiRCq4mG2svCS1DT3GRpyjUIlxncfY0MS4pmP4utfYv8vmcNQObJTKysq1jysqKlTLriUnH+OWmZnlz6fL4ZLJ8OcZUF2WtFMHuOEQOHbHDT9HXeNWsWfQSs4OHYodgVn+BLDeZzDl83mMlYlTd974BM3MzCxfOraDmw6Fb/eEc5+BFz5av//tL2DOoo1L0OpSCkVCSsqFfaDLJnXvZ2ZmG8/Lcc3MrBT16wzPnQy3fAU6t1vX3nNLOLdX457bM2hZdt8C3hkFj74HnyxPvqi6agvWn5GoerzevxmPqaE9s43q2uvqz2pbG8sGnqPa99PAc9TnmtR1jrWx1hKvr3vdxzBrSpyfmZlZqWohOLMHnNQdLnsRbpgG1w+E1i0b97xO0KrRrhWc2L3YUZhtuGqTvA1JjGs6xgYkxjUeI5+J8QYmtTUeo1SvWU3H2IBz1HiMfP53qeWa9OuMmZlZSduiLVw7EH7YuzC3QzlBMytDa4syeHrCzMzMLC8KVavC96CZmZmZmZmVCCdoZmZmZmZmJcIJmpmZmZmZWYlwgmZmZmZmZlYinKCZmZmZmZmVCCdoZmZmZmZmJaJsy+xXVlYWOwQzM7N687hlZmbgGTQzMzMzM7OS4QTNzMzMzMysRCgiih2DmZmZmZmZ4Rk0MzMzMzOzkuEELYukYyRNkzRb0sXFjqdQJLWT9LikeZLmVL13SR0lPZK2PSJpq4zXXJJep2mShhYv+sKQ9CNJ09LHzf66SGot6RpJcyW9J2nL5n5dJH09fX9zJN0lqX1zviaS+kqamvG8wddCUn9Jr6avuV6Sx60S4/Fj43hsqT+POxvGY1P9lcy4FRHe0g3YDHgH6EpS4fIZoG+x4yrQe28HDEkfbwK8DuwL/C/w3bT9u8D16eNBwLNAS2AbYA7QutjvoxGvz8HAq8C09Hmzvy7AzcDlgDK2ZntdgK2BfwEd0uc3Ahc112sCXAN8WvUzk7Y1+FoAs4G90sejgVOK/d685fy39vix4dfOY0vDrpfHnYZfM49N9b9WJTNu+ZPI9e0PvBIR8yNiFXAXcEyRYyqIiFgeERPSx8uAuSQ/1IOBseluY1h3PQYD4yJidUR8CEwHBhQ26sKQ1An4HXB2RnOzvi6SugIHAZdFBpr3dWlD8iFP+/T5fOBLmuk1iYjzgX5ZzQ26FpK6A0sjYno1r7ES4fFjw3hsaRiPOxvMY1M9ldK45QRtfdsCCzKef0wym9asSNoaOACYDHSMiEUAEVEJVE3tNotrJUnA/wE/Bj7K6GrW1wXoBQQwMZ3ev0PSZjTj6xIR75H8sTVT0l+A/Ug+qWy216QaDb0WzfEaNWkeP+rHY8sG8bizATw2bbSijFtO0HKtyXrepihRFImktsA44JL0f8jsMp+Z16M5XKsfAJMi4qms9uZ+XbqQTOcfBfQk+QPjZzTj6yKpAjgBOBB4FNgZOJxmfE2qsSHXorldoybL40eDeGxpOI87G8Bj00YryrjVqqEvKHPzgU4Zzzunbc2CpDbAeODhiLg1bV4oqX1ELE5/yD9L25vLteoOHCnpq0BrYHtJz+DrshBYEhErACTdC/yI5n1dhgAzI2ImySeVi4FzaN7XJFtDr0VzvEZNksePBvPY0nAedzaMx6aNU5RxyzNo65sM7Cepi6RWwKnAE0WOqSAkbQo8ADwTEVdkdE0EhqePR7DuejwBnCappaRtgL7AlELFWygRcV5E7BERPUjWG78ZEYfQzK8L8BwwSNJO6fOhJD8/zfm6/As4JKPCU39gFs37mmRr0LWIiHlAhaQe1bzGSoTHj4bz2LJBPO5sGI9NG6c441ZDq4qU+wYcT3Kj3xzg0mLHU8D3fRiwguSHtmq7giTzfyy9Ho8BnTNe8zOSSjUzgOOK/R4KcI12Yl2lrWZ/XYAjSKq1zQD+CrRt7tcF+H763meSVG7q0FyvCUmltanAMuAl4NANuRYkxZteA94E/gC0LPZ785bz39rjx8ZdP48t9b9WHnc27Lp5bKrfdSqZcUvpgczMzMzMzKzIvMTRzMzMzMysRDhBMzMzMzMzKxFO0MzMzMzMzEqEEzQzMzMzM7MS4QTNzMzMzMysRDhBM2sASaskHZo+vlXSLxvxXGdKejar7QlJlzbWOWuIIyTtWshzmplZ/njsMmtaWhU7ALOmJCKK+jMTEYOLeX4zM2t6PHaZNS2eQTNrgPQTuZ0kfQ34GnBx+snks2n/tpLGS/pM0tuSzs147a2SRku6X9Lnki6R9C1J70laLmm+pN9KaiFpR5Iv4Tw4Pf4qSW0lPSXpzPR4knShpHckfZIef/O0b6c01h9LelPSIklX1/K+OqdxL5T0saQHJG0u6Yl0l1lpDN9J9z9W0muSvpA0SVKvrGt0paSZ6fFulOQPg8zMisRjl8cua1qcoJltgIi4DbgN+FVEtIqIgZJaAA8AbwDdgBOBSyUdlPHS/sDvgK2A3wITgQOATYG9Sb61/tSIeAc4C3guPX6riFiRFcbXgK8DRwC7pse4PmufCuBg4CDgXEl9a3hLVwCLgJ2BPsBTQJuMTz17pDH8OT3GLcC5QGdgPDAuff9VNgEOB3oDQ4BRNZzXzMwKxGOXxy5rGpygmeXPfkAn4OcRsSQiXgfuAo7J2GdsRDwZEasiYhmwErgEmAbMJhkUetbzfF8FfhcRb0bEIuBi4D+yPvH7WUQsiIgZwHSgRw3H2gzYBugQEe9HxDUR8UkN+34buCkino2I5SSDdTeSAbLK9RHxYUS8C9wKHF3P92RmZoXlsWsdj11WEpygmeXPjiS/7Jelyz6WA98Ctq5uZ0kCHiP5RPIUkkHmdup/b+h2wPsZz99LX9ulhv2XA21q6PsJsAyYLunf6TKPmn4/7AhclPEelwFt0/ir8wnJezQzs9Ljsat6HrusaLy21mzDrQaU8fwD4K2I2KWer+9C8qngARFRCZCMezUeP9sHJINqlW7AKmABsH09YwAg/bRwWDqw9QEmAE8DDwFryH2fl0fEL+p5+O7AWw2Jx8zMGo3Hrvrx2GVF4xk0sw33HnCIpC0kbQtMIfkE8n8kbSWpg6RDJQ2q4fWfApXAEZJaSTqB9ZdTvAfsKal7egN39oD3N+CHknaXtAXJWvw7ImJVQ99IGvMgkk8pPwNWpP9WxXFU+n62IlnDf56kIenN390kfUNS+4xD7iWptaQBwDdJbho3M7Pi89jlsctKnBM0sw13E8ks9EfAjRGxEjgW2A2YSfJp3f+QfIqXIx2Mvg5cBywkWZc/PWOXp4EHSdb4vwK0zjrEbSRr5B8H5gFLge9v4Hv5DPhLGsdE4IqIeCHtu4DkXoOPgCERMYlkLf8VJAP1ZJKbvVdnHO+C9JhjgPMjYsoGxmVmZvnlsctjl5U4RUSxYzCzMiIpgN0iYm6xYzEzM6sPj11WSjyDZmZmZmZmViKcoJmZmZmZmZUIL3E0MzMzMzMrEZ5BMzMzMzMzKxFO0MzMzMzMzEqEEzQzMzMzM7MS4QTNzMzMzMysRDhBMzMzMzMzKxFO0MzMzMzMzErE/wfU10W6SVJaxAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot cost versus iteration  \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "ax1.plot(J_hist)\n",
    "ax2.plot(100 + np.arange(len(J_hist[100:])), J_hist[100:])\n",
    "ax1.set_title(\"Cost vs. iteration\");  ax2.set_title(\"Cost vs. iteration (tail)\")\n",
    "ax1.set_ylabel('Cost')             ;  ax2.set_ylabel('Cost') \n",
    "ax1.set_xlabel('iteration step')   ;  ax2.set_xlabel('iteration step') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363e1044",
   "metadata": {},
   "source": [
    "*These results are not inspiring*! Cost is still declining and our predictions are not very accurate. We can improve this in the next notebook."
   ]
  }
 ],
 "metadata": {
  "dl_toc_settings": {
   "rndtag": "15456"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('python_for_machine_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc-autonumbering": false,
  "vscode": {
   "interpreter": {
    "hash": "40fc6ebffc74793621f684cf09d9f3d0a501c91440a6f462aebac8d38ed47133"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
